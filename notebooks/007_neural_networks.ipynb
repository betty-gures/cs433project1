{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14cea815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from metrics import f_score, auc_roc\n",
    "from models import LinearSVM\n",
    "from visualizations import plot_roc, plot_losses\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5212a220",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load(\"../data/dataset_prep/train.npz\")\n",
    "x_train = train[\"x_train\"]\n",
    "y_train = train[\"y_train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2854c681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((328135, 622), (328135,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape\n",
    "# shape is (N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dc9afaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import COL_REMOVE_MANUAL, impute_missing_values, remove_duplicate_columns\n",
    "\n",
    "# copied and altered `normalize_and_bias_data` from preprocessing.py to make it suitable for neural networks\n",
    "# alterations\n",
    "# > no need for bias since neuralnet already does it\n",
    "# > no need for squared features since neural net can learn non-linear relationships\n",
    "\n",
    "variable_type = []\n",
    "with open(\"../data/metadata/variable_type.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip().strip('\"')  # remove whitespace and surrounding quotes\n",
    "        # split by comma and convert to int\n",
    "        if line == \"\":\n",
    "            variable_type.append([])\n",
    "        else:\n",
    "            variable_type.append(line)\n",
    "\n",
    "# def normalize_and_bias_data(x_train, x_test=None, squared_features=True, feature_names=None):\n",
    "def normalise_data(x_train, x_test=None, feature_names=None):\n",
    "\n",
    "    \"\"\"Standardize data and add bias term.\n",
    "    Args:\n",
    "        x_train: np.ndarray of shape (N_train, D)\n",
    "        x_test: np.ndarray of shape (N_test, D)\n",
    "    Returns:\n",
    "        x_train_std: np.ndarray of shape (N_train, D+1)\n",
    "        x_test_std: np.ndarray of shape (N_test, D+1)\n",
    "    \"\"\" \n",
    "    if feature_names is None: feature_names = np.arange(x_train.shape[1])\n",
    "    one_hot_encoding = x_train.shape[1] > len(variable_type) # one-hot encoding was applied\n",
    "\n",
    "    # missing data imputation\n",
    "    x_train = impute_missing_values(x_train, x_train)\n",
    "    if x_test is not None: x_test = impute_missing_values(x_train, x_test)\n",
    "    \n",
    "    # manual duplicates removal\n",
    "    x_train = np.delete(x_train, COL_REMOVE_MANUAL, axis=1)\n",
    "    remaining_var_types = np.delete(variable_type, COL_REMOVE_MANUAL)\n",
    "    feature_names = np.delete(feature_names, COL_REMOVE_MANUAL)\n",
    "    assert feature_names.shape[0] == x_train.shape[1], \"Feature names and data shape mismatch after manual duplicate removal.\"\n",
    "\n",
    "    # invariant feature removal\n",
    "    stds_train = np.std(x_train, axis=0)\n",
    "    remove = (stds_train < 1e-10)\n",
    "    x_train = x_train[:, ~remove]\n",
    "    remaining_var_types = np.delete(remaining_var_types, remove[:len(remaining_var_types)])\n",
    "    feature_names = np.delete(feature_names, remove)\n",
    "    assert feature_names.shape[0] == x_train.shape[1], \"Feature names and data shape mismatch after invariant feature removal.\"\n",
    "\n",
    "    # feature scaling by standardization\n",
    "    mean = x_train.mean(axis=0)\n",
    "    std = x_train.std(axis=0)\n",
    "    assert not np.any(std == 0), \"At least one feature has zero standard deviation.\"\n",
    "    x_train = (x_train - mean) / std\n",
    "    \n",
    "    # # squaring continuous features\n",
    "    # if squared_features:\n",
    "    #     squared_feature_idx = np.array([i for i, var_type in enumerate(remaining_var_types) if var_type in [\"ordinal\", \"continuous\"]])\n",
    "    #     x_train = np.column_stack([x_train, x_train[:, squared_feature_idx] ** 2])\n",
    "    #     squared_feature_names = [f\"{feature_names[i]}_squared\" for i in squared_feature_idx]\n",
    "    #     feature_names = np.concatenate([feature_names, squared_feature_names])\n",
    "    # assert feature_names.shape[0] == x_train.shape[1], \"Feature names and data shape mismatch after squaring features.\"\n",
    "    \n",
    "    if one_hot_encoding: \n",
    "        one_hot_encoded = [i for i, t in enumerate(remaining_var_types) if t == \"nominal\"]\n",
    "        x_train = np.delete(x_train, one_hot_encoded, axis=1)\n",
    "        feature_names = np.delete(feature_names, one_hot_encoded)\n",
    "        assert feature_names.shape[0] == x_train.shape[1], \"Feature names and data shape mismatch after removing one-hot encoded features.\"\n",
    "\n",
    "    # # add bias term\n",
    "    # x_train = np.c_[np.ones((x_train.shape[0], 1)), x_train]\n",
    "    # feature_names = np.concatenate([np.array([\"bias\"]), feature_names])\n",
    "\n",
    "    if x_test is None:\n",
    "        x_train, kept_idx = remove_duplicate_columns(x_train)\n",
    "        return x_train, feature_names[kept_idx]\n",
    "\n",
    "    x_test = np.delete(x_test, COL_REMOVE_MANUAL, axis=1)\n",
    "    x_test = x_test[:, ~remove]\n",
    "    x_test = (x_test - mean) / std\n",
    "    # if squared_features:\n",
    "    #     x_test = np.column_stack([x_test, x_test[:, squared_feature_idx] ** 2])\n",
    "    if one_hot_encoding: x_test = np.delete(x_test, one_hot_encoded, axis=1)\n",
    "    # x_test = np.c_[np.ones((x_test.shape[0], 1)), x_test]\n",
    "    x_train, x_test, kept_idx = remove_duplicate_columns(x_train, x_test)\n",
    "    return x_train, x_test, kept_idx, feature_names[kept_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80d6596c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m x_test_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      4\u001b[0m lazy_load_data()\n\u001b[0;32m----> 5\u001b[0m x_test \u001b[38;5;241m=\u001b[39m \u001b[43mx_test_orig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m()\n\u001b[1;32m      7\u001b[0m x_train_norm, x_submission_test, test_ids, _ \u001b[38;5;241m=\u001b[39m normalise_data(x_train, x_test)\n\u001b[1;32m      8\u001b[0m N \u001b[38;5;241m=\u001b[39m x_train_norm\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "from preprocessing import lazy_load_data\n",
    "\n",
    "x_test_orig = None\n",
    "lazy_load_data()\n",
    "x_test = x_test_orig.copy()\n",
    "\n",
    "x_train_norm, x_submission_test, test_ids, _ = normalise_data(x_train, x_test)\n",
    "N = x_train_norm.shape[0]\n",
    "D = x_train_norm.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf47c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import floor\n",
    "\n",
    "train_test_ratio = 0.9\n",
    "split_index = int(N * train_test_ratio)\n",
    "\n",
    "permutation = np.random.permutation(N) # randomising data incase of skew\n",
    "\n",
    "x_train_reshape = x_train_norm.transpose()[:, permutation]\n",
    "y_train_reshape = y_train.reshape(1, -1)[:, permutation]\n",
    "\n",
    "x_train_final = x_train_reshape[:, :split_index]\n",
    "y_train_final = y_train_reshape[:, :split_index]\n",
    "\n",
    "x_test_local = x_train_reshape[:, split_index:N]\n",
    "y_test_local = y_train_reshape[:, split_index:N]\n",
    "\n",
    "x_train_final.shape, y_train_final.shape, x_test_local.shape, y_test_local.shape, y_train_final.shape, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26a0290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eww why does python not have inline funcs\n",
    "def ReLU(Z):\n",
    "    return np.maximum(0, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e36aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i don't import binary_cross_entropy_loss from models, since it is not suitable for neuralnet\n",
    "def binary_cross_entropy_loss(Y, A):\n",
    "    # Y: (1, N)\n",
    "    # A: (1, N)\n",
    "    m = Y.shape[1]\n",
    "    epsilon = 1e-9\n",
    "    cost = -(1/m) * np.sum( Y * np.log(A + epsilon) + (1 - Y) * np.log(1 - A + epsilon) )\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b39237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import sigmoid\n",
    "\n",
    "class NeuralNet:\n",
    "    \n",
    "    # Output size will be 1 given it is a binary classification\n",
    "    def __init__(self):\n",
    "        self.layer_dims = None\n",
    "        self.L = None\n",
    "        self.learning_rate = None\n",
    "        self.batch_size = None\n",
    "        self.params = {}\n",
    "        self.patience = None\n",
    "        self.hyperparameter_log = []\n",
    "        \n",
    "    # note, X assumes shape (D, N) to perform multiple at the same time, returns logits of shape (1, N)\n",
    "    def forward_propagation(self, X):\n",
    "        cache = [] # need to store the history of all activations for backprop\n",
    "\n",
    "        A = X # since the first layer is already normalised\n",
    "        Z = X # just tmp\n",
    "\n",
    "        for i in range(1, self.L):\n",
    "            A_prev = A\n",
    "            Z = np.dot(self.params[f\"W{i}\"], A_prev) + self.params[f\"b{i}\"]\n",
    "            cache.append((A_prev, self.params[f\"W{i}\"], Z))\n",
    "            if (i != self.L - 1): A = ReLU(Z)\n",
    "\n",
    "        return Z, cache\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # X: (D, N)\n",
    "        # output: (1, N)\n",
    "\n",
    "        Z, _ = self.forward_propagation(X)\n",
    "        return (Z > 0).astype(int)\n",
    "    \n",
    "    def reinitialise_params(self):\n",
    "        self.params = {}\n",
    "        for i in range(1, self.L): # start from 1, since 0 is input layer\n",
    "                # Each column in weights matrix represents weights for one neuron, and small randomisation is necessary to break symmetry\n",
    "                self.params[f\"W{i}\"] = np.random.randn(self.layer_dims[i], self.layer_dims[i-1]) * 0.01 # shape (curr, prev)\n",
    "                self.params[f\"b{i}\"] = np.zeros((self.layer_dims[i], 1)) # shape (curr, 1)\n",
    "\n",
    "    def hyperparameter_tuning(self, X, y, metric, verbose=False):\n",
    "        D = X.shape[0]\n",
    "\n",
    "        # In layer_dims, we have [input_size, hidden_sizes ..., output_size]\n",
    "        self.layer_dims = [D, 16, 8, 1]\n",
    "        self.L = len(self.layer_dims)\n",
    "        self.learning_rate = 0.01\n",
    "\n",
    "        layers = [[D, 128, 1], [D, 16, 8, 1], [D, 32, 16, 1]]\n",
    "        batch_sizes = [16, 32]\n",
    "        learning_rates = [0.01, 0.001]\n",
    "        patiences = [3, 5, 10]\n",
    "\n",
    "        for layer in layers:\n",
    "            self.layer_dims = layer\n",
    "            self.L = len(self.layer_dims)\n",
    "            self.reinitialise_params()\n",
    "\n",
    "            for batch_size in batch_sizes:\n",
    "                self.batch_size = batch_size\n",
    "\n",
    "                for learning_rate in learning_rates:\n",
    "                    self.learning_rate = learning_rate\n",
    "\n",
    "                    for patience in patiences:\n",
    "                        self.patience = patience\n",
    "\n",
    "                        print(f\"layers: {self.layer_dims}, batch size: {self.batch_size}, learning rate: {self.learning_rate}, patience: {self.patience}\")\n",
    "                        costs, test_costs, _ = self.train(X, y)\n",
    "\n",
    "                        y_pred = self.predict(X)\n",
    "                        f1 = f_score(y, y_pred)\n",
    "\n",
    "                        self.hyperparameter_log.append({\n",
    "                            # \"final_test_cost\": test_costs[-1], # accidentally optimised for cost\n",
    "                            \"f1\": f1,\n",
    "                            \"layers\": self.layer_dims,\n",
    "                            \"batch_size\": self.batch_size,\n",
    "                            \"learning_rate\": self.learning_rate,\n",
    "                            \"patience\": self.patience,\n",
    "                        })\n",
    "\n",
    "                        self.reinitialise_params()\n",
    "\n",
    "        self.hyperparameter_log.sort(key=lambda x: x[\"f1\"])\n",
    "        best_hyperparams = self.hyperparameter_log[-1] \n",
    "\n",
    "        self.layer_dims = best_hyperparams[\"layers\"]\n",
    "        self.L = len(self.layer_dims)\n",
    "        self.reinitialise_params()\n",
    "        self.learning_rate = best_hyperparams[\"learning_rate\"]\n",
    "        self.batch_size = best_hyperparams[\"batch_size\"]\n",
    "        self.patience = best_hyperparams[\"patience\"]\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        # X: (D, N)\n",
    "        # Y: (1, N)\n",
    "        N = X.shape[1]\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        # oversampling the minority class due to the bias in the dataset\n",
    "        ratio = int(floor(np.sum(Y == 0) / np.sum(Y == 1)))\n",
    "        mask_0 = (Y[0] == 0)\n",
    "        mask_1 = (Y[0] == 1)\n",
    "        x_zeros = X[:, mask_0]\n",
    "        x_ones = X[:, mask_1]\n",
    "        y_zeros = Y[:, mask_0]\n",
    "        y_ones = Y[:, mask_1]\n",
    "        x_ones_oversampled = np.tile(x_ones, (1, ratio))\n",
    "        y_ones_oversampled = np.tile(y_ones, (1, ratio))\n",
    "        X = np.concatenate([x_zeros, x_ones_oversampled], axis=1)\n",
    "        Y = np.concatenate([y_zeros, y_ones_oversampled], axis=1)\n",
    "\n",
    "        # need to do another permutation so it is not just ordered as zeros then ones\n",
    "        train_permutation = np.random.permutation(X.shape[1])\n",
    "        X = X[:, train_permutation]\n",
    "        Y = Y[:, train_permutation]\n",
    "\n",
    "        cost_sample_rate_in_epochs = 1\n",
    "        costs = []\n",
    "        test_costs = []\n",
    "\n",
    "        max_epochs= 100\n",
    "        patience = self.patience # how many epochs to wait before giving up\n",
    "        patience_counter = 0 \n",
    "\n",
    "        for i in range(max_epochs): \n",
    "            # need to do some shuffling to stop memorisation and overfitting\n",
    "            permutation = np.random.permutation(N)\n",
    "            X_shuffled = X[:, permutation]\n",
    "            Y_shuffled = Y[:, permutation]\n",
    "\n",
    "            for j in range(int(np.ceil(N/batch_size))):\n",
    "                start = j * batch_size\n",
    "                end = min((j + 1) * batch_size, N)\n",
    "\n",
    "                X_batch = X_shuffled[:, start:end]\n",
    "                Y_batch = Y_shuffled[:, start:end]\n",
    "                real_batch_size = end - start\n",
    "\n",
    "                # 1. for each batch perform forward propagation (we don't need to find the cross entropy loss since it works with derivatives)\n",
    "                Z, cache = self.forward_propagation(X_batch)\n",
    "                A = sigmoid(Z) \n",
    "                # C = binary_cross_entropy_loss(Y_batch, A)\n",
    "\n",
    "                dA_prev = None\n",
    "                for l in range(self.L - 1, 0, -1):\n",
    "                    # 2. backprop formulas condensed\n",
    "                    A_prev, W, Z = cache[l-1]\n",
    "\n",
    "                    is_last_layer = l == self.L - 1\n",
    "                    dZ = A - Y_batch if is_last_layer else dA_prev * (Z > 0).astype(int) # this last term is the derivate (kind of) of ReLU\n",
    "                    dW = (1 / real_batch_size) * np.dot(dZ, A_prev.T)\n",
    "                    db = (1 / real_batch_size) * np.sum(dZ, axis=1, keepdims=True)\n",
    "                    \n",
    "                    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "                    # 3. Update weights and biases of the network\n",
    "                    self.params[f\"W{l}\"] = self.params[f\"W{l}\"] - self.learning_rate * dW\n",
    "                    self.params[f\"b{l}\"] = self.params[f\"b{l}\"] - self.learning_rate * db\n",
    "                \n",
    "            # Doing some printing and tracking off cost.\n",
    "            if i % cost_sample_rate_in_epochs == 0: # i just decided to print every epoch ... so it doesn't really make sense to have in if-statement, but just in-case\n",
    "                Z, _ = self.forward_propagation(X) \n",
    "                A = sigmoid(Z)\n",
    "                cost = binary_cross_entropy_loss(Y, A)\n",
    "                costs.append(cost)\n",
    "                print(f\"Epoch {i}, Cost: {cost}\")\n",
    "\n",
    "                Z, _ = self.forward_propagation(x_test_local)\n",
    "                A = sigmoid(Z)\n",
    "                test_cost = binary_cross_entropy_loss(y_test_local, A)\n",
    "                test_costs.append(test_cost)\n",
    "                print(f\".          Test Cost: {test_cost}\")\n",
    "\n",
    "            if test_cost < min(test_costs):\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience: # give up if no improvement \n",
    "                    break\n",
    "\n",
    "        return costs, test_costs, cost_sample_rate_in_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9db502",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNet()\n",
    "nn.hyperparameter_tuning(x_train_final, y_train_final, metric=\"loss\", verbose=True)\n",
    "nn.train(x_train_final, y_train_final)\n",
    "predictions = nn.predict(x_submission_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5bace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from helpers import create_csv_submission\n",
    "\n",
    "# Convert to -1 and 1\n",
    "predictions = np.where(predictions == 1, 1, -1)\n",
    "\n",
    "# Create submission file\n",
    "os.makedirs(\"data/submissions\", exist_ok=True)\n",
    "create_csv_submission(test_ids, predictions, f\"data/submissions/neural_network.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9c49f1",
   "metadata": {},
   "source": [
    "# ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d71fda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = x_train_final.shape[0]\n",
    "\n",
    "nn = NeuralNet([D, 12, 8, 1], learning_rate=0.001)\n",
    "costs, test_costs, cost_sample_rate_in_epochs = nn.train(x_train_final, y_train_final, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0cbad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1, len(costs) + 1) * cost_sample_rate_in_epochs\n",
    "plt.plot(epochs, costs, marker='o', linestyle='-', label='Train Cost')\n",
    "plt.plot(epochs, test_costs, marker='x', linestyle='--', label='Test Cost')\n",
    "\n",
    "plt.title('Training & Test Cost vs. Epochs')\n",
    "plt.xlabel(f'Epoch (Sampled every {cost_sample_rate_in_epochs})')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b058ea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nn.predict(x_test_local)\n",
    "print(\"F1 Score:\", f_score(y_test_local, y_pred))\n",
    "y_pred.shape, y_test_local.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
