{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8ff706e",
   "metadata": {},
   "source": [
    "# 005: Cross Validate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8110fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from models import OrdinaryLeastSquares, LogisticRegression, LinearSVM, KNearestNeighbors\n",
    "from model_selection import cross_validation\n",
    "from preprocessing import preprocess, preprocess_splits\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def print_results(cv_results):\n",
    "    print(f\"Average F1-score: {np.mean(cv_results.f1_scores)*100:.1f}% ± {np.std(cv_results.f1_scores)*100:.1f}\")\n",
    "    print(f\"Average F2-score: {np.mean(cv_results.f2_scores)*100:.1f}% ± {np.std(cv_results.f2_scores)*100:.1f}\")\n",
    "    print(f\"Average AUC-ROC: {np.mean(cv_results.auc_rocs)*100:.1f}% ± {np.std(cv_results.auc_rocs)*100:.1f}\")\n",
    "    print(f\" & {np.mean(cv_results.f1_scores)*100:.1f}±{np.std(cv_results.f1_scores)*100:.1f} & \"\n",
    "          f\"{np.mean(cv_results.f2_scores)*100:.1f}±{np.std(cv_results.f2_scores)*100:.1f} & \"\n",
    "          f\"{np.mean(cv_results.auc_rocs)*100:.1f}±{np.std(cv_results.auc_rocs)*100:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f45de3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data...\n",
      "Replacing missing value codes with np.nan...\n",
      "Applying one-hot encoding...\n",
      "Saving preprocessed data to ../data/dataset_prep...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(328135, 622)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, _, y_train, *_ = preprocess(save_dir=\"../data/dataset_prep\")\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b58a9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328135, 622)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = np.load(\"../data/dataset_prep/train.npz\")\n",
    "x_train, y_train = train[\"x_train\"], train[\"y_train\"]\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1508ff7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing missing value codes with np.nan...\n"
     ]
    }
   ],
   "source": [
    "x_train, _, y_train, *_ = preprocess(one_hot_encoding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06903609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validating model: LogisticRegression\n",
      "Starting fold 1/5 with 80000 samples\n",
      "Evaluating lambda=0\n",
      "Iter    0: loss=0.6931\n",
      "Iter   10: loss=0.6698\n",
      "Iter   20: loss=0.6488\n",
      "Iter   30: loss=0.6265\n",
      "Iter   40: loss=0.6072\n",
      "Iter   50: loss=0.5909\n",
      "Iter   60: loss=0.5771\n",
      "Iter   70: loss=0.5656\n",
      "Iter   80: loss=0.5557\n",
      "Iter   90: loss=0.5473\n",
      "Iter  100: loss=0.5401\n",
      "Iter  110: loss=0.5339\n",
      "Iter  120: loss=0.5285\n",
      "Iter  130: loss=0.5238\n",
      "Iter  140: loss=0.5196\n",
      "Iter  150: loss=0.5160\n",
      "Iter  160: loss=0.5127\n",
      "Iter  170: loss=0.5099\n",
      "Iter  180: loss=0.5073\n",
      "Iter  190: loss=0.5050\n",
      "Iter  200: loss=0.5030\n",
      "Iter  210: loss=0.5011\n",
      "Iter  220: loss=0.4994\n",
      "Iter  230: loss=0.4979\n",
      "Iter  240: loss=0.4965\n",
      "Iter  250: loss=0.4953\n",
      "Iter  260: loss=0.4941\n",
      "Iter  270: loss=0.4931\n",
      "Iter  280: loss=0.4921\n",
      "Iter  290: loss=0.4913\n",
      "Iter  300: loss=0.4904\n",
      "Iter  310: loss=0.4897\n",
      "Iter  320: loss=0.4890\n",
      "Iter  330: loss=0.4884\n",
      "Iter  340: loss=0.4878\n",
      "Iter  350: loss=0.4873\n",
      "Iter  360: loss=0.4868\n",
      "Iter  370: loss=0.4863\n",
      "Iter  380: loss=0.4859\n",
      "Iter  390: loss=0.4855\n",
      "Iter  400: loss=0.4851\n",
      "Iter  410: loss=0.4847\n",
      "Iter  420: loss=0.4844\n",
      "Iter  430: loss=0.4841\n",
      "Iter  440: loss=0.4838\n",
      "Iter  450: loss=0.4835\n",
      "Iter  460: loss=0.4833\n",
      "Iter  470: loss=0.4830\n",
      "Iter  480: loss=0.4828\n",
      "Iter  490: loss=0.4826\n",
      "Iter  500: loss=0.4824\n",
      "Iter  510: loss=0.4822\n",
      "Iter  520: loss=0.4820\n",
      "Iter  530: loss=0.4819\n",
      "Iter  540: loss=0.4817\n",
      "Iter  550: loss=0.4816\n",
      "Iter  560: loss=0.4814\n",
      "Iter  570: loss=0.4813\n",
      "Iter  580: loss=0.4812\n",
      "Iter  590: loss=0.4810\n",
      "Iter  600: loss=0.4809\n",
      "Iter  610: loss=0.4808\n",
      "Iter  620: loss=0.4807\n",
      "Iter  630: loss=0.4806\n",
      "Iter  640: loss=0.4805\n",
      "Iter  650: loss=0.4804\n",
      "Iter  660: loss=0.4803\n",
      "Iter  670: loss=0.4802\n",
      "Iter  680: loss=0.4802\n",
      "Iter  690: loss=0.4801\n",
      "Iter  700: loss=0.4800\n",
      "Iter  710: loss=0.4800\n",
      "Iter  720: loss=0.4799\n",
      "Iter  730: loss=0.4798\n",
      "Iter  740: loss=0.4798\n",
      "Iter  750: loss=0.4797\n",
      "Iter  760: loss=0.4797\n",
      "Iter  770: loss=0.4796\n",
      "Iter  780: loss=0.4795\n",
      "Iter  790: loss=0.4795\n",
      "Iter  800: loss=0.4794\n",
      "Iter  810: loss=0.4794\n",
      "Iter  820: loss=0.4794\n",
      "Iter  830: loss=0.4793\n",
      "Iter  840: loss=0.4793\n",
      "Iter  850: loss=0.4792\n",
      "Iter  860: loss=0.4792\n",
      "Early stopping at iteration 867\n",
      "train loss=0.47107203392720703\n",
      "val loss=0.47926671499624834\n",
      "Best threshold: 0.72 with score 0.41723110268219993\n",
      "Fold 1 - F1: 0.418066768492254, F2: 0.4922412907203782, AUC-ROC: 0.8540173137225204\n",
      "Starting fold 2/5 with 80000 samples\n",
      "Evaluating lambda=0\n",
      "Iter    0: loss=0.6931\n",
      "Iter   10: loss=0.6724\n",
      "Iter   20: loss=0.6511\n",
      "Iter   30: loss=0.6286\n",
      "Iter   40: loss=0.6091\n",
      "Iter   50: loss=0.5928\n",
      "Iter   60: loss=0.5791\n",
      "Iter   70: loss=0.5675\n",
      "Iter   80: loss=0.5577\n",
      "Iter   90: loss=0.5494\n",
      "Iter  100: loss=0.5422\n",
      "Iter  110: loss=0.5360\n",
      "Iter  120: loss=0.5307\n",
      "Iter  130: loss=0.5260\n",
      "Iter  140: loss=0.5219\n",
      "Iter  150: loss=0.5183\n",
      "Iter  160: loss=0.5151\n",
      "Iter  170: loss=0.5122\n",
      "Iter  180: loss=0.5097\n",
      "Iter  190: loss=0.5074\n",
      "Iter  200: loss=0.5054\n",
      "Iter  210: loss=0.5036\n",
      "Iter  220: loss=0.5019\n",
      "Iter  230: loss=0.5004\n",
      "Iter  240: loss=0.4991\n",
      "Iter  250: loss=0.4979\n",
      "Iter  260: loss=0.4967\n",
      "Iter  270: loss=0.4957\n",
      "Iter  280: loss=0.4948\n",
      "Iter  290: loss=0.4939\n",
      "Iter  300: loss=0.4931\n",
      "Iter  310: loss=0.4924\n",
      "Iter  320: loss=0.4917\n",
      "Iter  330: loss=0.4911\n",
      "Iter  340: loss=0.4906\n",
      "Iter  350: loss=0.4900\n",
      "Iter  360: loss=0.4895\n",
      "Iter  370: loss=0.4891\n",
      "Iter  380: loss=0.4887\n",
      "Iter  390: loss=0.4883\n",
      "Iter  400: loss=0.4879\n",
      "Iter  410: loss=0.4876\n",
      "Iter  420: loss=0.4873\n",
      "Iter  430: loss=0.4870\n",
      "Iter  440: loss=0.4867\n",
      "Iter  450: loss=0.4864\n",
      "Iter  460: loss=0.4862\n",
      "Iter  470: loss=0.4860\n",
      "Iter  480: loss=0.4857\n",
      "Iter  490: loss=0.4855\n",
      "Iter  500: loss=0.4853\n",
      "Iter  510: loss=0.4852\n",
      "Iter  520: loss=0.4850\n",
      "Iter  530: loss=0.4848\n",
      "Iter  540: loss=0.4847\n",
      "Iter  550: loss=0.4845\n",
      "Iter  560: loss=0.4844\n",
      "Iter  570: loss=0.4843\n",
      "Iter  580: loss=0.4842\n",
      "Iter  590: loss=0.4841\n",
      "Iter  600: loss=0.4839\n",
      "Iter  610: loss=0.4838\n",
      "Iter  620: loss=0.4837\n",
      "Iter  630: loss=0.4837\n",
      "Iter  640: loss=0.4836\n",
      "Iter  650: loss=0.4835\n",
      "Iter  660: loss=0.4834\n",
      "Iter  670: loss=0.4833\n",
      "Iter  680: loss=0.4832\n",
      "Iter  690: loss=0.4832\n",
      "Iter  700: loss=0.4831\n",
      "Iter  710: loss=0.4830\n",
      "Iter  720: loss=0.4830\n",
      "Iter  730: loss=0.4829\n",
      "Iter  740: loss=0.4829\n",
      "Iter  750: loss=0.4828\n",
      "Iter  760: loss=0.4828\n",
      "Iter  770: loss=0.4827\n",
      "Iter  780: loss=0.4827\n",
      "Iter  790: loss=0.4826\n",
      "Iter  800: loss=0.4826\n",
      "Iter  810: loss=0.4825\n",
      "Iter  820: loss=0.4825\n",
      "Iter  830: loss=0.4825\n",
      "Early stopping at iteration 838\n",
      "train loss=0.47140409368078484\n",
      "val loss=0.4825306279528769\n",
      "Best threshold: 0.725 with score 0.42379788101059496\n",
      "Fold 2 - F1: 0.42552255225522556, F2: 0.4898682877406281, AUC-ROC: 0.8553535578353524\n",
      "Starting fold 3/5 with 80000 samples\n",
      "Evaluating lambda=0\n",
      "Iter    0: loss=0.6931\n",
      "Iter   10: loss=0.6702\n",
      "Iter   20: loss=0.6491\n",
      "Iter   30: loss=0.6266\n",
      "Iter   40: loss=0.6072\n",
      "Iter   50: loss=0.5909\n",
      "Iter   60: loss=0.5772\n",
      "Iter   70: loss=0.5657\n",
      "Iter   80: loss=0.5559\n",
      "Iter   90: loss=0.5476\n",
      "Iter  100: loss=0.5404\n",
      "Iter  110: loss=0.5343\n",
      "Iter  120: loss=0.5289\n",
      "Iter  130: loss=0.5243\n",
      "Iter  140: loss=0.5202\n",
      "Iter  150: loss=0.5166\n",
      "Iter  160: loss=0.5134\n",
      "Iter  170: loss=0.5106\n",
      "Iter  180: loss=0.5081\n",
      "Iter  190: loss=0.5058\n",
      "Iter  200: loss=0.5038\n",
      "Iter  210: loss=0.5020\n",
      "Iter  220: loss=0.5004\n",
      "Iter  230: loss=0.4989\n",
      "Iter  240: loss=0.4976\n",
      "Iter  250: loss=0.4963\n",
      "Iter  260: loss=0.4952\n",
      "Iter  270: loss=0.4942\n",
      "Iter  280: loss=0.4933\n",
      "Iter  290: loss=0.4924\n",
      "Iter  300: loss=0.4917\n",
      "Iter  310: loss=0.4910\n",
      "Iter  320: loss=0.4903\n",
      "Iter  330: loss=0.4897\n",
      "Iter  340: loss=0.4891\n",
      "Iter  350: loss=0.4886\n",
      "Iter  360: loss=0.4881\n",
      "Iter  370: loss=0.4877\n",
      "Iter  380: loss=0.4873\n",
      "Iter  390: loss=0.4869\n",
      "Iter  400: loss=0.4865\n",
      "Iter  410: loss=0.4862\n",
      "Iter  420: loss=0.4859\n",
      "Iter  430: loss=0.4856\n",
      "Iter  440: loss=0.4853\n",
      "Iter  450: loss=0.4851\n",
      "Iter  460: loss=0.4848\n",
      "Iter  470: loss=0.4846\n",
      "Iter  480: loss=0.4844\n",
      "Iter  490: loss=0.4842\n",
      "Iter  500: loss=0.4840\n",
      "Iter  510: loss=0.4838\n",
      "Iter  520: loss=0.4837\n",
      "Iter  530: loss=0.4835\n",
      "Iter  540: loss=0.4834\n",
      "Iter  550: loss=0.4832\n",
      "Iter  560: loss=0.4831\n",
      "Iter  570: loss=0.4830\n",
      "Iter  580: loss=0.4828\n",
      "Iter  590: loss=0.4827\n",
      "Iter  600: loss=0.4826\n",
      "Iter  610: loss=0.4825\n",
      "Iter  620: loss=0.4824\n",
      "Iter  630: loss=0.4823\n",
      "Iter  640: loss=0.4822\n",
      "Iter  650: loss=0.4822\n",
      "Iter  660: loss=0.4821\n",
      "Iter  670: loss=0.4820\n",
      "Iter  680: loss=0.4819\n",
      "Iter  690: loss=0.4819\n",
      "Iter  700: loss=0.4818\n",
      "Iter  710: loss=0.4817\n",
      "Iter  720: loss=0.4817\n",
      "Iter  730: loss=0.4816\n",
      "Iter  740: loss=0.4816\n",
      "Iter  750: loss=0.4815\n",
      "Iter  760: loss=0.4815\n",
      "Iter  770: loss=0.4814\n",
      "Iter  780: loss=0.4814\n",
      "Iter  790: loss=0.4813\n",
      "Iter  800: loss=0.4813\n",
      "Iter  810: loss=0.4812\n",
      "Iter  820: loss=0.4812\n",
      "Iter  830: loss=0.4811\n",
      "Iter  840: loss=0.4811\n",
      "Iter  850: loss=0.4811\n",
      "Early stopping at iteration 853\n",
      "train loss=0.4738972942507759\n",
      "val loss=0.481152788970019\n",
      "Best threshold: 0.735 with score 0.42504211117349805\n",
      "Fold 3 - F1: 0.4192139737991266, F2: 0.4752475247524752, AUC-ROC: 0.8594535704309512\n",
      "Starting fold 4/5 with 80000 samples\n",
      "Evaluating lambda=0\n",
      "Iter    0: loss=0.6931\n",
      "Iter   10: loss=0.6686\n",
      "Iter   20: loss=0.6478\n",
      "Iter   30: loss=0.6257\n",
      "Iter   40: loss=0.6066\n",
      "Iter   50: loss=0.5905\n",
      "Iter   60: loss=0.5770\n",
      "Iter   70: loss=0.5657\n",
      "Iter   80: loss=0.5560\n",
      "Iter   90: loss=0.5478\n",
      "Iter  100: loss=0.5408\n",
      "Iter  110: loss=0.5347\n",
      "Iter  120: loss=0.5294\n",
      "Iter  130: loss=0.5248\n",
      "Iter  140: loss=0.5208\n",
      "Iter  150: loss=0.5173\n",
      "Iter  160: loss=0.5141\n",
      "Iter  170: loss=0.5114\n",
      "Iter  180: loss=0.5089\n",
      "Iter  190: loss=0.5067\n",
      "Iter  200: loss=0.5047\n",
      "Iter  210: loss=0.5029\n",
      "Iter  220: loss=0.5013\n",
      "Iter  230: loss=0.4999\n",
      "Iter  240: loss=0.4986\n",
      "Iter  250: loss=0.4974\n",
      "Iter  260: loss=0.4963\n",
      "Iter  270: loss=0.4953\n",
      "Iter  280: loss=0.4944\n",
      "Iter  290: loss=0.4936\n",
      "Iter  300: loss=0.4928\n",
      "Iter  310: loss=0.4921\n",
      "Iter  320: loss=0.4915\n",
      "Iter  330: loss=0.4909\n",
      "Iter  340: loss=0.4903\n",
      "Iter  350: loss=0.4898\n",
      "Iter  360: loss=0.4894\n",
      "Iter  370: loss=0.4889\n",
      "Iter  380: loss=0.4885\n",
      "Iter  390: loss=0.4882\n",
      "Iter  400: loss=0.4878\n",
      "Iter  410: loss=0.4875\n",
      "Iter  420: loss=0.4872\n",
      "Iter  430: loss=0.4869\n",
      "Iter  440: loss=0.4867\n",
      "Iter  450: loss=0.4864\n",
      "Iter  460: loss=0.4862\n",
      "Iter  470: loss=0.4860\n",
      "Iter  480: loss=0.4858\n",
      "Iter  490: loss=0.4856\n",
      "Iter  500: loss=0.4854\n",
      "Iter  510: loss=0.4852\n",
      "Iter  520: loss=0.4851\n",
      "Iter  530: loss=0.4849\n",
      "Iter  540: loss=0.4848\n",
      "Iter  550: loss=0.4846\n",
      "Iter  560: loss=0.4845\n",
      "Iter  570: loss=0.4844\n",
      "Iter  580: loss=0.4843\n",
      "Iter  590: loss=0.4842\n",
      "Iter  600: loss=0.4841\n",
      "Iter  610: loss=0.4840\n",
      "Iter  620: loss=0.4839\n",
      "Iter  630: loss=0.4838\n",
      "Iter  640: loss=0.4837\n",
      "Iter  650: loss=0.4836\n",
      "Iter  660: loss=0.4836\n",
      "Iter  670: loss=0.4835\n",
      "Iter  680: loss=0.4834\n",
      "Iter  690: loss=0.4834\n",
      "Iter  700: loss=0.4833\n",
      "Iter  710: loss=0.4832\n",
      "Iter  720: loss=0.4832\n",
      "Iter  730: loss=0.4831\n",
      "Iter  740: loss=0.4831\n",
      "Iter  750: loss=0.4830\n",
      "Iter  760: loss=0.4830\n",
      "Iter  770: loss=0.4829\n",
      "Iter  780: loss=0.4829\n",
      "Iter  790: loss=0.4829\n",
      "Iter  800: loss=0.4828\n",
      "Iter  810: loss=0.4828\n",
      "Iter  820: loss=0.4827\n",
      "Early stopping at iteration 822\n",
      "train loss=0.4746109646893871\n",
      "val loss=0.48283374883508795\n",
      "Best threshold: 0.765 with score 0.42996941896024465\n",
      "Fold 4 - F1: 0.42012269938650304, F2: 0.45459373340414233, AUC-ROC: 0.8605667928810173\n",
      "Starting fold 5/5 with 80000 samples\n",
      "Evaluating lambda=0\n",
      "Iter    0: loss=0.6931\n",
      "Iter   10: loss=0.6695\n",
      "Iter   20: loss=0.6491\n",
      "Iter   30: loss=0.6272\n",
      "Iter   40: loss=0.6082\n",
      "Iter   50: loss=0.5922\n",
      "Iter   60: loss=0.5788\n",
      "Iter   70: loss=0.5675\n",
      "Iter   80: loss=0.5579\n",
      "Iter   90: loss=0.5497\n",
      "Iter  100: loss=0.5427\n",
      "Iter  110: loss=0.5366\n",
      "Iter  120: loss=0.5313\n",
      "Iter  130: loss=0.5267\n",
      "Iter  140: loss=0.5227\n",
      "Iter  150: loss=0.5192\n",
      "Iter  160: loss=0.5160\n",
      "Iter  170: loss=0.5132\n",
      "Iter  180: loss=0.5108\n",
      "Iter  190: loss=0.5085\n",
      "Iter  200: loss=0.5065\n",
      "Iter  210: loss=0.5047\n",
      "Iter  220: loss=0.5031\n",
      "Iter  230: loss=0.5017\n",
      "Iter  240: loss=0.5003\n",
      "Iter  250: loss=0.4991\n",
      "Iter  260: loss=0.4980\n",
      "Iter  270: loss=0.4970\n",
      "Iter  280: loss=0.4961\n",
      "Iter  290: loss=0.4953\n",
      "Iter  300: loss=0.4945\n",
      "Iter  310: loss=0.4938\n",
      "Iter  320: loss=0.4931\n",
      "Iter  330: loss=0.4925\n",
      "Iter  340: loss=0.4919\n",
      "Iter  350: loss=0.4914\n",
      "Iter  360: loss=0.4909\n",
      "Iter  370: loss=0.4905\n",
      "Iter  380: loss=0.4901\n",
      "Iter  390: loss=0.4897\n",
      "Iter  400: loss=0.4893\n",
      "Iter  410: loss=0.4890\n",
      "Iter  420: loss=0.4887\n",
      "Iter  430: loss=0.4884\n",
      "Iter  440: loss=0.4881\n",
      "Iter  450: loss=0.4879\n",
      "Iter  460: loss=0.4876\n",
      "Iter  470: loss=0.4874\n",
      "Iter  480: loss=0.4872\n",
      "Iter  490: loss=0.4870\n",
      "Iter  500: loss=0.4868\n",
      "Iter  510: loss=0.4866\n",
      "Iter  520: loss=0.4865\n",
      "Iter  530: loss=0.4863\n",
      "Iter  540: loss=0.4861\n",
      "Iter  550: loss=0.4860\n",
      "Iter  560: loss=0.4859\n",
      "Iter  570: loss=0.4857\n",
      "Iter  580: loss=0.4856\n",
      "Iter  590: loss=0.4855\n",
      "Iter  600: loss=0.4854\n",
      "Iter  610: loss=0.4853\n",
      "Iter  620: loss=0.4852\n",
      "Iter  630: loss=0.4851\n",
      "Iter  640: loss=0.4850\n",
      "Iter  650: loss=0.4849\n",
      "Iter  660: loss=0.4848\n",
      "Iter  670: loss=0.4848\n",
      "Iter  680: loss=0.4847\n",
      "Iter  690: loss=0.4846\n",
      "Iter  700: loss=0.4846\n",
      "Iter  710: loss=0.4845\n",
      "Iter  720: loss=0.4844\n",
      "Iter  730: loss=0.4844\n",
      "Iter  740: loss=0.4843\n",
      "Iter  750: loss=0.4843\n",
      "Iter  760: loss=0.4842\n",
      "Iter  770: loss=0.4842\n",
      "Iter  780: loss=0.4841\n",
      "Iter  790: loss=0.4841\n",
      "Iter  800: loss=0.4840\n",
      "Iter  810: loss=0.4840\n",
      "Iter  820: loss=0.4839\n",
      "Iter  830: loss=0.4839\n",
      "Early stopping at iteration 836\n",
      "train loss=0.47313587879644253\n",
      "val loss=0.4839826656707112\n",
      "Best threshold: 0.755 with score 0.43270072992700725\n",
      "Fold 5 - F1: 0.41873943491910165, F2: 0.4573749736231273, AUC-ROC: 0.8526726029633234\n",
      "Average F1-score: 42.0% ± 0.3\n",
      "Average F2-score: 47.4% ± 1.6\n",
      "Average AUC-ROC: 85.6% ± 0.3\n",
      " & 42.0±0.3 & 47.4±1.6 & 85.6±0.3\n"
     ]
    }
   ],
   "source": [
    "model_settings = [\n",
    "    #{\"model_class\": OrdinaryLeastSquares},\n",
    "    {\"model_class\": LogisticRegression},\n",
    "    #{\"model_class\": LinearSVM},\n",
    "    #{\"model_class\": KNearestNeighbors},\n",
    "]\n",
    "for model in model_settings:\n",
    "    print(f\"Cross-validating model: {model['model_class'].__name__}\")\n",
    "    num_samples = int(1e6) if model['model_class'] != KNearestNeighbors else int(1e5)\n",
    "    cv_results = cross_validation(x_train[:num_samples][:,:321], y_train[:num_samples], verbose=True, **model)\n",
    "    print_results(cv_results)\n",
    "    if num_samples != 1e6: break\n",
    "    out_dir = \"../results\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_path = os.path.join(out_dir, f\"{model['model_class'].__name__}.txt\")\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(f\"Model: {model['model_class'].__name__}\\n\\n\")\n",
    "        f.write(\"cv_results repr:\\n\")\n",
    "        f.write(repr(cv_results))\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.write(\"Aggregated metrics:\\n\")\n",
    "        f.write(f\"Average F1-score: {np.mean(cv_results.f1_scores)*100:.1f}% ± {np.std(cv_results.f1_scores)*100:.1f}\\n\")\n",
    "        f.write(f\"Average F2-score: {np.mean(cv_results.f2_scores)*100:.1f}% ± {np.std(cv_results.f2_scores)*100:.1f}\\n\")\n",
    "        f.write(f\"Average AUC-ROC: {np.mean(cv_results.auc_rocs)*100:.1f}% ± {np.std(cv_results.auc_rocs)*100:.1f}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81032daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "& 41.8±0.2 & 48.7±0.7 & 85.4±0.4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
