{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff5bbcc",
   "metadata": {},
   "source": [
    "# 010: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9352bc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from metrics import f_score, auc_roc\n",
    "from models import LinearSVM\n",
    "from visualizations import plot_roc, plot_losses\n",
    "from implementations import sigmoid\n",
    "from model_selection import test_val_split, find_best_threshold\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed563196",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load(\"../data/dataset_prep/train.npz\")\n",
    "x_train = train[\"x_train\"]\n",
    "y_train = train[\"y_train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a694d222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Training Loss: 1.0\n",
      "Iteration 1, Training Loss: 0.9887626467494214\n",
      "Iteration 2, Training Loss: 0.9775533868819688\n",
      "Iteration 3, Training Loss: 0.9663721501641853\n",
      "Iteration 4, Training Loss: 0.9552188665381959\n",
      "Iteration 5, Training Loss: 0.9440934661212717\n",
      "Iteration 6, Training Loss: 0.9329958792053896\n",
      "Iteration 7, Training Loss: 0.9219260362567974\n",
      "Iteration 8, Training Loss: 0.9108838679155769\n",
      "Iteration 9, Training Loss: 0.8998693049952089\n",
      "Iteration 10, Training Loss: 0.8888822784821423\n",
      "Iteration 11, Training Loss: 0.8779227195353582\n",
      "Iteration 12, Training Loss: 0.8669922689705206\n",
      "Iteration 13, Training Loss: 0.8561005615755009\n",
      "Iteration 14, Training Loss: 0.8452570790581357\n",
      "Iteration 15, Training Loss: 0.8344700096837784\n",
      "Iteration 16, Training Loss: 0.8237528293471637\n",
      "Iteration 17, Training Loss: 0.8131275544442321\n",
      "Iteration 18, Training Loss: 0.8026146509530293\n",
      "Iteration 19, Training Loss: 0.7922255365779246\n",
      "Iteration 20, Training Loss: 0.7819694913912052\n",
      "Iteration 21, Training Loss: 0.7718475906951912\n",
      "Iteration 22, Training Loss: 0.7618886641452022\n",
      "Iteration 23, Training Loss: 0.7521084324671237\n",
      "Iteration 24, Training Loss: 0.7425791897932108\n",
      "Iteration 25, Training Loss: 0.7333822033556499\n",
      "Iteration 26, Training Loss: 0.7245791122485589\n",
      "Iteration 27, Training Loss: 0.7161990920136241\n",
      "Iteration 28, Training Loss: 0.708234337809593\n",
      "Iteration 29, Training Loss: 0.7006630703005923\n",
      "Iteration 30, Training Loss: 0.6934471421255495\n",
      "Iteration 31, Training Loss: 0.686522943575402\n",
      "Iteration 32, Training Loss: 0.679855197905709\n",
      "Iteration 33, Training Loss: 0.6734097067998431\n",
      "Iteration 34, Training Loss: 0.6671587629844261\n",
      "Iteration 35, Training Loss: 0.6610688761538224\n",
      "Iteration 36, Training Loss: 0.6551164318788274\n",
      "Iteration 37, Training Loss: 0.6492881941512131\n",
      "Iteration 38, Training Loss: 0.6435617905232492\n",
      "Iteration 39, Training Loss: 0.6379334583368855\n",
      "Iteration 40, Training Loss: 0.6323903374956062\n",
      "Iteration 41, Training Loss: 0.6269268237065126\n",
      "Iteration 42, Training Loss: 0.6215324137877402\n",
      "Iteration 43, Training Loss: 0.6162056613682313\n",
      "Iteration 44, Training Loss: 0.610945534843739\n",
      "Iteration 45, Training Loss: 0.6057421678360743\n",
      "Iteration 46, Training Loss: 0.6005947012242236\n",
      "Iteration 47, Training Loss: 0.5954980947172065\n",
      "Iteration 48, Training Loss: 0.5904580800071542\n",
      "Iteration 49, Training Loss: 0.5854706950015965\n",
      "Iteration 50, Training Loss: 0.5805344377472379\n",
      "Iteration 51, Training Loss: 0.5756442277919307\n",
      "Iteration 52, Training Loss: 0.5707995777999335\n",
      "Iteration 53, Training Loss: 0.5660031334002718\n",
      "Iteration 54, Training Loss: 0.5612528010999813\n",
      "Iteration 55, Training Loss: 0.556549606679797\n",
      "Iteration 56, Training Loss: 0.5518960775735097\n",
      "Iteration 57, Training Loss: 0.5472866909725906\n",
      "Iteration 58, Training Loss: 0.5427203457867021\n",
      "Iteration 59, Training Loss: 0.5381981192332429\n",
      "Iteration 60, Training Loss: 0.5337189217905661\n",
      "Iteration 61, Training Loss: 0.5292825027356698\n",
      "Iteration 62, Training Loss: 0.5248862753208443\n",
      "Iteration 63, Training Loss: 0.520528617973799\n",
      "Iteration 64, Training Loss: 0.5162094959040577\n",
      "Iteration 65, Training Loss: 0.5119301121030433\n",
      "Iteration 66, Training Loss: 0.5076924730108836\n",
      "Iteration 67, Training Loss: 0.5034912162618838\n",
      "Iteration 68, Training Loss: 0.4993275835386557\n",
      "Iteration 69, Training Loss: 0.4952025101056992\n",
      "Iteration 70, Training Loss: 0.49111405930702584\n",
      "Iteration 71, Training Loss: 0.4870611930465937\n",
      "Iteration 72, Training Loss: 0.48304379236508016\n",
      "Iteration 73, Training Loss: 0.47906129202686554\n",
      "Iteration 74, Training Loss: 0.47511490200038714\n",
      "Iteration 75, Training Loss: 0.4712038683873963\n",
      "Iteration 76, Training Loss: 0.4673282233920833\n",
      "Iteration 77, Training Loss: 0.4634916820690002\n",
      "Iteration 78, Training Loss: 0.4596913652021065\n",
      "Iteration 79, Training Loss: 0.45592932291425925\n",
      "Iteration 80, Training Loss: 0.4522030464247439\n",
      "Iteration 81, Training Loss: 0.4485110823464486\n",
      "Iteration 82, Training Loss: 0.444853446329809\n",
      "Iteration 83, Training Loss: 0.44122848328161984\n",
      "Iteration 84, Training Loss: 0.43763671103229274\n",
      "Iteration 85, Training Loss: 0.4340793136637712\n",
      "Iteration 86, Training Loss: 0.4305589677453116\n",
      "Iteration 87, Training Loss: 0.42707362422446843\n",
      "Iteration 88, Training Loss: 0.4236209559686602\n",
      "Iteration 89, Training Loss: 0.4202024112901134\n",
      "Iteration 90, Training Loss: 0.4168171618615837\n",
      "Iteration 91, Training Loss: 0.41346348659046334\n",
      "Iteration 92, Training Loss: 0.4101450157799522\n",
      "Iteration 93, Training Loss: 0.40685862696030106\n",
      "Iteration 94, Training Loss: 0.4036063091383037\n",
      "Iteration 95, Training Loss: 0.4003861078814573\n",
      "Iteration 96, Training Loss: 0.39719726750081424\n",
      "Iteration 97, Training Loss: 0.3940384545303436\n",
      "Iteration 98, Training Loss: 0.390913489147478\n",
      "Iteration 99, Training Loss: 0.3878197308399187\n",
      "Iteration 100, Training Loss: 0.38475713480399426\n",
      "Iteration 101, Training Loss: 0.38172671728267343\n",
      "Iteration 102, Training Loss: 0.37872784723039454\n",
      "Iteration 103, Training Loss: 0.37576016993391126\n",
      "Iteration 104, Training Loss: 0.3728262230951677\n",
      "Iteration 105, Training Loss: 0.3699221926819238\n",
      "Iteration 106, Training Loss: 0.36704547333503645\n",
      "Iteration 107, Training Loss: 0.36419787439634943\n",
      "Iteration 108, Training Loss: 0.3613818591857984\n",
      "Iteration 109, Training Loss: 0.35859589335665326\n",
      "Iteration 110, Training Loss: 0.3558396875647427\n",
      "Iteration 111, Training Loss: 0.3531143022386421\n",
      "Iteration 112, Training Loss: 0.35041738236953474\n",
      "Iteration 113, Training Loss: 0.3477506133176808\n",
      "Iteration 114, Training Loss: 0.3451127863875204\n",
      "Iteration 115, Training Loss: 0.34250279581178156\n",
      "Iteration 116, Training Loss: 0.3399213193461307\n",
      "Iteration 117, Training Loss: 0.3373692192387503\n",
      "Iteration 118, Training Loss: 0.3348456344928101\n",
      "Iteration 119, Training Loss: 0.33235008455824644\n",
      "Iteration 120, Training Loss: 0.32988188949299974\n",
      "Iteration 121, Training Loss: 0.3274413374164784\n",
      "Iteration 122, Training Loss: 0.32502911173770815\n",
      "Iteration 123, Training Loss: 0.3226433488161696\n",
      "Iteration 124, Training Loss: 0.3202835051467952\n",
      "Iteration 125, Training Loss: 0.3179504318908658\n",
      "Iteration 126, Training Loss: 0.3156416809401039\n",
      "Iteration 127, Training Loss: 0.3133563702623085\n",
      "Iteration 128, Training Loss: 0.3110959115742841\n",
      "Iteration 129, Training Loss: 0.30886131232308234\n",
      "Iteration 130, Training Loss: 0.30665365343616746\n",
      "Iteration 131, Training Loss: 0.30447166321916735\n",
      "Iteration 132, Training Loss: 0.30231338346121234\n",
      "Iteration 133, Training Loss: 0.3001780343381216\n",
      "Iteration 134, Training Loss: 0.29806566079928126\n",
      "Iteration 135, Training Loss: 0.29597680151347455\n",
      "Iteration 136, Training Loss: 0.29391015752821315\n",
      "Iteration 137, Training Loss: 0.2918661101762635\n",
      "Iteration 138, Training Loss: 0.2898445094300108\n",
      "Iteration 139, Training Loss: 0.2878458872844242\n",
      "Iteration 140, Training Loss: 0.2858716015579916\n",
      "Iteration 141, Training Loss: 0.283918886175819\n",
      "Iteration 142, Training Loss: 0.2819881362021103\n",
      "Iteration 143, Training Loss: 0.2800782983245038\n",
      "Iteration 144, Training Loss: 0.2781908840940922\n",
      "Iteration 145, Training Loss: 0.2763249787833348\n",
      "Iteration 146, Training Loss: 0.2744808681987907\n",
      "Iteration 147, Training Loss: 0.2726579158747728\n",
      "Iteration 148, Training Loss: 0.2708574230130559\n",
      "Iteration 149, Training Loss: 0.26907698627733256\n",
      "Iteration 150, Training Loss: 0.26731674290212054\n",
      "Iteration 151, Training Loss: 0.2655782339092963\n",
      "Iteration 152, Training Loss: 0.2638598171896183\n",
      "Iteration 153, Training Loss: 0.26216196217174464\n",
      "Iteration 154, Training Loss: 0.260483374588311\n",
      "Iteration 155, Training Loss: 0.2588250451833294\n",
      "Iteration 156, Training Loss: 0.25718661416875194\n",
      "Iteration 157, Training Loss: 0.25557034195954415\n",
      "Iteration 158, Training Loss: 0.253975270254991\n",
      "Iteration 159, Training Loss: 0.2524009993241029\n",
      "Iteration 160, Training Loss: 0.2508440602709014\n",
      "Iteration 161, Training Loss: 0.2493105922276033\n",
      "Iteration 162, Training Loss: 0.2477991376751223\n",
      "Iteration 163, Training Loss: 0.24630857459193892\n",
      "Iteration 164, Training Loss: 0.24483795949632808\n",
      "Iteration 165, Training Loss: 0.24338905774918956\n",
      "Iteration 166, Training Loss: 0.2419620274812029\n",
      "Iteration 167, Training Loss: 0.24055506225696677\n",
      "Iteration 168, Training Loss: 0.23916955432882678\n",
      "Iteration 169, Training Loss: 0.23780479219499936\n",
      "Iteration 170, Training Loss: 0.23646032757100519\n",
      "Iteration 171, Training Loss: 0.23513547127469922\n",
      "Iteration 172, Training Loss: 0.23383209045532777\n",
      "Iteration 173, Training Loss: 0.23255296360962693\n",
      "Iteration 174, Training Loss: 0.23129589863092645\n",
      "Iteration 175, Training Loss: 0.23006105254324372\n",
      "Iteration 176, Training Loss: 0.22884851137018072\n",
      "Iteration 177, Training Loss: 0.22765711298541227\n",
      "Iteration 178, Training Loss: 0.22648599793234633\n",
      "Iteration 179, Training Loss: 0.22533747808142707\n",
      "Iteration 180, Training Loss: 0.22421182775570062\n",
      "Iteration 181, Training Loss: 0.22310713504598823\n",
      "Iteration 182, Training Loss: 0.22202289607620831\n",
      "Iteration 183, Training Loss: 0.2209562584289045\n",
      "Iteration 184, Training Loss: 0.21991129252542072\n",
      "Iteration 185, Training Loss: 0.21888361520256014\n",
      "Iteration 186, Training Loss: 0.21787769001168883\n",
      "Iteration 187, Training Loss: 0.21688964865750707\n",
      "Iteration 188, Training Loss: 0.21592029142349806\n",
      "Iteration 189, Training Loss: 0.21496785047531752\n",
      "Iteration 190, Training Loss: 0.2140348703904246\n",
      "Iteration 191, Training Loss: 0.21312150629420507\n",
      "Iteration 192, Training Loss: 0.2122267617820511\n",
      "Iteration 193, Training Loss: 0.21134512245295303\n",
      "Iteration 194, Training Loss: 0.2104864971383602\n",
      "Iteration 195, Training Loss: 0.2096396263899282\n",
      "Iteration 196, Training Loss: 0.20882119849805877\n",
      "Iteration 197, Training Loss: 0.208006204228141\n",
      "Iteration 198, Training Loss: 0.20722946968407308\n",
      "Iteration 199, Training Loss: 0.2064427007667745\n",
      "Iteration 200, Training Loss: 0.20571714431531182\n",
      "Iteration 201, Training Loss: 0.20495139338497423\n",
      "Iteration 202, Training Loss: 0.2043297064407705\n",
      "Iteration 203, Training Loss: 0.20361080066334394\n",
      "Iteration 204, Training Loss: 0.20314480959082235\n",
      "Iteration 205, Training Loss: 0.20248055413822363\n",
      "Iteration 206, Training Loss: 0.20186938564944884\n",
      "Iteration 207, Training Loss: 0.20124099712617238\n",
      "Iteration 208, Training Loss: 0.200588866864588\n",
      "Iteration 209, Training Loss: 0.20002451361035972\n",
      "Iteration 210, Training Loss: 0.1993766419036782\n",
      "Iteration 211, Training Loss: 0.19888103685204297\n",
      "Iteration 212, Training Loss: 0.19823397519996822\n",
      "Iteration 213, Training Loss: 0.19781853321325418\n",
      "Iteration 214, Training Loss: 0.19715450753144712\n",
      "Iteration 215, Training Loss: 0.19681875512673916\n",
      "Iteration 216, Training Loss: 0.196140566516551\n",
      "Iteration 217, Training Loss: 0.19589439729428762\n",
      "Iteration 218, Training Loss: 0.1951886419740801\n",
      "Iteration 219, Training Loss: 0.19504322091959372\n",
      "Iteration 220, Training Loss: 0.19429992330652504\n",
      "Iteration 221, Training Loss: 0.19426624163074513\n",
      "Iteration 222, Training Loss: 0.1934888339627881\n",
      "Iteration 223, Training Loss: 0.1936002938727753\n",
      "Iteration 224, Training Loss: 0.19272900061981119\n",
      "Iteration 225, Training Loss: 0.1929937761377009\n",
      "Iteration 226, Training Loss: 0.19202850993996037\n",
      "Iteration 227, Training Loss: 0.19247947045376054\n",
      "Iteration 228, Training Loss: 0.1913929974040349\n",
      "Iteration 229, Training Loss: 0.19205130542425353\n",
      "Iteration 230, Training Loss: 0.190814689353481\n",
      "Iteration 231, Training Loss: 0.19170578131860536\n",
      "Iteration 232, Training Loss: 0.19027793211641217\n",
      "Iteration 233, Training Loss: 0.19138779591958002\n",
      "Iteration 234, Training Loss: 0.1897978230591252\n",
      "Iteration 235, Training Loss: 0.19108334539416205\n",
      "Iteration 236, Training Loss: 0.1893690298857143\n",
      "Iteration 237, Training Loss: 0.19072824466633334\n",
      "Iteration 238, Training Loss: 0.1890010406976482\n",
      "Iteration 239, Training Loss: 0.19037513082421814\n",
      "Iteration 240, Training Loss: 0.18871226099723026\n",
      "Iteration 241, Training Loss: 0.1900176827032415\n",
      "Iteration 242, Training Loss: 0.18874692292981576\n",
      "Iteration 243, Training Loss: 0.1896628034199125\n",
      "Iteration 244, Training Loss: 0.18896818732824042\n",
      "Iteration 245, Training Loss: 0.18870664489135844\n",
      "Iteration 246, Training Loss: 0.18888755508319788\n",
      "Iteration 247, Training Loss: 0.18805790375624082\n",
      "Iteration 248, Training Loss: 0.18841176634040435\n",
      "Iteration 249, Training Loss: 0.18784202239468747\n",
      "Iteration 250, Training Loss: 0.1880140594713796\n",
      "Iteration 251, Training Loss: 0.18753454467874967\n",
      "Iteration 252, Training Loss: 0.18763909638993503\n",
      "Iteration 253, Training Loss: 0.18724454751000896\n",
      "Iteration 254, Training Loss: 0.18732405294467258\n",
      "Iteration 255, Training Loss: 0.18692751735673996\n",
      "Iteration 256, Training Loss: 0.18703252213924224\n",
      "Iteration 257, Training Loss: 0.18662532339786944\n",
      "Iteration 258, Training Loss: 0.18675349649388484\n",
      "Iteration 259, Training Loss: 0.18632744352352812\n",
      "Iteration 260, Training Loss: 0.18649039773535175\n",
      "Iteration 261, Training Loss: 0.18602944996241283\n",
      "Iteration 262, Training Loss: 0.18622379614236767\n",
      "Iteration 263, Training Loss: 0.18575527172368592\n",
      "Iteration 264, Training Loss: 0.1859827763615033\n",
      "Iteration 265, Training Loss: 0.18548555137839667\n",
      "Iteration 266, Training Loss: 0.18575439460737084\n",
      "Iteration 267, Training Loss: 0.18522741469275145\n",
      "Iteration 268, Training Loss: 0.1855229078647345\n",
      "Iteration 269, Training Loss: 0.1849880277532446\n",
      "Iteration 270, Training Loss: 0.18529488602636549\n",
      "Iteration 271, Training Loss: 0.18475680284984072\n",
      "Iteration 272, Training Loss: 0.185108932205698\n",
      "Iteration 273, Training Loss: 0.18454262500399599\n",
      "Iteration 274, Training Loss: 0.1849527498135587\n",
      "Iteration 275, Training Loss: 0.18433547823915228\n",
      "Iteration 276, Training Loss: 0.18481387979030411\n",
      "Iteration 277, Training Loss: 0.1841451310905258\n",
      "Iteration 278, Training Loss: 0.18468237650628858\n",
      "Iteration 279, Training Loss: 0.18395795496830164\n",
      "Iteration 280, Training Loss: 0.18458246550796367\n",
      "Iteration 281, Training Loss: 0.18379450916519113\n",
      "Iteration 282, Training Loss: 0.18447249362649965\n",
      "Iteration 283, Training Loss: 0.18363669551231687\n",
      "Iteration 284, Training Loss: 0.18435548293269544\n",
      "Iteration 285, Training Loss: 0.18352344094034057\n",
      "Iteration 286, Training Loss: 0.1842730532136778\n",
      "Iteration 287, Training Loss: 0.18336570671833388\n",
      "Iteration 288, Training Loss: 0.18416968468367398\n",
      "Iteration 289, Training Loss: 0.1832535316977803\n",
      "Iteration 290, Training Loss: 0.1840939284157412\n",
      "Iteration 291, Training Loss: 0.18312214287957868\n",
      "Iteration 292, Training Loss: 0.18399110863217044\n",
      "Iteration 293, Training Loss: 0.18303236691100727\n",
      "Iteration 294, Training Loss: 0.18391086499823453\n",
      "Iteration 295, Training Loss: 0.1829258795704614\n",
      "Iteration 296, Training Loss: 0.1838179349063844\n",
      "Iteration 297, Training Loss: 0.18284492930294458\n",
      "Iteration 298, Training Loss: 0.18375302195752655\n",
      "Iteration 299, Training Loss: 0.18274195636051782\n",
      "Iteration 300, Training Loss: 0.183672823249071\n",
      "Iteration 301, Training Loss: 0.18265384255821587\n",
      "Iteration 302, Training Loss: 0.18361738163987285\n",
      "Iteration 303, Training Loss: 0.18256482678446667\n",
      "Iteration 304, Training Loss: 0.1835475589103053\n",
      "Iteration 305, Training Loss: 0.18250508360425335\n",
      "Iteration 306, Training Loss: 0.18348537827695494\n",
      "Iteration 307, Training Loss: 0.1824504635678167\n",
      "Iteration 308, Training Loss: 0.1834213311293921\n",
      "Iteration 309, Training Loss: 0.18238991009284736\n",
      "Iteration 310, Training Loss: 0.18335034295577313\n",
      "Iteration 311, Training Loss: 0.18235550344555004\n",
      "Iteration 312, Training Loss: 0.1833074181785602\n",
      "Iteration 313, Training Loss: 0.18228369563471725\n",
      "Iteration 314, Training Loss: 0.18326516964465678\n",
      "Iteration 315, Training Loss: 0.18224249737941148\n",
      "Iteration 316, Training Loss: 0.18324548651640915\n",
      "Iteration 317, Training Loss: 0.1821768630375634\n",
      "Iteration 318, Training Loss: 0.18320534485863527\n",
      "Iteration 319, Training Loss: 0.18213522435657895\n",
      "Iteration 320, Training Loss: 0.18318197072565862\n",
      "Iteration 321, Training Loss: 0.18210184105905708\n",
      "Iteration 322, Training Loss: 0.18313484324371132\n",
      "Iteration 323, Training Loss: 0.1820850285792438\n",
      "Iteration 324, Training Loss: 0.18311379318392337\n",
      "Iteration 325, Training Loss: 0.1820411115939806\n",
      "Iteration 326, Training Loss: 0.18306257518265226\n",
      "Iteration 327, Training Loss: 0.18202252972446514\n",
      "Iteration 328, Training Loss: 0.18306782105286046\n",
      "Iteration 329, Training Loss: 0.18196100110160537\n",
      "Iteration 330, Training Loss: 0.1830347947610144\n",
      "Iteration 331, Training Loss: 0.18193892477614806\n",
      "Iteration 332, Training Loss: 0.1830323948453928\n",
      "Iteration 333, Training Loss: 0.18189989708287227\n",
      "Iteration 334, Training Loss: 0.1829886182214\n",
      "Iteration 335, Training Loss: 0.18189406493927185\n",
      "Iteration 336, Training Loss: 0.18298236934101206\n",
      "Iteration 337, Training Loss: 0.18185285872619472\n",
      "Iteration 338, Training Loss: 0.1829226704387564\n",
      "Iteration 339, Training Loss: 0.18186395789234622\n",
      "Iteration 340, Training Loss: 0.18293779599459797\n",
      "Iteration 341, Training Loss: 0.1817909229137654\n",
      "Iteration 342, Training Loss: 0.18289163700460184\n",
      "Iteration 343, Training Loss: 0.18179709009217357\n",
      "Iteration 344, Training Loss: 0.18291059571694757\n",
      "Iteration 345, Training Loss: 0.18173790936509526\n",
      "Iteration 346, Training Loss: 0.18283655407456936\n",
      "Iteration 347, Training Loss: 0.18176665449736132\n",
      "Iteration 348, Training Loss: 0.18286249312241692\n",
      "Iteration 349, Training Loss: 0.18169318198818724\n",
      "Iteration 350, Training Loss: 0.1827832069685716\n",
      "Iteration 351, Training Loss: 0.1817269238607457\n",
      "Iteration 352, Training Loss: 0.18281986774636189\n",
      "Iteration 353, Training Loss: 0.1816509235002311\n",
      "Iteration 354, Training Loss: 0.1827361121349241\n",
      "Iteration 355, Training Loss: 0.1816813639362551\n",
      "Iteration 356, Training Loss: 0.1827871336171445\n",
      "Iteration 357, Training Loss: 0.18159239708612102\n",
      "Iteration 358, Training Loss: 0.1827009424403796\n",
      "Iteration 359, Training Loss: 0.18164159908320057\n",
      "Iteration 360, Training Loss: 0.18273452971879062\n",
      "Iteration 361, Training Loss: 0.1815581322124327\n",
      "Iteration 362, Training Loss: 0.1826723123428583\n",
      "Iteration 363, Training Loss: 0.1815836864078549\n",
      "Iteration 364, Training Loss: 0.18267826108817461\n",
      "Iteration 365, Training Loss: 0.18153960058847446\n",
      "Iteration 366, Training Loss: 0.18263900771386296\n",
      "Iteration 367, Training Loss: 0.1815271480511595\n",
      "Iteration 368, Training Loss: 0.1826390082149977\n",
      "Iteration 369, Training Loss: 0.18149023630254807\n",
      "Iteration 370, Training Loss: 0.18260760726717373\n",
      "Iteration 371, Training Loss: 0.18147671502343185\n",
      "Iteration 372, Training Loss: 0.18261020470693207\n",
      "Iteration 373, Training Loss: 0.18144707857648987\n",
      "Iteration 374, Training Loss: 0.18255409580040782\n",
      "Iteration 375, Training Loss: 0.18146426819276257\n",
      "Iteration 376, Training Loss: 0.18254922248516905\n",
      "Iteration 377, Training Loss: 0.18141839969208223\n",
      "Iteration 378, Training Loss: 0.1825037578413251\n",
      "Iteration 379, Training Loss: 0.18141553835244026\n",
      "Iteration 380, Training Loss: 0.18252692407251922\n",
      "Iteration 381, Training Loss: 0.18135361535179922\n",
      "Iteration 382, Training Loss: 0.18247182685795532\n",
      "Iteration 383, Training Loss: 0.18137125043754201\n",
      "Iteration 384, Training Loss: 0.18249795222754153\n",
      "Iteration 385, Training Loss: 0.18130553198304108\n",
      "Iteration 386, Training Loss: 0.18243663413890768\n",
      "Iteration 387, Training Loss: 0.18133460808661273\n",
      "Iteration 388, Training Loss: 0.18245541714944175\n",
      "Iteration 389, Training Loss: 0.1812827130807952\n",
      "Iteration 390, Training Loss: 0.18238231320738107\n",
      "Iteration 391, Training Loss: 0.18130925676720158\n",
      "Iteration 392, Training Loss: 0.18239752122778918\n",
      "Iteration 393, Training Loss: 0.18125007637980523\n",
      "Iteration 394, Training Loss: 0.18234539904225724\n",
      "Iteration 395, Training Loss: 0.18125741528511924\n",
      "Iteration 396, Training Loss: 0.1823820809733441\n",
      "Iteration 397, Training Loss: 0.18118751622191837\n",
      "Iteration 398, Training Loss: 0.18230479851838816\n",
      "Iteration 399, Training Loss: 0.1812315922780227\n",
      "Iteration 400, Training Loss: 0.18235042539314658\n",
      "Iteration 401, Training Loss: 0.1811450746567395\n",
      "Iteration 402, Training Loss: 0.1822671880487885\n",
      "Iteration 403, Training Loss: 0.18119393479775284\n",
      "Iteration 404, Training Loss: 0.18230361405442574\n",
      "Iteration 405, Training Loss: 0.18112046993940015\n",
      "Iteration 406, Training Loss: 0.1822300128801338\n",
      "Iteration 407, Training Loss: 0.18115525802423776\n",
      "Iteration 408, Training Loss: 0.1822563762050128\n",
      "Iteration 409, Training Loss: 0.18108863789181706\n",
      "Iteration 410, Training Loss: 0.18219824564365314\n",
      "Iteration 411, Training Loss: 0.18111112826936895\n",
      "Iteration 412, Training Loss: 0.1822212556612468\n",
      "Iteration 413, Training Loss: 0.18104568042833966\n",
      "Iteration 414, Training Loss: 0.18216614447736232\n",
      "Iteration 415, Training Loss: 0.18106018711382202\n",
      "Iteration 416, Training Loss: 0.18219568812737125\n",
      "Iteration 417, Training Loss: 0.1810003465817097\n",
      "Iteration 418, Training Loss: 0.18213929766505785\n",
      "Iteration 419, Training Loss: 0.18102553951596917\n",
      "Iteration 420, Training Loss: 0.18215324036787792\n",
      "Iteration 421, Training Loss: 0.18098124074304128\n",
      "Iteration 422, Training Loss: 0.18208997184324907\n",
      "Iteration 423, Training Loss: 0.1809982016897262\n",
      "Iteration 424, Training Loss: 0.18210541355398652\n",
      "Iteration 425, Training Loss: 0.18094317434933774\n",
      "Iteration 426, Training Loss: 0.1820464911892762\n",
      "Iteration 427, Training Loss: 0.1809738823717443\n",
      "Iteration 428, Training Loss: 0.1820846951082933\n",
      "Iteration 429, Training Loss: 0.18089001463221674\n",
      "Iteration 430, Training Loss: 0.18201360883359285\n",
      "Iteration 431, Training Loss: 0.18092657013641728\n",
      "Iteration 432, Training Loss: 0.1820614401607002\n",
      "Iteration 433, Training Loss: 0.18085143458402358\n",
      "Iteration 434, Training Loss: 0.18198443330939448\n",
      "Iteration 435, Training Loss: 0.1808961728820862\n",
      "Iteration 436, Training Loss: 0.18202430293862143\n",
      "Iteration 437, Training Loss: 0.1808295735902534\n",
      "Iteration 438, Training Loss: 0.18193803159529925\n",
      "Iteration 439, Training Loss: 0.18087503949640443\n",
      "Iteration 440, Training Loss: 0.1819677320446352\n",
      "Iteration 441, Training Loss: 0.18080012058951808\n",
      "Iteration 442, Training Loss: 0.18191998196915726\n",
      "Iteration 443, Training Loss: 0.1808253371135442\n",
      "Iteration 444, Training Loss: 0.1819439122156275\n",
      "Iteration 445, Training Loss: 0.18075461765250805\n",
      "Iteration 446, Training Loss: 0.18188803662182046\n",
      "Iteration 447, Training Loss: 0.1807891802282104\n",
      "Iteration 448, Training Loss: 0.18191803825306627\n",
      "Iteration 449, Training Loss: 0.18073400255265482\n",
      "Iteration 450, Training Loss: 0.1818433916604751\n",
      "Iteration 451, Training Loss: 0.1807649493697624\n",
      "Iteration 452, Training Loss: 0.18187061138086633\n",
      "Iteration 453, Training Loss: 0.18070578038748353\n",
      "Iteration 454, Training Loss: 0.1818257013150684\n",
      "Iteration 455, Training Loss: 0.18071333892317412\n",
      "Iteration 456, Training Loss: 0.18185044732337705\n",
      "Iteration 457, Training Loss: 0.18066527370873114\n",
      "Iteration 458, Training Loss: 0.18180019030883013\n",
      "Iteration 459, Training Loss: 0.1806905643836738\n",
      "Iteration 460, Training Loss: 0.18181470442395356\n",
      "Iteration 461, Training Loss: 0.18064291594710033\n",
      "Iteration 462, Training Loss: 0.18174832595363533\n",
      "Iteration 463, Training Loss: 0.18067678524333913\n",
      "Iteration 464, Training Loss: 0.18178700442298387\n",
      "Iteration 465, Training Loss: 0.18059988631633928\n",
      "Iteration 466, Training Loss: 0.1817178363626969\n",
      "Iteration 467, Training Loss: 0.18064311149340698\n",
      "Iteration 468, Training Loss: 0.1817776142922782\n",
      "Iteration 469, Training Loss: 0.18055249659927483\n",
      "Iteration 470, Training Loss: 0.18168790647075192\n",
      "Iteration 471, Training Loss: 0.1806204942086403\n",
      "Iteration 472, Training Loss: 0.1817579609061202\n",
      "Iteration 473, Training Loss: 0.18052251984676881\n",
      "Iteration 474, Training Loss: 0.1816481718192291\n",
      "Iteration 475, Training Loss: 0.18060076262092523\n",
      "Iteration 476, Training Loss: 0.18172690122224255\n",
      "Iteration 477, Training Loss: 0.18049299318420872\n",
      "Iteration 478, Training Loss: 0.1816170960498786\n",
      "Iteration 479, Training Loss: 0.18057667400714816\n",
      "Iteration 480, Training Loss: 0.18169063466452415\n",
      "Iteration 481, Training Loss: 0.18046853637970464\n",
      "Iteration 482, Training Loss: 0.18159275494440325\n",
      "Iteration 483, Training Loss: 0.1805504369211724\n",
      "Iteration 484, Training Loss: 0.18165643447950547\n",
      "Iteration 485, Training Loss: 0.18043962936969918\n",
      "Iteration 486, Training Loss: 0.18158044040752963\n",
      "Iteration 487, Training Loss: 0.1805066559769708\n",
      "Iteration 488, Training Loss: 0.18164045471931867\n",
      "Iteration 489, Training Loss: 0.18040346965112397\n",
      "Iteration 490, Training Loss: 0.18155062857318546\n",
      "Iteration 491, Training Loss: 0.18047867054645644\n",
      "Iteration 492, Training Loss: 0.18161813887239628\n",
      "Iteration 493, Training Loss: 0.18037701633325123\n",
      "Iteration 494, Training Loss: 0.18151366761195412\n",
      "Iteration 495, Training Loss: 0.1804682602483502\n",
      "Iteration 496, Training Loss: 0.181579990124327\n",
      "Iteration 497, Training Loss: 0.18035928628807374\n",
      "Iteration 498, Training Loss: 0.18149169988736638\n",
      "Iteration 499, Training Loss: 0.1804295386846654\n",
      "Iteration 500, Training Loss: 0.1815615725903907\n",
      "Iteration 501, Training Loss: 0.18032191870171288\n",
      "Iteration 502, Training Loss: 0.18147112143027957\n",
      "Iteration 503, Training Loss: 0.18040428428438918\n",
      "Iteration 504, Training Loss: 0.18153399810366908\n",
      "Iteration 505, Training Loss: 0.18029531552617267\n",
      "Iteration 506, Training Loss: 0.18143242181267222\n",
      "Iteration 507, Training Loss: 0.18038804312107484\n",
      "Iteration 508, Training Loss: 0.18150569623948662\n",
      "Iteration 509, Training Loss: 0.18027311745036076\n",
      "Iteration 510, Training Loss: 0.18141109816289522\n",
      "Iteration 511, Training Loss: 0.18036667158598585\n",
      "Iteration 512, Training Loss: 0.18147630618100327\n",
      "Iteration 513, Training Loss: 0.18023925136511684\n",
      "Iteration 514, Training Loss: 0.181391981078925\n",
      "Iteration 515, Training Loss: 0.18032440649734127\n",
      "Iteration 516, Training Loss: 0.18146933724952244\n",
      "Iteration 517, Training Loss: 0.18021225726048512\n",
      "Iteration 518, Training Loss: 0.18134743423565414\n",
      "Iteration 519, Training Loss: 0.18032808408156561\n",
      "Iteration 520, Training Loss: 0.18143364471706802\n",
      "Iteration 521, Training Loss: 0.18019405271430303\n",
      "Iteration 522, Training Loss: 0.18132810724028445\n",
      "Iteration 523, Training Loss: 0.18029307960618007\n",
      "Iteration 524, Training Loss: 0.181402726457119\n",
      "Iteration 525, Training Loss: 0.18016769635568178\n",
      "Iteration 526, Training Loss: 0.18131546157440748\n",
      "Iteration 527, Training Loss: 0.18024843459024031\n",
      "Iteration 528, Training Loss: 0.18139735380577426\n",
      "Iteration 529, Training Loss: 0.18013815923462756\n",
      "Iteration 530, Training Loss: 0.18127747748484893\n",
      "Iteration 531, Training Loss: 0.1802472091864752\n",
      "Iteration 532, Training Loss: 0.18135300990638872\n",
      "Iteration 533, Training Loss: 0.1801225964005718\n",
      "Iteration 534, Training Loss: 0.1812675676963901\n",
      "Iteration 535, Training Loss: 0.18020455676583316\n",
      "Iteration 536, Training Loss: 0.18133716605260491\n",
      "Iteration 537, Training Loss: 0.1800874379157558\n",
      "Iteration 538, Training Loss: 0.18124210390128012\n",
      "Iteration 539, Training Loss: 0.1801829333475514\n",
      "Iteration 540, Training Loss: 0.18132757293908303\n",
      "Iteration 541, Training Loss: 0.18005913483972094\n",
      "Iteration 542, Training Loss: 0.18119877940940518\n",
      "Iteration 543, Training Loss: 0.18018181408478573\n",
      "Iteration 544, Training Loss: 0.18128807281087495\n",
      "Iteration 545, Training Loss: 0.18005402824100153\n",
      "Iteration 546, Training Loss: 0.18118400338472268\n",
      "Iteration 547, Training Loss: 0.18013935355367375\n",
      "Iteration 548, Training Loss: 0.18125776255099582\n",
      "Iteration 549, Training Loss: 0.1800260959805641\n",
      "Iteration 550, Training Loss: 0.18118013895959448\n",
      "Iteration 551, Training Loss: 0.18009947476262067\n",
      "Iteration 552, Training Loss: 0.18124811802268648\n",
      "Iteration 553, Training Loss: 0.18000266076448518\n",
      "Iteration 554, Training Loss: 0.1811473449898884\n",
      "Iteration 555, Training Loss: 0.18008916697172905\n",
      "Iteration 556, Training Loss: 0.1812150876823116\n",
      "Iteration 557, Training Loss: 0.17998468393537104\n",
      "Iteration 558, Training Loss: 0.1811251761268701\n",
      "Iteration 559, Training Loss: 0.18006444807097985\n",
      "Iteration 560, Training Loss: 0.1811971885837925\n",
      "Iteration 561, Training Loss: 0.17995365971534102\n",
      "Iteration 562, Training Loss: 0.1811074890440366\n",
      "Iteration 563, Training Loss: 0.18003959418843818\n",
      "Iteration 564, Training Loss: 0.18117376524082615\n",
      "Iteration 565, Training Loss: 0.17994006618192715\n",
      "Iteration 566, Training Loss: 0.1810819273827001\n",
      "Iteration 567, Training Loss: 0.18001268715182087\n",
      "Iteration 568, Training Loss: 0.18114382964848144\n",
      "Iteration 569, Training Loss: 0.1799313710677778\n",
      "Iteration 570, Training Loss: 0.18106335390845338\n",
      "Iteration 571, Training Loss: 0.18000019312409793\n",
      "Iteration 572, Training Loss: 0.1811073539680445\n",
      "Iteration 573, Training Loss: 0.17989847896328334\n",
      "Iteration 574, Training Loss: 0.1810413207377478\n",
      "Iteration 575, Training Loss: 0.1799698154718126\n",
      "Iteration 576, Training Loss: 0.18110518061018172\n",
      "Iteration 577, Training Loss: 0.17987532066402123\n",
      "Iteration 578, Training Loss: 0.18101677599063154\n",
      "Iteration 579, Training Loss: 0.179951051387954\n",
      "Iteration 580, Training Loss: 0.1810911516166932\n",
      "Iteration 581, Training Loss: 0.17983636545256174\n",
      "Iteration 582, Training Loss: 0.18099204211430017\n",
      "Iteration 583, Training Loss: 0.1799416569434922\n",
      "Iteration 584, Training Loss: 0.18106751482627248\n",
      "Iteration 585, Training Loss: 0.17983105319781165\n",
      "Iteration 586, Training Loss: 0.1809678345924193\n",
      "Iteration 587, Training Loss: 0.17990997475534823\n",
      "Iteration 588, Training Loss: 0.1810436600987657\n",
      "Iteration 589, Training Loss: 0.17981084864537442\n",
      "Iteration 590, Training Loss: 0.18095167502427256\n",
      "Iteration 591, Training Loss: 0.179885307338973\n",
      "Iteration 592, Training Loss: 0.18102441818673717\n",
      "Iteration 593, Training Loss: 0.17978503016556802\n",
      "Iteration 594, Training Loss: 0.1809444954525573\n",
      "Iteration 595, Training Loss: 0.1798630722427072\n",
      "Iteration 596, Training Loss: 0.18099338215583216\n",
      "Iteration 597, Training Loss: 0.1797754359486592\n",
      "Iteration 598, Training Loss: 0.18092145053160483\n",
      "Iteration 599, Training Loss: 0.17984530821526074\n",
      "Iteration 600, Training Loss: 0.18095588449483108\n",
      "Iteration 601, Training Loss: 0.17977041185620474\n",
      "Iteration 602, Training Loss: 0.1809075086449898\n",
      "Iteration 603, Training Loss: 0.1798090738919868\n",
      "Iteration 604, Training Loss: 0.18093915740953276\n",
      "Iteration 605, Training Loss: 0.1797463203298863\n",
      "Iteration 606, Training Loss: 0.18088496054794345\n",
      "Iteration 607, Training Loss: 0.17978360491314127\n",
      "Iteration 608, Training Loss: 0.18094272536794176\n",
      "Iteration 609, Training Loss: 0.17971274460200276\n",
      "Iteration 610, Training Loss: 0.18085225845676062\n",
      "Iteration 611, Training Loss: 0.1797804357681058\n",
      "Iteration 612, Training Loss: 0.18092425204494375\n",
      "Iteration 613, Training Loss: 0.17968921732586512\n",
      "Iteration 614, Training Loss: 0.18083040790791413\n",
      "Iteration 615, Training Loss: 0.1797686533097263\n",
      "Iteration 616, Training Loss: 0.18088922322723927\n",
      "Iteration 617, Training Loss: 0.17968285270919493\n",
      "Iteration 618, Training Loss: 0.1808153485775081\n",
      "Iteration 619, Training Loss: 0.17974122565826858\n",
      "Iteration 620, Training Loss: 0.18087389188320582\n",
      "Iteration 621, Training Loss: 0.17965598056178533\n",
      "Iteration 622, Training Loss: 0.18079526893698042\n",
      "Iteration 623, Training Loss: 0.17971546013707393\n",
      "Iteration 624, Training Loss: 0.18086781376713115\n",
      "Iteration 625, Training Loss: 0.17962803543448025\n",
      "Iteration 626, Training Loss: 0.1807768133091337\n",
      "Iteration 627, Training Loss: 0.1797045674455849\n",
      "Iteration 628, Training Loss: 0.18085213909173098\n",
      "Iteration 629, Training Loss: 0.17960920800628222\n",
      "Iteration 630, Training Loss: 0.18075291599042403\n",
      "Iteration 631, Training Loss: 0.1796922856991067\n",
      "Iteration 632, Training Loss: 0.180816305705265\n",
      "Iteration 633, Training Loss: 0.17960215371548519\n",
      "Iteration 634, Training Loss: 0.1807368007039932\n",
      "Iteration 635, Training Loss: 0.17966468993716506\n",
      "Iteration 636, Training Loss: 0.1808104607225402\n",
      "Iteration 637, Training Loss: 0.17957274294911033\n",
      "Iteration 638, Training Loss: 0.18071542831222245\n",
      "Iteration 639, Training Loss: 0.1796470497531593\n",
      "Iteration 640, Training Loss: 0.18079498961962204\n",
      "Iteration 641, Training Loss: 0.17955401095008489\n",
      "Iteration 642, Training Loss: 0.18069923900812812\n",
      "Iteration 643, Training Loss: 0.17963417369685009\n",
      "Iteration 644, Training Loss: 0.18076824827533644\n",
      "Iteration 645, Training Loss: 0.17953733714258574\n",
      "Iteration 646, Training Loss: 0.18068256776981242\n",
      "Iteration 647, Training Loss: 0.17961444517000666\n",
      "Iteration 648, Training Loss: 0.1807484305999927\n",
      "Iteration 649, Training Loss: 0.17952734900527278\n",
      "Iteration 650, Training Loss: 0.1806741866767944\n",
      "Iteration 651, Training Loss: 0.17958336895694596\n",
      "Iteration 652, Training Loss: 0.18071857921041856\n",
      "Iteration 653, Training Loss: 0.17951901795985126\n",
      "Iteration 654, Training Loss: 0.18065848006239962\n",
      "Iteration 655, Training Loss: 0.179568222223887\n",
      "Iteration 656, Training Loss: 0.18069762636980663\n",
      "Iteration 657, Training Loss: 0.17950368132248587\n",
      "Iteration 658, Training Loss: 0.18065096489541704\n",
      "Iteration 659, Training Loss: 0.17952779520180628\n",
      "Iteration 660, Training Loss: 0.1806936174156265\n",
      "Iteration 661, Training Loss: 0.17948671806315433\n",
      "Iteration 662, Training Loss: 0.1806263984387891\n",
      "Iteration 663, Training Loss: 0.17952754335029772\n",
      "Iteration 664, Training Loss: 0.1806612927450711\n",
      "Iteration 665, Training Loss: 0.17946984623624557\n",
      "Iteration 666, Training Loss: 0.18060525646536352\n",
      "Iteration 667, Training Loss: 0.17950960727447654\n",
      "Iteration 668, Training Loss: 0.1806560064607736\n",
      "Iteration 669, Training Loss: 0.1794446560539485\n",
      "Iteration 670, Training Loss: 0.18057358158381423\n",
      "Iteration 671, Training Loss: 0.17950473046737894\n",
      "Iteration 672, Training Loss: 0.18064811306345974\n",
      "Iteration 673, Training Loss: 0.179415305859154\n",
      "Iteration 674, Training Loss: 0.18054963901183355\n",
      "Iteration 675, Training Loss: 0.179488790164433\n",
      "Iteration 676, Training Loss: 0.18065372620232376\n",
      "Iteration 677, Training Loss: 0.17938285983419516\n",
      "Iteration 678, Training Loss: 0.18053294057381397\n",
      "Iteration 679, Training Loss: 0.17949178764283605\n",
      "Iteration 680, Training Loss: 0.18062389136557483\n",
      "Iteration 681, Training Loss: 0.17937410818768304\n",
      "Iteration 682, Training Loss: 0.18050269447694994\n",
      "Iteration 683, Training Loss: 0.17948564592048827\n",
      "Iteration 684, Training Loss: 0.18059760268587757\n",
      "Iteration 685, Training Loss: 0.17936662595271546\n",
      "Iteration 686, Training Loss: 0.18049340747940765\n",
      "Iteration 687, Training Loss: 0.179454896504196\n",
      "Iteration 688, Training Loss: 0.18058451967748798\n",
      "Iteration 689, Training Loss: 0.1793403876688351\n",
      "Iteration 690, Training Loss: 0.18049692243149773\n",
      "Iteration 691, Training Loss: 0.17941787717166172\n",
      "Iteration 692, Training Loss: 0.18058016441058905\n",
      "Iteration 693, Training Loss: 0.17932748558642794\n",
      "Iteration 694, Training Loss: 0.18047459332337995\n",
      "Iteration 695, Training Loss: 0.17942110489764185\n",
      "Iteration 696, Training Loss: 0.18054307672862244\n",
      "Iteration 697, Training Loss: 0.17931818877764436\n",
      "Iteration 698, Training Loss: 0.1804611574948225\n",
      "Iteration 699, Training Loss: 0.17939422249058828\n",
      "Iteration 700, Training Loss: 0.18052886073632687\n",
      "Iteration 701, Training Loss: 0.17930586933594103\n",
      "Iteration 702, Training Loss: 0.1804551108980084\n",
      "Iteration 703, Training Loss: 0.17937151392847475\n",
      "Iteration 704, Training Loss: 0.18050990524079502\n",
      "Iteration 705, Training Loss: 0.17929392317832885\n",
      "Iteration 706, Training Loss: 0.18044091469335916\n",
      "Iteration 707, Training Loss: 0.17935282371925484\n",
      "Iteration 708, Training Loss: 0.180484653451593\n",
      "Iteration 709, Training Loss: 0.17928822127648247\n",
      "Iteration 710, Training Loss: 0.1804263731361924\n",
      "Iteration 711, Training Loss: 0.17933648566816512\n",
      "Iteration 712, Training Loss: 0.1804677693003154\n",
      "Iteration 713, Training Loss: 0.17927194592824275\n",
      "Iteration 714, Training Loss: 0.18041748945340827\n",
      "Iteration 715, Training Loss: 0.17931316856970367\n",
      "Iteration 716, Training Loss: 0.18045694968560838\n",
      "Iteration 717, Training Loss: 0.17925060445651475\n",
      "Iteration 718, Training Loss: 0.1804025154170477\n",
      "Iteration 719, Training Loss: 0.17929050896862286\n",
      "Iteration 720, Training Loss: 0.18045015633589703\n",
      "Iteration 721, Training Loss: 0.17924251088025092\n",
      "Iteration 722, Training Loss: 0.18039006509108188\n",
      "Iteration 723, Training Loss: 0.1792796275716228\n",
      "Iteration 724, Training Loss: 0.18042637401403822\n",
      "Iteration 725, Training Loss: 0.1792279311646117\n",
      "Iteration 726, Training Loss: 0.18036643419299941\n",
      "Iteration 727, Training Loss: 0.17927587099909065\n",
      "Iteration 728, Training Loss: 0.1803989269825866\n",
      "Iteration 729, Training Loss: 0.17922375479129696\n",
      "Iteration 730, Training Loss: 0.1803520743246087\n",
      "Iteration 731, Training Loss: 0.17926427052813512\n",
      "Iteration 732, Training Loss: 0.18038130225973195\n",
      "Iteration 733, Training Loss: 0.1791959409512647\n",
      "Iteration 734, Training Loss: 0.180349720232183\n",
      "Iteration 735, Training Loss: 0.17922069442635452\n",
      "Iteration 736, Training Loss: 0.18040202530934824\n",
      "Iteration 737, Training Loss: 0.17916846583540155\n",
      "Iteration 738, Training Loss: 0.18032837886282413\n",
      "Iteration 739, Training Loss: 0.1792298089900046\n",
      "Iteration 740, Training Loss: 0.18037626345646793\n",
      "Iteration 741, Training Loss: 0.17916667076821888\n",
      "Iteration 742, Training Loss: 0.18030391332099793\n",
      "Iteration 743, Training Loss: 0.17921886747275345\n",
      "Iteration 744, Training Loss: 0.18034624990483478\n",
      "Iteration 745, Training Loss: 0.17916463292118037\n",
      "Iteration 746, Training Loss: 0.18028753349260157\n",
      "Iteration 747, Training Loss: 0.17920884581362778\n",
      "Iteration 748, Training Loss: 0.18033312828466588\n",
      "Iteration 749, Training Loss: 0.17913277538197403\n",
      "Iteration 750, Training Loss: 0.18028477650978694\n",
      "Iteration 751, Training Loss: 0.1791756568404493\n",
      "Iteration 752, Training Loss: 0.18033939805074403\n",
      "Iteration 753, Training Loss: 0.17911540366169304\n",
      "Iteration 754, Training Loss: 0.18027192261978453\n",
      "Iteration 755, Training Loss: 0.17916565842169185\n",
      "Iteration 756, Training Loss: 0.18032763559772452\n",
      "Iteration 757, Training Loss: 0.17910857668984917\n",
      "Iteration 758, Training Loss: 0.18023635807831215\n",
      "Iteration 759, Training Loss: 0.17918104511627572\n",
      "Iteration 760, Training Loss: 0.1803020315087701\n",
      "Iteration 761, Training Loss: 0.1790837178113216\n",
      "Iteration 762, Training Loss: 0.18022320113303825\n",
      "Iteration 763, Training Loss: 0.1791625655018055\n",
      "Iteration 764, Training Loss: 0.18030810356286434\n",
      "Iteration 765, Training Loss: 0.17906343153483908\n",
      "Iteration 766, Training Loss: 0.18020457251060853\n",
      "Iteration 767, Training Loss: 0.17914864454592197\n",
      "Iteration 768, Training Loss: 0.18030202058540762\n",
      "Iteration 769, Training Loss: 0.1790428328799546\n",
      "Iteration 770, Training Loss: 0.18019214541371545\n",
      "Iteration 771, Training Loss: 0.17914447633532746\n",
      "Iteration 772, Training Loss: 0.18028282550460928\n",
      "Iteration 773, Training Loss: 0.17903509739328824\n",
      "Iteration 774, Training Loss: 0.180176054690005\n",
      "Iteration 775, Training Loss: 0.17912716066583761\n",
      "Iteration 776, Training Loss: 0.18026434986668827\n",
      "Iteration 777, Training Loss: 0.17902958759025908\n",
      "Iteration 778, Training Loss: 0.1801724345005322\n",
      "Iteration 779, Training Loss: 0.1791137005202663\n",
      "Iteration 780, Training Loss: 0.18023949761475633\n",
      "Iteration 781, Training Loss: 0.1790215814180384\n",
      "Iteration 782, Training Loss: 0.18016905665455513\n",
      "Iteration 783, Training Loss: 0.1790853255153429\n",
      "Iteration 784, Training Loss: 0.18022274231672186\n",
      "Iteration 785, Training Loss: 0.1790136050565628\n",
      "Iteration 786, Training Loss: 0.18015876526213717\n",
      "Iteration 787, Training Loss: 0.17907130733843568\n",
      "Iteration 788, Training Loss: 0.18021039884877854\n",
      "Iteration 789, Training Loss: 0.1789980396366047\n",
      "Iteration 790, Training Loss: 0.1801544900626616\n",
      "Iteration 791, Training Loss: 0.17904820178969294\n",
      "Iteration 792, Training Loss: 0.18020031538434567\n",
      "Iteration 793, Training Loss: 0.17899592869684794\n",
      "Iteration 794, Training Loss: 0.1801448060590015\n",
      "Iteration 795, Training Loss: 0.17903924763552986\n",
      "Iteration 796, Training Loss: 0.18017149139330355\n",
      "Iteration 797, Training Loss: 0.17899248954632957\n",
      "Iteration 798, Training Loss: 0.18012179061545266\n",
      "Iteration 799, Training Loss: 0.17903610210748278\n",
      "Iteration 800, Training Loss: 0.18015548406986623\n",
      "Iteration 801, Training Loss: 0.17896968878119374\n",
      "Iteration 802, Training Loss: 0.18011193145995377\n",
      "Iteration 803, Training Loss: 0.17901536437745347\n",
      "Iteration 804, Training Loss: 0.18016421686818845\n",
      "Iteration 805, Training Loss: 0.17894883819183455\n",
      "Iteration 806, Training Loss: 0.1801032358618656\n",
      "Iteration 807, Training Loss: 0.17899143490893046\n",
      "Iteration 808, Training Loss: 0.18016240321911328\n",
      "Iteration 809, Training Loss: 0.17894093781665746\n",
      "Iteration 810, Training Loss: 0.18008591859988984\n",
      "Iteration 811, Training Loss: 0.17899935971121655\n",
      "Iteration 812, Training Loss: 0.18013359544917887\n",
      "Iteration 813, Training Loss: 0.17892980948593296\n",
      "Iteration 814, Training Loss: 0.180068620809002\n",
      "Iteration 815, Training Loss: 0.17898473211284482\n",
      "Iteration 816, Training Loss: 0.1801254520550601\n",
      "Iteration 817, Training Loss: 0.17891942399461222\n",
      "Iteration 818, Training Loss: 0.1800597460289053\n",
      "Iteration 819, Training Loss: 0.17897219275572104\n",
      "Iteration 820, Training Loss: 0.18011241382927165\n",
      "Iteration 821, Training Loss: 0.17890552603938698\n",
      "Iteration 822, Training Loss: 0.18005161519848437\n",
      "Iteration 823, Training Loss: 0.17895544563394714\n",
      "Iteration 824, Training Loss: 0.18010713871459072\n",
      "Iteration 825, Training Loss: 0.17889121798666197\n",
      "Iteration 826, Training Loss: 0.18004858777874796\n",
      "Iteration 827, Training Loss: 0.17894208633576003\n",
      "Iteration 828, Training Loss: 0.18009311382207696\n",
      "Iteration 829, Training Loss: 0.17888974535122948\n",
      "Iteration 830, Training Loss: 0.18002927358688142\n",
      "Iteration 831, Training Loss: 0.1789352168883047\n",
      "Iteration 832, Training Loss: 0.18006815935981604\n",
      "Iteration 833, Training Loss: 0.17888478778873124\n",
      "Iteration 834, Training Loss: 0.1800196359716412\n",
      "Iteration 835, Training Loss: 0.1789201572965983\n",
      "Iteration 836, Training Loss: 0.1800562736338393\n",
      "Iteration 837, Training Loss: 0.17886457790231777\n",
      "Iteration 838, Training Loss: 0.18000661630473497\n",
      "Iteration 839, Training Loss: 0.17890639454305626\n",
      "Iteration 840, Training Loss: 0.1800600704253492\n",
      "Iteration 841, Training Loss: 0.17885255514029122\n",
      "Iteration 842, Training Loss: 0.18000368554890675\n",
      "Iteration 843, Training Loss: 0.17889474430203398\n",
      "Iteration 844, Training Loss: 0.18004018618617804\n",
      "Iteration 845, Training Loss: 0.1788416477085125\n",
      "Iteration 846, Training Loss: 0.17999114334083155\n",
      "Iteration 847, Training Loss: 0.17888321854532419\n",
      "Iteration 848, Training Loss: 0.1800311619629051\n",
      "Iteration 849, Training Loss: 0.17884220920690397\n",
      "Iteration 850, Training Loss: 0.17998376282914869\n",
      "Iteration 851, Training Loss: 0.1788772900492147\n",
      "Iteration 852, Training Loss: 0.17999403907973482\n",
      "Iteration 853, Training Loss: 0.17883767500183415\n",
      "Iteration 854, Training Loss: 0.17998157708175397\n",
      "Iteration 855, Training Loss: 0.17885072076882264\n",
      "Iteration 856, Training Loss: 0.17999137775141874\n",
      "Iteration 857, Training Loss: 0.17883291139326385\n",
      "Iteration 858, Training Loss: 0.1799697169433743\n",
      "Iteration 859, Training Loss: 0.1788263975565459\n",
      "Iteration 860, Training Loss: 0.17999522237614954\n",
      "Iteration 861, Training Loss: 0.1788076567366927\n",
      "Iteration 862, Training Loss: 0.17997432885534395\n",
      "Iteration 863, Training Loss: 0.17881919123095916\n",
      "Iteration 864, Training Loss: 0.17998086512046427\n",
      "Iteration 865, Training Loss: 0.1788023012039797\n",
      "Iteration 866, Training Loss: 0.17995901658449998\n",
      "Iteration 867, Training Loss: 0.17881846690233627\n",
      "Iteration 868, Training Loss: 0.17994772247679827\n",
      "Iteration 869, Training Loss: 0.17881695035975356\n",
      "Iteration 870, Training Loss: 0.17992540171003743\n",
      "Iteration 871, Training Loss: 0.1788251372571849\n",
      "Iteration 872, Training Loss: 0.1799269216807449\n",
      "Iteration 873, Training Loss: 0.1788024774325736\n",
      "Iteration 874, Training Loss: 0.17993320562354911\n",
      "Iteration 875, Training Loss: 0.17878554260013588\n",
      "Iteration 876, Training Loss: 0.17995265825492482\n",
      "Iteration 877, Training Loss: 0.17876431641147278\n",
      "Iteration 878, Training Loss: 0.1799372089439747\n",
      "Iteration 879, Training Loss: 0.1787699934858613\n",
      "Iteration 880, Training Loss: 0.17994491393379608\n",
      "Iteration 881, Training Loss: 0.17877069655610325\n",
      "Iteration 882, Training Loss: 0.17991369114498001\n",
      "Iteration 883, Training Loss: 0.1787810930460268\n",
      "Iteration 884, Training Loss: 0.1799100093494188\n",
      "Iteration 885, Training Loss: 0.1787748605810498\n",
      "Iteration 886, Training Loss: 0.1798895221781202\n",
      "Iteration 887, Training Loss: 0.17878258519643103\n",
      "Early stopping at iteration 887\n",
      "Iteration 0, Training Loss: 1.0\n",
      "Iteration 1, Training Loss: 0.8876264674942123\n",
      "Iteration 2, Training Loss: 0.7789841520734808\n",
      "Iteration 3, Training Loss: 0.6885813599438364\n",
      "Iteration 4, Training Loss: 0.6295667223378377\n",
      "Iteration 5, Training Loss: 0.5776302638402226\n",
      "Iteration 6, Training Loss: 0.5302569657059086\n",
      "Iteration 7, Training Loss: 0.4873882268631486\n",
      "Iteration 8, Training Loss: 0.44806034396150285\n",
      "Iteration 9, Training Loss: 0.412518446314944\n",
      "Iteration 10, Training Loss: 0.3800949043531103\n",
      "Iteration 11, Training Loss: 0.35153461644656564\n",
      "Iteration 12, Training Loss: 0.3257758193790133\n",
      "Iteration 13, Training Loss: 0.3082050034594487\n",
      "Iteration 14, Training Loss: 0.29544681291451363\n",
      "Iteration 15, Training Loss: 0.2785164199707619\n",
      "Iteration 16, Training Loss: 0.27466061542417697\n",
      "Iteration 17, Training Loss: 0.24795298654666886\n",
      "Iteration 18, Training Loss: 0.25796871397575327\n",
      "Iteration 19, Training Loss: 0.2277606092475413\n",
      "Iteration 20, Training Loss: 0.24942651674240052\n",
      "Iteration 21, Training Loss: 0.2127161087321462\n",
      "Iteration 22, Training Loss: 0.23985769897316783\n",
      "Iteration 23, Training Loss: 0.21510966100762594\n",
      "Iteration 24, Training Loss: 0.23768825742366384\n",
      "Iteration 25, Training Loss: 0.21793869186058284\n",
      "Iteration 26, Training Loss: 0.2217193254746311\n",
      "Iteration 27, Training Loss: 0.2299886608181518\n",
      "Iteration 28, Training Loss: 0.20857321725780428\n",
      "Iteration 29, Training Loss: 0.22218821258256835\n",
      "Iteration 30, Training Loss: 0.21571690789216746\n",
      "Iteration 31, Training Loss: 0.21797052595457742\n",
      "Iteration 32, Training Loss: 0.21120541130430537\n",
      "Iteration 33, Training Loss: 0.22023116983539573\n",
      "Iteration 34, Training Loss: 0.20803057818648651\n",
      "Iteration 35, Training Loss: 0.21866705641253262\n",
      "Iteration 36, Training Loss: 0.20831140838447249\n",
      "Iteration 37, Training Loss: 0.21804316740480353\n",
      "Iteration 38, Training Loss: 0.2071664936775693\n",
      "Iteration 39, Training Loss: 0.21793147604834331\n",
      "Iteration 40, Training Loss: 0.2065861672391263\n",
      "Iteration 41, Training Loss: 0.21757490595622037\n",
      "Iteration 42, Training Loss: 0.20631688795522377\n",
      "Iteration 43, Training Loss: 0.2172398777196291\n",
      "Iteration 44, Training Loss: 0.20603123727550499\n",
      "Iteration 45, Training Loss: 0.21711986283942922\n",
      "Iteration 46, Training Loss: 0.20580124498050195\n",
      "Iteration 47, Training Loss: 0.21698893751999584\n",
      "Iteration 48, Training Loss: 0.2055350830054206\n",
      "Iteration 49, Training Loss: 0.2171299202859488\n",
      "Iteration 50, Training Loss: 0.20519697936905856\n",
      "Iteration 51, Training Loss: 0.216841789374022\n",
      "Iteration 52, Training Loss: 0.20537437088412783\n",
      "Iteration 53, Training Loss: 0.21696515625214124\n",
      "Iteration 54, Training Loss: 0.2050724787254493\n",
      "Iteration 55, Training Loss: 0.21647042773779213\n",
      "Iteration 56, Training Loss: 0.20547038323874992\n",
      "Iteration 57, Training Loss: 0.21670619705426228\n",
      "Iteration 58, Training Loss: 0.20495085167697452\n",
      "Iteration 59, Training Loss: 0.21655523299891163\n",
      "Iteration 60, Training Loss: 0.20511399284875054\n",
      "Iteration 61, Training Loss: 0.21674567839878367\n",
      "Iteration 62, Training Loss: 0.204764126858743\n",
      "Iteration 63, Training Loss: 0.21655781251795891\n",
      "Iteration 64, Training Loss: 0.20505326055974551\n",
      "Iteration 65, Training Loss: 0.21663265918293054\n",
      "Iteration 66, Training Loss: 0.20482220525748412\n",
      "Iteration 67, Training Loss: 0.2163995036349473\n",
      "Iteration 68, Training Loss: 0.20501755278763995\n",
      "Iteration 69, Training Loss: 0.21655074778494326\n",
      "Iteration 70, Training Loss: 0.20473164629224933\n",
      "Iteration 71, Training Loss: 0.21655368032574057\n",
      "Iteration 72, Training Loss: 0.20483668610201225\n",
      "Iteration 73, Training Loss: 0.21673587882647433\n",
      "Iteration 74, Training Loss: 0.20459200552472412\n",
      "Iteration 75, Training Loss: 0.2165359701395465\n",
      "Iteration 76, Training Loss: 0.20479055191041312\n",
      "Iteration 77, Training Loss: 0.2166000013726373\n",
      "Iteration 78, Training Loss: 0.2047822373157177\n",
      "Iteration 79, Training Loss: 0.21639553175551177\n",
      "Iteration 80, Training Loss: 0.20485084072297868\n",
      "Iteration 81, Training Loss: 0.21651942272810232\n",
      "Iteration 82, Training Loss: 0.20467663904420497\n",
      "Iteration 83, Training Loss: 0.2164601218726717\n",
      "Iteration 84, Training Loss: 0.20480717235942789\n",
      "Early stopping at iteration 84\n",
      "Iteration 0, Training Loss: 1.0\n",
      "Iteration 1, Training Loss: 0.9887626467494214\n",
      "Iteration 2, Training Loss: 0.9775814802650953\n",
      "Iteration 3, Training Loss: 0.9664562196131911\n",
      "Iteration 4, Training Loss: 0.9553865852645465\n",
      "Iteration 5, Training Loss: 0.944372299087645\n",
      "Iteration 6, Training Loss: 0.933413084341628\n",
      "Iteration 7, Training Loss: 0.9225086656693409\n",
      "Iteration 8, Training Loss: 0.9116587690904154\n",
      "Iteration 9, Training Loss: 0.9008631219943846\n",
      "Iteration 10, Training Loss: 0.8901214531338338\n",
      "Iteration 11, Training Loss: 0.8794334926175859\n",
      "Iteration 12, Training Loss: 0.868800030427909\n",
      "Iteration 13, Training Loss: 0.858228906337048\n",
      "Iteration 14, Training Loss: 0.8477291959918674\n",
      "Iteration 15, Training Loss: 0.8373062509830657\n",
      "Iteration 16, Training Loss: 0.8269731347247845\n",
      "Iteration 17, Training Loss: 0.8167438699965218\n",
      "Iteration 18, Training Loss: 0.806636565269152\n",
      "Iteration 19, Training Loss: 0.796676033830843\n",
      "Iteration 20, Training Loss: 0.7868594016226627\n",
      "Iteration 21, Training Loss: 0.7771860727124332\n",
      "Iteration 22, Training Loss: 0.767672399752233\n",
      "Iteration 23, Training Loss: 0.758345478349028\n",
      "Iteration 24, Training Loss: 0.7492183659395869\n",
      "Iteration 25, Training Loss: 0.7403617185057721\n",
      "Iteration 26, Training Loss: 0.731836705214727\n",
      "Iteration 27, Training Loss: 0.7236965953810212\n",
      "Iteration 28, Training Loss: 0.7159430006565594\n",
      "Iteration 29, Training Loss: 0.7085715913324498\n",
      "Iteration 30, Training Loss: 0.7015543257093082\n",
      "Iteration 31, Training Loss: 0.6948552094542204\n",
      "Iteration 32, Training Loss: 0.6884320987289101\n",
      "Iteration 33, Training Loss: 0.6822409029826834\n",
      "Iteration 34, Training Loss: 0.6762643183710622\n",
      "Iteration 35, Training Loss: 0.670467560038137\n",
      "Iteration 36, Training Loss: 0.6648313693669504\n",
      "Iteration 37, Training Loss: 0.6593315643861797\n",
      "Iteration 38, Training Loss: 0.6539564414529281\n",
      "Iteration 39, Training Loss: 0.6486922369706432\n",
      "Iteration 40, Training Loss: 0.6435222455510521\n",
      "Iteration 41, Training Loss: 0.6384376931072246\n",
      "Iteration 42, Training Loss: 0.633437978164511\n",
      "Iteration 43, Training Loss: 0.6285136684620464\n",
      "Iteration 44, Training Loss: 0.6236674785027004\n",
      "Iteration 45, Training Loss: 0.6188870937446586\n",
      "Iteration 46, Training Loss: 0.6141716909873319\n",
      "Iteration 47, Training Loss: 0.6095183639439038\n",
      "Iteration 48, Training Loss: 0.6049256425335438\n",
      "Iteration 49, Training Loss: 0.6003924489137578\n",
      "Iteration 50, Training Loss: 0.5959134092728944\n",
      "Iteration 51, Training Loss: 0.5914841582002729\n",
      "Iteration 52, Training Loss: 0.5871093904405553\n",
      "Iteration 53, Training Loss: 0.5827858993462934\n",
      "Iteration 54, Training Loss: 0.5785110308531335\n",
      "Iteration 55, Training Loss: 0.574287112852472\n",
      "Iteration 56, Training Loss: 0.5701131798813361\n",
      "Iteration 57, Training Loss: 0.5659859516425939\n",
      "Iteration 58, Training Loss: 0.561904078401844\n",
      "Iteration 59, Training Loss: 0.5578694350300978\n",
      "Iteration 60, Training Loss: 0.5538791103380226\n",
      "Iteration 61, Training Loss: 0.5499351049591067\n",
      "Iteration 62, Training Loss: 0.546038572966048\n",
      "Iteration 63, Training Loss: 0.5421838169134329\n",
      "Iteration 64, Training Loss: 0.538370006803007\n",
      "Iteration 65, Training Loss: 0.5345983836575586\n",
      "Iteration 66, Training Loss: 0.5308666496618513\n",
      "Iteration 67, Training Loss: 0.5271755463731714\n",
      "Iteration 68, Training Loss: 0.5235248665602849\n",
      "Iteration 69, Training Loss: 0.5199124073161506\n",
      "Iteration 70, Training Loss: 0.5163391741189539\n",
      "Iteration 71, Training Loss: 0.5128036832555077\n",
      "Iteration 72, Training Loss: 0.5093047876686745\n",
      "Iteration 73, Training Loss: 0.5058437240830526\n",
      "Iteration 74, Training Loss: 0.5024205516801579\n",
      "Iteration 75, Training Loss: 0.49903352117098954\n",
      "Iteration 76, Training Loss: 0.4956818265152498\n",
      "Iteration 77, Training Loss: 0.492366816742198\n",
      "Iteration 78, Training Loss: 0.48908516708732247\n",
      "Iteration 79, Training Loss: 0.4858350135248406\n",
      "Iteration 80, Training Loss: 0.48261735601217143\n",
      "Iteration 81, Training Loss: 0.47943292280841165\n",
      "Iteration 82, Training Loss: 0.47628344166107933\n",
      "Iteration 83, Training Loss: 0.47316698647663835\n",
      "Iteration 84, Training Loss: 0.47008328421694967\n",
      "Iteration 85, Training Loss: 0.4670329782465474\n",
      "Iteration 86, Training Loss: 0.4640147499539653\n",
      "Iteration 87, Training Loss: 0.4610290738576308\n",
      "Iteration 88, Training Loss: 0.4580752540472793\n",
      "Iteration 89, Training Loss: 0.45515376439362687\n",
      "Iteration 90, Training Loss: 0.45226339929223897\n",
      "Iteration 91, Training Loss: 0.4494014546496847\n",
      "Iteration 92, Training Loss: 0.44656822092624615\n",
      "Iteration 93, Training Loss: 0.4437652667671338\n",
      "Iteration 94, Training Loss: 0.440994069351073\n",
      "Iteration 95, Training Loss: 0.4382530149118554\n",
      "Iteration 96, Training Loss: 0.435542774199731\n",
      "Iteration 97, Training Loss: 0.4328616200501199\n",
      "Iteration 98, Training Loss: 0.43020987635724056\n",
      "Iteration 99, Training Loss: 0.4275870535778747\n",
      "Iteration 100, Training Loss: 0.4249901080673143\n",
      "Iteration 101, Training Loss: 0.42241954937858284\n",
      "Iteration 102, Training Loss: 0.4198780494773842\n",
      "Iteration 103, Training Loss: 0.4173650414717471\n",
      "Iteration 104, Training Loss: 0.4148783733001571\n",
      "Iteration 105, Training Loss: 0.41241824675306243\n",
      "Iteration 106, Training Loss: 0.4099837228492117\n",
      "Iteration 107, Training Loss: 0.4075761308460979\n",
      "Iteration 108, Training Loss: 0.4051953713303473\n",
      "Iteration 109, Training Loss: 0.40284082358933476\n",
      "Iteration 110, Training Loss: 0.40051001521685864\n",
      "Iteration 111, Training Loss: 0.39820440104840776\n",
      "Iteration 112, Training Loss: 0.3959256909852923\n",
      "Iteration 113, Training Loss: 0.39367085757285897\n",
      "Iteration 114, Training Loss: 0.39143941316110026\n",
      "Iteration 115, Training Loss: 0.3892336852975384\n",
      "Iteration 116, Training Loss: 0.38705147183218874\n",
      "Iteration 117, Training Loss: 0.384894109185378\n",
      "Iteration 118, Training Loss: 0.38275889065611307\n",
      "Iteration 119, Training Loss: 0.3806473333595402\n",
      "Iteration 120, Training Loss: 0.3785610003095026\n",
      "Iteration 121, Training Loss: 0.3764958361037421\n",
      "Iteration 122, Training Loss: 0.37445361881567746\n",
      "Iteration 123, Training Loss: 0.37243248328472417\n",
      "Iteration 124, Training Loss: 0.37043432725009867\n",
      "Iteration 125, Training Loss: 0.36845792839723157\n",
      "Iteration 126, Training Loss: 0.3665014855276335\n",
      "Iteration 127, Training Loss: 0.3645667726066259\n",
      "Iteration 128, Training Loss: 0.3626517189759253\n",
      "Iteration 129, Training Loss: 0.36075894111564405\n",
      "Iteration 130, Training Loss: 0.3588864833751604\n",
      "Iteration 131, Training Loss: 0.3570329053889567\n",
      "Iteration 132, Training Loss: 0.35519708381958554\n",
      "Iteration 133, Training Loss: 0.35337936546133514\n",
      "Iteration 134, Training Loss: 0.35158207781862605\n",
      "Iteration 135, Training Loss: 0.349806061773841\n",
      "Iteration 136, Training Loss: 0.34804937770041283\n",
      "Iteration 137, Training Loss: 0.34631202096583147\n",
      "Iteration 138, Training Loss: 0.3445926421865994\n",
      "Iteration 139, Training Loss: 0.3428922458567696\n",
      "Iteration 140, Training Loss: 0.34120814679398465\n",
      "Iteration 141, Training Loss: 0.33954281598266994\n",
      "Iteration 142, Training Loss: 0.3378945428929639\n",
      "Iteration 143, Training Loss: 0.33626275215694157\n",
      "Iteration 144, Training Loss: 0.33464683648176535\n",
      "Iteration 145, Training Loss: 0.33304842815913266\n",
      "Iteration 146, Training Loss: 0.3314680600836248\n",
      "Iteration 147, Training Loss: 0.3299056726216015\n",
      "Iteration 148, Training Loss: 0.32836242866159365\n",
      "Iteration 149, Training Loss: 0.3268365340281024\n",
      "Iteration 150, Training Loss: 0.32532812435431757\n",
      "Iteration 151, Training Loss: 0.32383688319529863\n",
      "Iteration 152, Training Loss: 0.32236125013737205\n",
      "Iteration 153, Training Loss: 0.3209012096298877\n",
      "Iteration 154, Training Loss: 0.3194563285482992\n",
      "Iteration 155, Training Loss: 0.3180253478157096\n",
      "Iteration 156, Training Loss: 0.31660991438090236\n",
      "Iteration 157, Training Loss: 0.3152092443428948\n",
      "Iteration 158, Training Loss: 0.3138234285319857\n",
      "Iteration 159, Training Loss: 0.3124519555586009\n",
      "Iteration 160, Training Loss: 0.3110934268440286\n",
      "Iteration 161, Training Loss: 0.30974753522604564\n",
      "Iteration 162, Training Loss: 0.3084160888499028\n",
      "Iteration 163, Training Loss: 0.3070972181476085\n",
      "Iteration 164, Training Loss: 0.30579141687473754\n",
      "Iteration 165, Training Loss: 0.3044986974286497\n",
      "Iteration 166, Training Loss: 0.3032181327906666\n",
      "Iteration 167, Training Loss: 0.3019506146819611\n",
      "Iteration 168, Training Loss: 0.30069504460859453\n",
      "Iteration 169, Training Loss: 0.2994529311354852\n",
      "Iteration 170, Training Loss: 0.29822357136245103\n",
      "Iteration 171, Training Loss: 0.2970059070891698\n",
      "Iteration 172, Training Loss: 0.2957994416679326\n",
      "Iteration 173, Training Loss: 0.29460546002928134\n",
      "Iteration 174, Training Loss: 0.29342401716733896\n",
      "Iteration 175, Training Loss: 0.292252564835858\n",
      "Iteration 176, Training Loss: 0.2910929787937575\n",
      "Iteration 177, Training Loss: 0.2899457915045137\n",
      "Iteration 178, Training Loss: 0.2888100768475428\n",
      "Iteration 179, Training Loss: 0.28768503118142136\n",
      "Iteration 180, Training Loss: 0.28656949694035067\n",
      "Iteration 181, Training Loss: 0.28546538421317735\n",
      "Iteration 182, Training Loss: 0.28437241500126725\n",
      "Iteration 183, Training Loss: 0.2832895099705474\n",
      "Iteration 184, Training Loss: 0.2822189733420017\n",
      "Iteration 185, Training Loss: 0.2811589458694062\n",
      "Iteration 186, Training Loss: 0.2801081940903256\n",
      "Iteration 187, Training Loss: 0.27906705305226875\n",
      "Iteration 188, Training Loss: 0.27803657309366486\n",
      "Iteration 189, Training Loss: 0.27701684475410343\n",
      "Iteration 190, Training Loss: 0.27600702795269483\n",
      "Iteration 191, Training Loss: 0.27500730744960156\n",
      "Iteration 192, Training Loss: 0.2740165749432589\n",
      "Iteration 193, Training Loss: 0.2730357301117493\n",
      "Iteration 194, Training Loss: 0.27206376213726347\n",
      "Iteration 195, Training Loss: 0.2711012631941795\n",
      "Iteration 196, Training Loss: 0.270147437928295\n",
      "Iteration 197, Training Loss: 0.26920376958917336\n",
      "Iteration 198, Training Loss: 0.2682698424826104\n",
      "Iteration 199, Training Loss: 0.2673482218519428\n",
      "Iteration 200, Training Loss: 0.2664329919078952\n",
      "Iteration 201, Training Loss: 0.2655286186589377\n",
      "Iteration 202, Training Loss: 0.26463189193921166\n",
      "Iteration 203, Training Loss: 0.26374516326581227\n",
      "Iteration 204, Training Loss: 0.2628666173207357\n",
      "Iteration 205, Training Loss: 0.2619985446631361\n",
      "Iteration 206, Training Loss: 0.26113663159559347\n",
      "Iteration 207, Training Loss: 0.2602858940857218\n",
      "Iteration 208, Training Loss: 0.2594426044920794\n",
      "Iteration 209, Training Loss: 0.2586103433471124\n",
      "Iteration 210, Training Loss: 0.25778524049827406\n",
      "Iteration 211, Training Loss: 0.2569707549195831\n",
      "Iteration 212, Training Loss: 0.25616358249884186\n",
      "Iteration 213, Training Loss: 0.25536694216546774\n",
      "Iteration 214, Training Loss: 0.2545759303398224\n",
      "Iteration 215, Training Loss: 0.2537941048289285\n",
      "Iteration 216, Training Loss: 0.2530200091740845\n",
      "Iteration 217, Training Loss: 0.2522550050477121\n",
      "Iteration 218, Training Loss: 0.2514999786553338\n",
      "Iteration 219, Training Loss: 0.25075387393343934\n",
      "Iteration 220, Training Loss: 0.2500156675232516\n",
      "Iteration 221, Training Loss: 0.24928599502602647\n",
      "Iteration 222, Training Loss: 0.24856547548150287\n",
      "Iteration 223, Training Loss: 0.24785401327373352\n",
      "Iteration 224, Training Loss: 0.24714900832228487\n",
      "Iteration 225, Training Loss: 0.24645547633294587\n",
      "Iteration 226, Training Loss: 0.24576049579778123\n",
      "Iteration 227, Training Loss: 0.2450915781330911\n",
      "Iteration 228, Training Loss: 0.24440417227821157\n",
      "Iteration 229, Training Loss: 0.24377244277679022\n",
      "Iteration 230, Training Loss: 0.24308307439142984\n",
      "Iteration 231, Training Loss: 0.24251638385994376\n",
      "Iteration 232, Training Loss: 0.24180278065346292\n",
      "Iteration 233, Training Loss: 0.24138607603631218\n",
      "Iteration 234, Training Loss: 0.24064362999859965\n",
      "Iteration 235, Training Loss: 0.24048135307625343\n",
      "Iteration 236, Training Loss: 0.23963886970778814\n",
      "Iteration 237, Training Loss: 0.23948484010892696\n",
      "Iteration 238, Training Loss: 0.23856407725626366\n",
      "Iteration 239, Training Loss: 0.23838262850148675\n",
      "Iteration 240, Training Loss: 0.23749517920641555\n",
      "Iteration 241, Training Loss: 0.23733096831273764\n",
      "Iteration 242, Training Loss: 0.23647215609714567\n",
      "Iteration 243, Training Loss: 0.23631857381105867\n",
      "Iteration 244, Training Loss: 0.23548803026753806\n",
      "Iteration 245, Training Loss: 0.23533267572932484\n",
      "Iteration 246, Training Loss: 0.23452049754597745\n",
      "Iteration 247, Training Loss: 0.23437565147874012\n",
      "Iteration 248, Training Loss: 0.23359269731221877\n",
      "Iteration 249, Training Loss: 0.23345232078185865\n",
      "Iteration 250, Training Loss: 0.23269587019724672\n",
      "Iteration 251, Training Loss: 0.2325483142544559\n",
      "Iteration 252, Training Loss: 0.2318225501305089\n",
      "Iteration 253, Training Loss: 0.23167541537886469\n",
      "Iteration 254, Training Loss: 0.23097985967831305\n",
      "Iteration 255, Training Loss: 0.23082317891688803\n",
      "Iteration 256, Training Loss: 0.23015873080348323\n",
      "Iteration 257, Training Loss: 0.2299905949818753\n",
      "Iteration 258, Training Loss: 0.22935732184267296\n",
      "Iteration 259, Training Loss: 0.22918155149857572\n",
      "Iteration 260, Training Loss: 0.22857797196362525\n",
      "Iteration 261, Training Loss: 0.2283947244717626\n",
      "Iteration 262, Training Loss: 0.2278280569718266\n",
      "Iteration 263, Training Loss: 0.22761563310350574\n",
      "Iteration 264, Training Loss: 0.2270767860120411\n",
      "Iteration 265, Training Loss: 0.22686388174089145\n",
      "Iteration 266, Training Loss: 0.22635593607484225\n",
      "Iteration 267, Training Loss: 0.22611902474582846\n",
      "Iteration 268, Training Loss: 0.22564464276703347\n",
      "Iteration 269, Training Loss: 0.225402410222483\n",
      "Iteration 270, Training Loss: 0.224951574733114\n",
      "Iteration 271, Training Loss: 0.22469203684529956\n",
      "Iteration 272, Training Loss: 0.22427450068187468\n",
      "Iteration 273, Training Loss: 0.22399496429977037\n",
      "Iteration 274, Training Loss: 0.22360642124368357\n",
      "Iteration 275, Training Loss: 0.22331482217846566\n",
      "Iteration 276, Training Loss: 0.22295722730557754\n",
      "Iteration 277, Training Loss: 0.22264566619566184\n",
      "Iteration 278, Training Loss: 0.22231549296157685\n",
      "Iteration 279, Training Loss: 0.2219897035764722\n",
      "Iteration 280, Training Loss: 0.22168510271984174\n",
      "Iteration 281, Training Loss: 0.22133992550674705\n",
      "Iteration 282, Training Loss: 0.22106454689805846\n",
      "Iteration 283, Training Loss: 0.2207019791236474\n",
      "Iteration 284, Training Loss: 0.22045028780295783\n",
      "Iteration 285, Training Loss: 0.22007426735257882\n",
      "Iteration 286, Training Loss: 0.2198500814575341\n",
      "Iteration 287, Training Loss: 0.21945543147004692\n",
      "Iteration 288, Training Loss: 0.21925731209276525\n",
      "Iteration 289, Training Loss: 0.2188462643922981\n",
      "Iteration 290, Training Loss: 0.21867320760577105\n",
      "Iteration 291, Training Loss: 0.21825155709855903\n",
      "Iteration 292, Training Loss: 0.21810297303865456\n",
      "Iteration 293, Training Loss: 0.21764961461712634\n",
      "Iteration 294, Training Loss: 0.21753462391292328\n",
      "Iteration 295, Training Loss: 0.2170785225652242\n",
      "Iteration 296, Training Loss: 0.21698338886233437\n",
      "Iteration 297, Training Loss: 0.21650024714648766\n",
      "Iteration 298, Training Loss: 0.21642737777566123\n",
      "Iteration 299, Training Loss: 0.21594510666266997\n",
      "Iteration 300, Training Loss: 0.2158986555830287\n",
      "Iteration 301, Training Loss: 0.2153756235662958\n",
      "Iteration 302, Training Loss: 0.21535736264662617\n",
      "Iteration 303, Training Loss: 0.21483565871569488\n",
      "Iteration 304, Training Loss: 0.2148413961618048\n",
      "Iteration 305, Training Loss: 0.21427916244760134\n",
      "Iteration 306, Training Loss: 0.21431063273877327\n",
      "Iteration 307, Training Loss: 0.21375935736874374\n",
      "Iteration 308, Training Loss: 0.21381871306652517\n",
      "Iteration 309, Training Loss: 0.21321733269921894\n",
      "Iteration 310, Training Loss: 0.21329951332156022\n",
      "Iteration 311, Training Loss: 0.21271436734484236\n",
      "Iteration 312, Training Loss: 0.21282288407934666\n",
      "Iteration 313, Training Loss: 0.21218437180568944\n",
      "Iteration 314, Training Loss: 0.212323126326329\n",
      "Iteration 315, Training Loss: 0.21169633249830025\n",
      "Iteration 316, Training Loss: 0.21185685608021507\n",
      "Iteration 317, Training Loss: 0.21118717237758658\n",
      "Iteration 318, Training Loss: 0.21138056303941566\n",
      "Iteration 319, Training Loss: 0.21070694393010042\n",
      "Iteration 320, Training Loss: 0.21092449458845633\n",
      "Iteration 321, Training Loss: 0.21021880118571099\n",
      "Iteration 322, Training Loss: 0.21045737265255146\n",
      "Iteration 323, Training Loss: 0.2097517064951198\n",
      "Iteration 324, Training Loss: 0.21001803028143692\n",
      "Iteration 325, Training Loss: 0.20927553583768954\n",
      "Iteration 326, Training Loss: 0.20956779079089188\n",
      "Iteration 327, Training Loss: 0.2088245196733731\n",
      "Iteration 328, Training Loss: 0.2091503844361209\n",
      "Iteration 329, Training Loss: 0.20835607767796485\n",
      "Iteration 330, Training Loss: 0.20870990714327464\n",
      "Iteration 331, Training Loss: 0.20792896179054499\n",
      "Iteration 332, Training Loss: 0.2083156932691463\n",
      "Iteration 333, Training Loss: 0.20746561609204622\n",
      "Iteration 334, Training Loss: 0.20788723755669655\n",
      "Iteration 335, Training Loss: 0.2070602911532024\n",
      "Iteration 336, Training Loss: 0.20751440550719705\n",
      "Iteration 337, Training Loss: 0.20660671736530015\n",
      "Iteration 338, Training Loss: 0.207096414169373\n",
      "Iteration 339, Training Loss: 0.20621974475523883\n",
      "Iteration 340, Training Loss: 0.20675453756436582\n",
      "Iteration 341, Training Loss: 0.20577202820687715\n",
      "Iteration 342, Training Loss: 0.20634651663570133\n",
      "Iteration 343, Training Loss: 0.2054121625122714\n",
      "Iteration 344, Training Loss: 0.206028447113289\n",
      "Iteration 345, Training Loss: 0.2049714720619244\n",
      "Iteration 346, Training Loss: 0.20561818230621903\n",
      "Iteration 347, Training Loss: 0.20464044237970574\n",
      "Iteration 348, Training Loss: 0.2053303917060941\n",
      "Iteration 349, Training Loss: 0.20419665812624396\n",
      "Iteration 350, Training Loss: 0.2049247464374415\n",
      "Iteration 351, Training Loss: 0.2039040130524085\n",
      "Iteration 352, Training Loss: 0.20467822355691057\n",
      "Iteration 353, Training Loss: 0.20344868108978012\n",
      "Iteration 354, Training Loss: 0.20425597043295718\n",
      "Iteration 355, Training Loss: 0.20321080423553034\n",
      "Iteration 356, Training Loss: 0.20406465017616535\n",
      "Iteration 357, Training Loss: 0.20272896204064064\n",
      "Iteration 358, Training Loss: 0.2036115199329855\n",
      "Iteration 359, Training Loss: 0.20255622556259353\n",
      "Iteration 360, Training Loss: 0.20347990797376192\n",
      "Iteration 361, Training Loss: 0.2020456108908805\n",
      "Iteration 362, Training Loss: 0.2030002473730706\n",
      "Iteration 363, Training Loss: 0.2019318954344975\n",
      "Iteration 364, Training Loss: 0.20293370997094484\n",
      "Iteration 365, Training Loss: 0.20139314747794218\n",
      "Iteration 366, Training Loss: 0.20242764951338538\n",
      "Iteration 367, Training Loss: 0.2013313783961513\n",
      "Iteration 368, Training Loss: 0.20241554658102476\n",
      "Iteration 369, Training Loss: 0.20078006743110619\n",
      "Iteration 370, Training Loss: 0.2018922400518911\n",
      "Iteration 371, Training Loss: 0.20077137970228173\n",
      "Iteration 372, Training Loss: 0.2019415240153066\n",
      "Iteration 373, Training Loss: 0.20019718405635611\n",
      "Iteration 374, Training Loss: 0.20139556230815858\n",
      "Iteration 375, Training Loss: 0.20025256914266523\n",
      "Iteration 376, Training Loss: 0.20153166502376857\n",
      "Iteration 377, Training Loss: 0.19964478645033873\n",
      "Iteration 378, Training Loss: 0.20095468904887714\n",
      "Iteration 379, Training Loss: 0.19977456488744635\n",
      "Iteration 380, Training Loss: 0.20115842825882416\n",
      "Iteration 381, Training Loss: 0.1991380922885787\n",
      "Iteration 382, Training Loss: 0.20053141272226815\n",
      "Iteration 383, Training Loss: 0.19934725047051358\n",
      "Iteration 384, Training Loss: 0.20081314814966889\n",
      "Iteration 385, Training Loss: 0.19867772683509138\n",
      "Iteration 386, Training Loss: 0.20013845196688107\n",
      "Iteration 387, Training Loss: 0.19894015393725265\n",
      "Iteration 388, Training Loss: 0.20047273997665557\n",
      "Iteration 389, Training Loss: 0.19824820907963206\n",
      "Iteration 390, Training Loss: 0.19975637368987642\n",
      "Iteration 391, Training Loss: 0.19855263666974785\n",
      "Iteration 392, Training Loss: 0.20013695999451317\n",
      "Iteration 393, Training Loss: 0.19785637861404398\n",
      "Iteration 394, Training Loss: 0.1994307207381016\n",
      "Iteration 395, Training Loss: 0.1981822009182589\n",
      "Iteration 396, Training Loss: 0.19981917033725002\n",
      "Iteration 397, Training Loss: 0.1975083692747483\n",
      "Iteration 398, Training Loss: 0.19915091875549676\n",
      "Iteration 399, Training Loss: 0.19783191513418588\n",
      "Iteration 400, Training Loss: 0.19953476714807242\n",
      "Iteration 401, Training Loss: 0.1971797458767383\n",
      "Iteration 402, Training Loss: 0.19888468033436985\n",
      "Iteration 403, Training Loss: 0.19750607017978902\n",
      "Iteration 404, Training Loss: 0.1992917445275234\n",
      "Iteration 405, Training Loss: 0.19688972975015614\n",
      "Iteration 406, Training Loss: 0.19865551069467588\n",
      "Iteration 407, Training Loss: 0.1972197911762006\n",
      "Iteration 408, Training Loss: 0.1990516348019708\n",
      "Iteration 409, Training Loss: 0.19666186469758834\n",
      "Iteration 410, Training Loss: 0.19851621226217148\n",
      "Iteration 411, Training Loss: 0.19694937269124296\n",
      "Iteration 412, Training Loss: 0.19885430051089578\n",
      "Iteration 413, Training Loss: 0.19653272990483311\n",
      "Iteration 414, Training Loss: 0.19846637763320502\n",
      "Iteration 415, Training Loss: 0.19677915934362514\n",
      "Iteration 416, Training Loss: 0.19863294103852852\n",
      "Iteration 417, Training Loss: 0.1965850072785973\n",
      "Iteration 418, Training Loss: 0.19841918998426972\n",
      "Iteration 419, Training Loss: 0.19671239300818674\n",
      "Iteration 420, Training Loss: 0.1983672953459897\n",
      "Iteration 421, Training Loss: 0.19669169036732786\n",
      "Iteration 422, Training Loss: 0.19822344435989572\n",
      "Iteration 423, Training Loss: 0.19673262367831204\n",
      "Early stopping at iteration 423\n",
      "Iteration 0, Training Loss: 1.0\n",
      "Iteration 1, Training Loss: 0.8876264674942123\n",
      "Iteration 2, Training Loss: 0.7817129903701945\n",
      "Iteration 3, Training Loss: 0.6941685672174122\n",
      "Iteration 4, Training Loss: 0.6379984565944582\n",
      "Iteration 5, Training Loss: 0.5906998943020028\n",
      "Iteration 6, Training Loss: 0.5478722363484596\n",
      "Iteration 7, Training Loss: 0.5102288557306759\n",
      "Iteration 8, Training Loss: 0.4759538768430714\n",
      "Iteration 9, Training Loss: 0.4457194033603837\n",
      "Iteration 10, Training Loss: 0.4179579780021749\n",
      "Iteration 11, Training Loss: 0.3943745616525365\n",
      "Iteration 12, Training Loss: 0.3718985948622439\n",
      "Iteration 13, Training Loss: 0.35863708988775933\n",
      "Iteration 14, Training Loss: 0.34279443020553096\n",
      "Iteration 15, Training Loss: 0.33600295807874353\n",
      "Iteration 16, Training Loss: 0.3184721187711091\n",
      "Iteration 17, Training Loss: 0.3128455362127459\n",
      "Iteration 18, Training Loss: 0.30066322641173193\n",
      "Iteration 19, Training Loss: 0.29523373558103894\n",
      "Iteration 20, Training Loss: 0.28896227196609553\n",
      "Iteration 21, Training Loss: 0.28093379721326817\n",
      "Iteration 22, Training Loss: 0.28150002477059827\n",
      "Iteration 23, Training Loss: 0.2690206840577443\n",
      "Iteration 24, Training Loss: 0.27657108870707403\n",
      "Iteration 25, Training Loss: 0.2593230675222457\n",
      "Iteration 26, Training Loss: 0.27309474703026154\n",
      "Iteration 27, Training Loss: 0.2519411988563088\n",
      "Iteration 28, Training Loss: 0.2710844097150118\n",
      "Iteration 29, Training Loss: 0.24613986861542944\n",
      "Iteration 30, Training Loss: 0.27005263499948523\n",
      "Iteration 31, Training Loss: 0.242423218791203\n",
      "Iteration 32, Training Loss: 0.27033722589068676\n",
      "Iteration 33, Training Loss: 0.24129520701343424\n",
      "Iteration 34, Training Loss: 0.2705722110672056\n",
      "Iteration 35, Training Loss: 0.24319836668888892\n",
      "Iteration 36, Training Loss: 0.26965129055167636\n",
      "Iteration 37, Training Loss: 0.24678372974557722\n",
      "Iteration 38, Training Loss: 0.26733614424723584\n",
      "Iteration 39, Training Loss: 0.2506202317945764\n",
      "Iteration 40, Training Loss: 0.26486046720223055\n",
      "Iteration 41, Training Loss: 0.2533201644321606\n",
      "Iteration 42, Training Loss: 0.2634732838659485\n",
      "Iteration 43, Training Loss: 0.2544174745878204\n",
      "Early stopping at iteration 43\n",
      "Iteration 0, Training Loss: 1.0\n",
      "Iteration 1, Training Loss: 0.9887626467494214\n",
      "Iteration 2, Training Loss: 0.9776376670313482\n",
      "Iteration 3, Training Loss: 0.966623937110456\n",
      "Iteration 4, Training Loss: 0.9557203444887726\n",
      "Iteration 5, Training Loss: 0.9449257877933063\n",
      "Iteration 6, Training Loss: 0.9342391766647941\n",
      "Iteration 7, Training Loss: 0.9236594316475676\n",
      "Iteration 8, Training Loss: 0.9131854840805133\n",
      "Iteration 9, Training Loss: 0.9028162759891292\n",
      "Iteration 10, Training Loss: 0.8925507599786593\n",
      "Iteration 11, Training Loss: 0.8823878991282937\n",
      "Iteration 12, Training Loss: 0.8723269738692703\n",
      "Iteration 13, Training Loss: 0.8623721714278064\n",
      "Iteration 14, Training Loss: 0.8525292964869255\n",
      "Iteration 15, Training Loss: 0.8428061214476003\n",
      "Iteration 16, Training Loss: 0.8332061509703407\n",
      "Iteration 17, Training Loss: 0.8237402665194365\n",
      "Iteration 18, Training Loss: 0.8144236191917144\n",
      "Iteration 19, Training Loss: 0.8052677375049969\n",
      "Iteration 20, Training Loss: 0.7962801020069269\n",
      "Iteration 21, Training Loss: 0.7874648901861114\n",
      "Iteration 22, Training Loss: 0.778818151106567\n",
      "Iteration 23, Training Loss: 0.7703449339376716\n",
      "Iteration 24, Training Loss: 0.7620664004750548\n",
      "Iteration 25, Training Loss: 0.7539828200999578\n",
      "Iteration 26, Training Loss: 0.746125440082415\n",
      "Iteration 27, Training Loss: 0.7385499173623236\n",
      "Iteration 28, Training Loss: 0.7312786927443584\n",
      "Iteration 29, Training Loss: 0.7243434120744435\n",
      "Iteration 30, Training Loss: 0.7177292479763724\n",
      "Iteration 31, Training Loss: 0.7114399779065584\n",
      "Iteration 32, Training Loss: 0.7054375368992981\n",
      "Iteration 33, Training Loss: 0.699701084526258\n",
      "Iteration 34, Training Loss: 0.6942082796293039\n",
      "Iteration 35, Training Loss: 0.6889283864761854\n",
      "Iteration 36, Training Loss: 0.683829903665056\n",
      "Iteration 37, Training Loss: 0.6788917981506428\n",
      "Iteration 38, Training Loss: 0.6741119683711014\n",
      "Iteration 39, Training Loss: 0.6694680272219704\n",
      "Iteration 40, Training Loss: 0.6649489881538914\n",
      "Iteration 41, Training Loss: 0.6605474500403617\n",
      "Iteration 42, Training Loss: 0.6562426458748382\n",
      "Iteration 43, Training Loss: 0.6520343982209871\n",
      "Iteration 44, Training Loss: 0.6479123425730257\n",
      "Iteration 45, Training Loss: 0.643881721380413\n",
      "Iteration 46, Training Loss: 0.6399334346954837\n",
      "Iteration 47, Training Loss: 0.6360536407264259\n",
      "Iteration 48, Training Loss: 0.6322418987685208\n",
      "Iteration 49, Training Loss: 0.628497762870844\n",
      "Iteration 50, Training Loss: 0.6248157226941828\n",
      "Iteration 51, Training Loss: 0.6211953899850906\n",
      "Iteration 52, Training Loss: 0.6176337011093466\n",
      "Iteration 53, Training Loss: 0.6141276933044644\n",
      "Iteration 54, Training Loss: 0.6106815584794781\n",
      "Iteration 55, Training Loss: 0.6072915005791788\n",
      "Iteration 56, Training Loss: 0.6039554233814904\n",
      "Iteration 57, Training Loss: 0.6006696027731518\n",
      "Iteration 58, Training Loss: 0.5974340566993295\n",
      "Iteration 59, Training Loss: 0.5942479415293818\n",
      "Iteration 60, Training Loss: 0.5911097398056114\n",
      "Iteration 61, Training Loss: 0.5880167503794989\n",
      "Iteration 62, Training Loss: 0.5849713708423198\n",
      "Iteration 63, Training Loss: 0.5819709906498057\n",
      "Iteration 64, Training Loss: 0.5790137816758877\n",
      "Iteration 65, Training Loss: 0.5761025362396132\n",
      "Iteration 66, Training Loss: 0.5732310852032779\n",
      "Iteration 67, Training Loss: 0.5704028978548028\n",
      "Iteration 68, Training Loss: 0.5676182734132491\n",
      "Iteration 69, Training Loss: 0.5648734101457584\n",
      "Iteration 70, Training Loss: 0.5621648166896277\n",
      "Iteration 71, Training Loss: 0.5594979325019244\n",
      "Iteration 72, Training Loss: 0.5568711694358965\n",
      "Iteration 73, Training Loss: 0.5542794509881085\n",
      "Iteration 74, Training Loss: 0.5517266840816336\n",
      "Iteration 75, Training Loss: 0.5492057941060944\n",
      "Iteration 76, Training Loss: 0.5467212465916371\n",
      "Iteration 77, Training Loss: 0.5442709387375274\n",
      "Iteration 78, Training Loss: 0.5418544463286331\n",
      "Iteration 79, Training Loss: 0.5394744573710231\n",
      "Iteration 80, Training Loss: 0.537127957137326\n",
      "Iteration 81, Training Loss: 0.5348141252892784\n",
      "Iteration 82, Training Loss: 0.5325326908245026\n",
      "Iteration 83, Training Loss: 0.5302834663747805\n",
      "Iteration 84, Training Loss: 0.5280665175385618\n",
      "Iteration 85, Training Loss: 0.5258830836025469\n",
      "Iteration 86, Training Loss: 0.5237306973380665\n",
      "Iteration 87, Training Loss: 0.5216064282975098\n",
      "Iteration 88, Training Loss: 0.519512609872722\n",
      "Iteration 89, Training Loss: 0.517446493374374\n",
      "Iteration 90, Training Loss: 0.5154112375464279\n",
      "Iteration 91, Training Loss: 0.5134036880274383\n",
      "Iteration 92, Training Loss: 0.5114224856998209\n",
      "Iteration 93, Training Loss: 0.5094681926435629\n",
      "Iteration 94, Training Loss: 0.50753916506183\n",
      "Iteration 95, Training Loss: 0.5056374353224761\n",
      "Iteration 96, Training Loss: 0.5037609606149902\n",
      "Iteration 97, Training Loss: 0.5019082225438183\n",
      "Iteration 98, Training Loss: 0.5000808286991824\n",
      "Iteration 99, Training Loss: 0.4982751915195223\n",
      "Iteration 100, Training Loss: 0.49649413713104323\n",
      "Iteration 101, Training Loss: 0.49473640592068957\n",
      "Iteration 102, Training Loss: 0.4930032484970674\n",
      "Iteration 103, Training Loss: 0.4912930161880632\n",
      "Iteration 104, Training Loss: 0.4896032357011511\n",
      "Iteration 105, Training Loss: 0.48793719634787747\n",
      "Iteration 106, Training Loss: 0.48629536745184093\n",
      "Iteration 107, Training Loss: 0.4846743081562202\n",
      "Iteration 108, Training Loss: 0.48307545383402195\n",
      "Iteration 109, Training Loss: 0.4814968369470754\n",
      "Iteration 110, Training Loss: 0.47993830349084243\n",
      "Iteration 111, Training Loss: 0.4784024020029743\n",
      "Iteration 112, Training Loss: 0.4768885951413495\n",
      "Iteration 113, Training Loss: 0.4753941776983287\n",
      "Iteration 114, Training Loss: 0.4739215122538522\n",
      "Iteration 115, Training Loss: 0.4724690605079417\n",
      "Iteration 116, Training Loss: 0.4710342757178721\n",
      "Iteration 117, Training Loss: 0.4696184998492356\n",
      "Iteration 118, Training Loss: 0.46821975290826484\n",
      "Iteration 119, Training Loss: 0.4668398110959717\n",
      "Iteration 120, Training Loss: 0.46547824653725967\n",
      "Iteration 121, Training Loss: 0.4641347333093363\n",
      "Iteration 122, Training Loss: 0.4628081415995214\n",
      "Iteration 123, Training Loss: 0.46149978999176705\n",
      "Iteration 124, Training Loss: 0.4602092367750724\n",
      "Iteration 125, Training Loss: 0.45893359742419787\n",
      "Iteration 126, Training Loss: 0.4576749969549979\n",
      "Iteration 127, Training Loss: 0.4564321032992944\n",
      "Iteration 128, Training Loss: 0.4552061178581169\n",
      "Iteration 129, Training Loss: 0.45399590132765455\n",
      "Iteration 130, Training Loss: 0.4528025140049589\n",
      "Iteration 131, Training Loss: 0.45162675914574035\n",
      "Iteration 132, Training Loss: 0.45046646736532153\n",
      "Iteration 133, Training Loss: 0.4493209923762865\n",
      "Iteration 134, Training Loss: 0.4481905969965132\n",
      "Iteration 135, Training Loss: 0.4470759197532632\n",
      "Iteration 136, Training Loss: 0.4459761483147141\n",
      "Iteration 137, Training Loss: 0.4448910226957355\n",
      "Iteration 138, Training Loss: 0.44381840624360835\n",
      "Iteration 139, Training Loss: 0.4427603940741656\n",
      "Iteration 140, Training Loss: 0.4417152767689553\n",
      "Iteration 141, Training Loss: 0.44068435053040556\n",
      "Iteration 142, Training Loss: 0.4396665342403147\n",
      "Iteration 143, Training Loss: 0.4386618225080339\n",
      "Iteration 144, Training Loss: 0.4376692597910287\n",
      "Iteration 145, Training Loss: 0.43668844306360227\n",
      "Iteration 146, Training Loss: 0.43572050116112726\n",
      "Iteration 147, Training Loss: 0.4347647018892289\n",
      "Iteration 148, Training Loss: 0.4338202695755828\n",
      "Iteration 149, Training Loss: 0.4328871011985844\n",
      "Iteration 150, Training Loss: 0.4319667335967045\n",
      "Iteration 151, Training Loss: 0.4310580901736282\n",
      "Iteration 152, Training Loss: 0.43016016691302134\n",
      "Iteration 153, Training Loss: 0.42927477027265726\n",
      "Iteration 154, Training Loss: 0.42840170213436896\n",
      "Iteration 155, Training Loss: 0.42753972628566383\n",
      "Iteration 156, Training Loss: 0.4266883847144368\n",
      "Iteration 157, Training Loss: 0.42584773799344794\n",
      "Iteration 158, Training Loss: 0.42501691861535124\n",
      "Iteration 159, Training Loss: 0.42419661964749866\n",
      "Iteration 160, Training Loss: 0.42338821741338106\n",
      "Iteration 161, Training Loss: 0.42259183339678996\n",
      "Iteration 162, Training Loss: 0.4218067454326921\n",
      "Iteration 163, Training Loss: 0.4210316770966659\n",
      "Iteration 164, Training Loss: 0.4202650149499953\n",
      "Iteration 165, Training Loss: 0.41950899136049863\n",
      "Iteration 166, Training Loss: 0.41876187162993744\n",
      "Iteration 167, Training Loss: 0.41802256171512703\n",
      "Iteration 168, Training Loss: 0.4172939275422103\n",
      "Iteration 169, Training Loss: 0.41657369142372047\n",
      "Iteration 170, Training Loss: 0.4158615946788412\n",
      "Iteration 171, Training Loss: 0.4151595278503685\n",
      "Iteration 172, Training Loss: 0.414465390250579\n",
      "Iteration 173, Training Loss: 0.41377976894508267\n",
      "Iteration 174, Training Loss: 0.41310301575449754\n",
      "Iteration 175, Training Loss: 0.41243487952666874\n",
      "Iteration 176, Training Loss: 0.4117751142930155\n",
      "Iteration 177, Training Loss: 0.4111242826039787\n",
      "Iteration 178, Training Loss: 0.4104799518010582\n",
      "Iteration 179, Training Loss: 0.40984394183997086\n",
      "Iteration 180, Training Loss: 0.4092154646401233\n",
      "Iteration 181, Training Loss: 0.4085952405919299\n",
      "Iteration 182, Training Loss: 0.40798246135774024\n",
      "Iteration 183, Training Loss: 0.4073779401837295\n",
      "Iteration 184, Training Loss: 0.4067802548105971\n",
      "Iteration 185, Training Loss: 0.40619043947502387\n",
      "Iteration 186, Training Loss: 0.40560785077059586\n",
      "Iteration 187, Training Loss: 0.4050332881231176\n",
      "Iteration 188, Training Loss: 0.40446554115582345\n",
      "Iteration 189, Training Loss: 0.4039037691469\n",
      "Iteration 190, Training Loss: 0.4033497191171463\n",
      "Iteration 191, Training Loss: 0.40280199817637097\n",
      "Iteration 192, Training Loss: 0.4022607505542968\n",
      "Iteration 193, Training Loss: 0.40172528362166754\n",
      "Iteration 194, Training Loss: 0.4011966535907152\n",
      "Iteration 195, Training Loss: 0.4006748242593088\n",
      "Iteration 196, Training Loss: 0.40015973477026323\n",
      "Iteration 197, Training Loss: 0.39965223503847497\n",
      "Iteration 198, Training Loss: 0.39914960045751025\n",
      "Iteration 199, Training Loss: 0.39865239095199134\n",
      "Iteration 200, Training Loss: 0.3981603639775001\n",
      "Iteration 201, Training Loss: 0.3976752180931461\n",
      "Iteration 202, Training Loss: 0.3971960678864856\n",
      "Iteration 203, Training Loss: 0.396722546140081\n",
      "Iteration 204, Training Loss: 0.39625490394320667\n",
      "Iteration 205, Training Loss: 0.3957932415817369\n",
      "Iteration 206, Training Loss: 0.39533743669303556\n",
      "Iteration 207, Training Loss: 0.3948875553119213\n",
      "Iteration 208, Training Loss: 0.39444220736969665\n",
      "Iteration 209, Training Loss: 0.39400381577571864\n",
      "Iteration 210, Training Loss: 0.3935702603420287\n",
      "Iteration 211, Training Loss: 0.3931417187474143\n",
      "Iteration 212, Training Loss: 0.39271801527444905\n",
      "Iteration 213, Training Loss: 0.39229949715091045\n",
      "Iteration 214, Training Loss: 0.3918856271942254\n",
      "Iteration 215, Training Loss: 0.39147684754236695\n",
      "Iteration 216, Training Loss: 0.39107325400333726\n",
      "Iteration 217, Training Loss: 0.3906731035926912\n",
      "Iteration 218, Training Loss: 0.39027883446339406\n",
      "Iteration 219, Training Loss: 0.3898877333777837\n",
      "Iteration 220, Training Loss: 0.3895006349514412\n",
      "Iteration 221, Training Loss: 0.38911891931017656\n",
      "Iteration 222, Training Loss: 0.3887415596559971\n",
      "Iteration 223, Training Loss: 0.3883693931079712\n",
      "Iteration 224, Training Loss: 0.38800137670968265\n",
      "Iteration 225, Training Loss: 0.38763811615028076\n",
      "Iteration 226, Training Loss: 0.387279287199828\n",
      "Iteration 227, Training Loss: 0.38692371041259555\n",
      "Iteration 228, Training Loss: 0.38657298730134854\n",
      "Iteration 229, Training Loss: 0.3862247403549567\n",
      "Iteration 230, Training Loss: 0.3858821819327234\n",
      "Iteration 231, Training Loss: 0.38554245858852143\n",
      "Iteration 232, Training Loss: 0.3852072431717888\n",
      "Iteration 233, Training Loss: 0.38487659057947937\n",
      "Iteration 234, Training Loss: 0.38455084648926524\n",
      "Iteration 235, Training Loss: 0.38422734230253475\n",
      "Iteration 236, Training Loss: 0.38390838369121355\n",
      "Iteration 237, Training Loss: 0.383592123144849\n",
      "Iteration 238, Training Loss: 0.38328021802229384\n",
      "Iteration 239, Training Loss: 0.38297167879993366\n",
      "Iteration 240, Training Loss: 0.3826671673965474\n",
      "Iteration 241, Training Loss: 0.38236545180399\n",
      "Iteration 242, Training Loss: 0.382068837363915\n",
      "Iteration 243, Training Loss: 0.3817741039714067\n",
      "Iteration 244, Training Loss: 0.38148386282841545\n",
      "Iteration 245, Training Loss: 0.38119516579562995\n",
      "Iteration 246, Training Loss: 0.3809110242591375\n",
      "Iteration 247, Training Loss: 0.3806293356314682\n",
      "Iteration 248, Training Loss: 0.3803516948942018\n",
      "Iteration 249, Training Loss: 0.38007690700363034\n",
      "Iteration 250, Training Loss: 0.37980656421942965\n",
      "Iteration 251, Training Loss: 0.3795373142426702\n",
      "Iteration 252, Training Loss: 0.3792735309643101\n",
      "Iteration 253, Training Loss: 0.37900939306167514\n",
      "Iteration 254, Training Loss: 0.3787502602693712\n",
      "Iteration 255, Training Loss: 0.3784935819981797\n",
      "Iteration 256, Training Loss: 0.37823926118061024\n",
      "Iteration 257, Training Loss: 0.37798846993761714\n",
      "Iteration 258, Training Loss: 0.37773994110467196\n",
      "Iteration 259, Training Loss: 0.3774945578380124\n",
      "Iteration 260, Training Loss: 0.37725166406579824\n",
      "Iteration 261, Training Loss: 0.3770124780328063\n",
      "Iteration 262, Training Loss: 0.37677546006584933\n",
      "Iteration 263, Training Loss: 0.37654014351703013\n",
      "Iteration 264, Training Loss: 0.3763080990115726\n",
      "Iteration 265, Training Loss: 0.3760794141025647\n",
      "Iteration 266, Training Loss: 0.3758521006073787\n",
      "Iteration 267, Training Loss: 0.37562813416735263\n",
      "Iteration 268, Training Loss: 0.37540482657646457\n",
      "Iteration 269, Training Loss: 0.37518586391354286\n",
      "Iteration 270, Training Loss: 0.37496610903986244\n",
      "Iteration 271, Training Loss: 0.3747524687496172\n",
      "Iteration 272, Training Loss: 0.37453784427088177\n",
      "Iteration 273, Training Loss: 0.3743266996926392\n",
      "Iteration 274, Training Loss: 0.3741183560292699\n",
      "Iteration 275, Training Loss: 0.3739114272957757\n",
      "Iteration 276, Training Loss: 0.37370873489539735\n",
      "Iteration 277, Training Loss: 0.3735052954529032\n",
      "Iteration 278, Training Loss: 0.3733079289799194\n",
      "Iteration 279, Training Loss: 0.37310979914108733\n",
      "Iteration 280, Training Loss: 0.37291616493485386\n",
      "Iteration 281, Training Loss: 0.37272196958342124\n",
      "Iteration 282, Training Loss: 0.3725333793711332\n",
      "Iteration 283, Training Loss: 0.3723430155515462\n",
      "Iteration 284, Training Loss: 0.3721580996168801\n",
      "Iteration 285, Training Loss: 0.371971606253143\n",
      "Iteration 286, Training Loss: 0.3717912278842146\n",
      "Iteration 287, Training Loss: 0.37160896270839106\n",
      "Iteration 288, Training Loss: 0.37143243657442765\n",
      "Iteration 289, Training Loss: 0.3712541636214132\n",
      "Iteration 290, Training Loss: 0.3710821343408085\n",
      "Iteration 291, Training Loss: 0.37090742761166745\n",
      "Iteration 292, Training Loss: 0.3707398710690705\n",
      "Iteration 293, Training Loss: 0.37056872130500434\n",
      "Iteration 294, Training Loss: 0.37040444582445287\n",
      "Iteration 295, Training Loss: 0.3702393235924152\n",
      "Iteration 296, Training Loss: 0.3700776747273401\n",
      "Iteration 297, Training Loss: 0.3699170503980988\n",
      "Iteration 298, Training Loss: 0.3697566707738718\n",
      "Iteration 299, Training Loss: 0.36960172907015243\n",
      "Iteration 300, Training Loss: 0.3694434727120613\n",
      "Iteration 301, Training Loss: 0.36929276087155644\n",
      "Iteration 302, Training Loss: 0.36913915819527016\n",
      "Iteration 303, Training Loss: 0.36899021598530257\n",
      "Iteration 304, Training Loss: 0.36884014315391983\n",
      "Iteration 305, Training Loss: 0.368694454500527\n",
      "Iteration 306, Training Loss: 0.3685481262959922\n",
      "Iteration 307, Training Loss: 0.3684045426151373\n",
      "Iteration 308, Training Loss: 0.36826259045362086\n",
      "Iteration 309, Training Loss: 0.36812029312172373\n",
      "Iteration 310, Training Loss: 0.36798239262167176\n",
      "Iteration 311, Training Loss: 0.3678440586579451\n",
      "Iteration 312, Training Loss: 0.36770880896608776\n",
      "Iteration 313, Training Loss: 0.3675733490194507\n",
      "Iteration 314, Training Loss: 0.3674408008844148\n",
      "Iteration 315, Training Loss: 0.36730968434378103\n",
      "Iteration 316, Training Loss: 0.3671787134989325\n",
      "Iteration 317, Training Loss: 0.36705076723841196\n",
      "Iteration 318, Training Loss: 0.3669222891732361\n",
      "Iteration 319, Training Loss: 0.36679833828444947\n",
      "Iteration 320, Training Loss: 0.36667073031186226\n",
      "Iteration 321, Training Loss: 0.3665518621719944\n",
      "Iteration 322, Training Loss: 0.3664262918254982\n",
      "Iteration 323, Training Loss: 0.36630831105872663\n",
      "Iteration 324, Training Loss: 0.3661865022091003\n",
      "Iteration 325, Training Loss: 0.36607121362209355\n",
      "Iteration 326, Training Loss: 0.36595147819193674\n",
      "Iteration 327, Training Loss: 0.3658391661772202\n",
      "Iteration 328, Training Loss: 0.365721902710834\n",
      "Iteration 329, Training Loss: 0.3656110836723433\n",
      "Iteration 330, Training Loss: 0.3654981842335353\n",
      "Iteration 331, Training Loss: 0.3653878217406961\n",
      "Iteration 332, Training Loss: 0.3652786660763888\n",
      "Iteration 333, Training Loss: 0.36517049881308733\n",
      "Iteration 334, Training Loss: 0.36506149373640207\n",
      "Iteration 335, Training Loss: 0.3649579170791591\n",
      "Iteration 336, Training Loss: 0.3648504146276103\n",
      "Iteration 337, Training Loss: 0.3647496269262655\n",
      "Iteration 338, Training Loss: 0.3646430284339584\n",
      "Iteration 339, Training Loss: 0.3645462452997083\n",
      "Iteration 340, Training Loss: 0.36444081256142113\n",
      "Iteration 341, Training Loss: 0.3643459941070055\n",
      "Iteration 342, Training Loss: 0.3642447040120122\n",
      "Iteration 343, Training Loss: 0.36414922692295115\n",
      "Iteration 344, Training Loss: 0.3640518574615078\n",
      "Iteration 345, Training Loss: 0.36395839405707936\n",
      "Iteration 346, Training Loss: 0.36386266325709626\n",
      "Iteration 347, Training Loss: 0.36377096675370885\n",
      "Iteration 348, Training Loss: 0.3636764670790192\n",
      "Iteration 349, Training Loss: 0.36358936583654994\n",
      "Iteration 350, Training Loss: 0.36349474905745066\n",
      "Iteration 351, Training Loss: 0.3634095674711347\n",
      "Iteration 352, Training Loss: 0.36331929139249725\n",
      "Iteration 353, Training Loss: 0.3632339271438728\n",
      "Iteration 354, Training Loss: 0.3631455784513364\n",
      "Iteration 355, Training Loss: 0.3630647943539664\n",
      "Iteration 356, Training Loss: 0.362975051785213\n",
      "Iteration 357, Training Loss: 0.3628954457564544\n",
      "Iteration 358, Training Loss: 0.36281070218755135\n",
      "Iteration 359, Training Loss: 0.36273199508850584\n",
      "Iteration 360, Training Loss: 0.3626479888594226\n",
      "Iteration 361, Training Loss: 0.362572914468753\n",
      "Iteration 362, Training Loss: 0.36248837994957067\n",
      "Iteration 363, Training Loss: 0.3624161200596526\n",
      "Iteration 364, Training Loss: 0.36233554709553106\n",
      "Iteration 365, Training Loss: 0.3622622338356554\n",
      "Iteration 366, Training Loss: 0.36218406127839264\n",
      "Iteration 367, Training Loss: 0.3621132959033113\n",
      "Iteration 368, Training Loss: 0.362035282219637\n",
      "Iteration 369, Training Loss: 0.3619670142283627\n",
      "Iteration 370, Training Loss: 0.3618913130314188\n",
      "Iteration 371, Training Loss: 0.36182295075495857\n",
      "Iteration 372, Training Loss: 0.3617497853772463\n",
      "Iteration 373, Training Loss: 0.36168318583785103\n",
      "Iteration 374, Training Loss: 0.3616118999724993\n",
      "Iteration 375, Training Loss: 0.3615458151651126\n",
      "Iteration 376, Training Loss: 0.361476020091017\n",
      "Iteration 377, Training Loss: 0.3614124349854509\n",
      "Iteration 378, Training Loss: 0.36134416117054446\n",
      "Iteration 379, Training Loss: 0.36128113257764605\n",
      "Iteration 380, Training Loss: 0.3612137635639014\n",
      "Iteration 381, Training Loss: 0.36115442050592594\n",
      "Iteration 382, Training Loss: 0.3610861377777807\n",
      "Iteration 383, Training Loss: 0.3610288840320925\n",
      "Iteration 384, Training Loss: 0.3609632395831541\n",
      "Iteration 385, Training Loss: 0.36090591750286294\n",
      "Iteration 386, Training Loss: 0.36084226793631186\n",
      "Iteration 387, Training Loss: 0.3607845137222447\n",
      "Iteration 388, Training Loss: 0.3607238777967507\n",
      "Iteration 389, Training Loss: 0.36066726896109125\n",
      "Iteration 390, Training Loss: 0.36060677215368897\n",
      "Iteration 391, Training Loss: 0.3605527037666406\n",
      "Iteration 392, Training Loss: 0.36049302717648707\n",
      "Iteration 393, Training Loss: 0.360440303567499\n",
      "Iteration 394, Training Loss: 0.36038220432879914\n",
      "Iteration 395, Training Loss: 0.36032994381970956\n",
      "Iteration 396, Training Loss: 0.3602717724781207\n",
      "Iteration 397, Training Loss: 0.36022216747599145\n",
      "Iteration 398, Training Loss: 0.3601653534822661\n",
      "Iteration 399, Training Loss: 0.3601165419087571\n",
      "Iteration 400, Training Loss: 0.36006170809974336\n",
      "Iteration 401, Training Loss: 0.36001289759087624\n",
      "Iteration 402, Training Loss: 0.3599600632131831\n",
      "Iteration 403, Training Loss: 0.35991235382528003\n",
      "Iteration 404, Training Loss: 0.3598608085381609\n",
      "Iteration 405, Training Loss: 0.3598143845995665\n",
      "Iteration 406, Training Loss: 0.35976353967446667\n",
      "Iteration 407, Training Loss: 0.35971735546285716\n",
      "Iteration 408, Training Loss: 0.35966787960984903\n",
      "Iteration 409, Training Loss: 0.35962311190658924\n",
      "Iteration 410, Training Loss: 0.35957354435943006\n",
      "Iteration 411, Training Loss: 0.3595315766851539\n",
      "Iteration 412, Training Loss: 0.35948196667227217\n",
      "Iteration 413, Training Loss: 0.3594405241565597\n",
      "Iteration 414, Training Loss: 0.3593920072921878\n",
      "Iteration 415, Training Loss: 0.3593519364855736\n",
      "Iteration 416, Training Loss: 0.35930424447340537\n",
      "Iteration 417, Training Loss: 0.35926576820564476\n",
      "Iteration 418, Training Loss: 0.3592187179659157\n",
      "Iteration 419, Training Loss: 0.3591800205583344\n",
      "Iteration 420, Training Loss: 0.3591356386214976\n",
      "Iteration 421, Training Loss: 0.35909713382384356\n",
      "Iteration 422, Training Loss: 0.3590537262276475\n",
      "Iteration 423, Training Loss: 0.35901526317648824\n",
      "Iteration 424, Training Loss: 0.35897301986112956\n",
      "Iteration 425, Training Loss: 0.35893654378350226\n",
      "Iteration 426, Training Loss: 0.35889295025475776\n",
      "Iteration 427, Training Loss: 0.35885865005119144\n",
      "Iteration 428, Training Loss: 0.3588162074231296\n",
      "Iteration 429, Training Loss: 0.3587826831892764\n",
      "Iteration 430, Training Loss: 0.35873938123407534\n",
      "Iteration 431, Training Loss: 0.35870746710663015\n",
      "Iteration 432, Training Loss: 0.35866693479197537\n",
      "Iteration 433, Training Loss: 0.3586335667390002\n",
      "Iteration 434, Training Loss: 0.35859472655792246\n",
      "Iteration 435, Training Loss: 0.35856232398335275\n",
      "Iteration 436, Training Loss: 0.35852367161151366\n",
      "Iteration 437, Training Loss: 0.3584932970586679\n",
      "Iteration 438, Training Loss: 0.35845298532686626\n",
      "Iteration 439, Training Loss: 0.358426108038356\n",
      "Iteration 440, Training Loss: 0.35838556409590977\n",
      "Iteration 441, Training Loss: 0.35835838800359365\n",
      "Iteration 442, Training Loss: 0.3583219607452942\n",
      "Iteration 443, Training Loss: 0.35829064447622805\n",
      "Iteration 444, Training Loss: 0.3582576136072039\n",
      "Iteration 445, Training Loss: 0.3582257139036555\n",
      "Iteration 446, Training Loss: 0.3581948493025925\n",
      "Iteration 447, Training Loss: 0.35816248592630706\n",
      "Iteration 448, Training Loss: 0.35813433294931313\n",
      "Iteration 449, Training Loss: 0.3580997735594433\n",
      "Iteration 450, Training Loss: 0.3580760984975064\n",
      "Iteration 451, Training Loss: 0.3580399711140419\n",
      "Iteration 452, Training Loss: 0.3580165807907729\n",
      "Iteration 453, Training Loss: 0.35798195594267634\n",
      "Iteration 454, Training Loss: 0.3579582859936423\n",
      "Iteration 455, Training Loss: 0.3579252248620393\n",
      "Iteration 456, Training Loss: 0.3579018533064907\n",
      "Iteration 457, Training Loss: 0.35786949190541995\n",
      "Iteration 458, Training Loss: 0.3578450513093114\n",
      "Iteration 459, Training Loss: 0.3578170174553357\n",
      "Iteration 460, Training Loss: 0.3577902936269569\n",
      "Iteration 461, Training Loss: 0.35776339791816975\n",
      "Iteration 462, Training Loss: 0.35773794873906967\n",
      "Iteration 463, Training Loss: 0.3577106511446223\n",
      "Iteration 464, Training Loss: 0.35768740008352656\n",
      "Iteration 465, Training Loss: 0.3576587766525634\n",
      "Iteration 466, Training Loss: 0.35763774624447714\n",
      "Iteration 467, Training Loss: 0.357607884174113\n",
      "Iteration 468, Training Loss: 0.3575895750151536\n",
      "Iteration 469, Training Loss: 0.3575585555297126\n",
      "Iteration 470, Training Loss: 0.3575400378309184\n",
      "Iteration 471, Training Loss: 0.35751027299153\n",
      "Iteration 472, Training Loss: 0.35749120216690294\n",
      "Iteration 473, Training Loss: 0.3574646002861043\n",
      "Iteration 474, Training Loss: 0.35744451444928493\n",
      "Iteration 475, Training Loss: 0.3574182840866213\n",
      "Iteration 476, Training Loss: 0.35739985528653073\n",
      "Iteration 477, Training Loss: 0.35737251428511185\n",
      "Iteration 478, Training Loss: 0.35735468628938927\n",
      "Iteration 479, Training Loss: 0.35732802364445626\n",
      "Iteration 480, Training Loss: 0.3573099848275034\n",
      "Iteration 481, Training Loss: 0.357285561824206\n",
      "Iteration 482, Training Loss: 0.35726834797401025\n",
      "Iteration 483, Training Loss: 0.3572417580420884\n",
      "Iteration 484, Training Loss: 0.3572280656104249\n",
      "Iteration 485, Training Loss: 0.35720008665328523\n",
      "Iteration 486, Training Loss: 0.3571853858643885\n",
      "Iteration 487, Training Loss: 0.357160092081825\n",
      "Iteration 488, Training Loss: 0.3571434396387019\n",
      "Iteration 489, Training Loss: 0.35712069383877226\n",
      "Iteration 490, Training Loss: 0.35710474868565734\n",
      "Iteration 491, Training Loss: 0.3570817350493325\n",
      "Iteration 492, Training Loss: 0.35706575338114815\n",
      "Iteration 493, Training Loss: 0.35704253068711767\n",
      "Iteration 494, Training Loss: 0.3570296700951047\n",
      "Iteration 495, Training Loss: 0.35700493854376175\n",
      "Iteration 496, Training Loss: 0.356991847041877\n",
      "Iteration 497, Training Loss: 0.3569683144799113\n",
      "Iteration 498, Training Loss: 0.3569548601121533\n",
      "Iteration 499, Training Loss: 0.3569337489663627\n",
      "Iteration 500, Training Loss: 0.3569177244038464\n",
      "Iteration 501, Training Loss: 0.3568986316803541\n",
      "Iteration 502, Training Loss: 0.3568845256877492\n",
      "Iteration 503, Training Loss: 0.35686271350081084\n",
      "Iteration 504, Training Loss: 0.3568526893973804\n",
      "Iteration 505, Training Loss: 0.3568280609863284\n",
      "Iteration 506, Training Loss: 0.35681708695688963\n",
      "Iteration 507, Training Loss: 0.35679697042372427\n",
      "Iteration 508, Training Loss: 0.3567835756846474\n",
      "Iteration 509, Training Loss: 0.35676481201513116\n",
      "Iteration 510, Training Loss: 0.35675177985300527\n",
      "Iteration 511, Training Loss: 0.35673179891195\n",
      "Iteration 512, Training Loss: 0.35672138230793093\n",
      "Iteration 513, Training Loss: 0.3566994050716231\n",
      "Iteration 514, Training Loss: 0.3566903008170032\n",
      "Iteration 515, Training Loss: 0.35666976346317564\n",
      "Iteration 516, Training Loss: 0.3566601065875864\n",
      "Iteration 517, Training Loss: 0.3566406577345831\n",
      "Iteration 518, Training Loss: 0.35662925102449555\n",
      "Iteration 519, Training Loss: 0.3566121568661889\n",
      "Iteration 520, Training Loss: 0.35660073595196984\n",
      "Iteration 521, Training Loss: 0.3565830427315568\n",
      "Iteration 522, Training Loss: 0.35657421196637357\n",
      "Iteration 523, Training Loss: 0.3565544700581251\n",
      "Iteration 524, Training Loss: 0.3565467408709103\n",
      "Iteration 525, Training Loss: 0.35652724375485906\n",
      "Iteration 526, Training Loss: 0.3565172191782523\n",
      "Iteration 527, Training Loss: 0.3564999869148665\n",
      "Iteration 528, Training Loss: 0.35649204078961205\n",
      "Iteration 529, Training Loss: 0.356472312814278\n",
      "Iteration 530, Training Loss: 0.35646629242072775\n",
      "Iteration 531, Training Loss: 0.3564461069859975\n",
      "Iteration 532, Training Loss: 0.3564395667627763\n",
      "Iteration 533, Training Loss: 0.3564231053099455\n",
      "Iteration 534, Training Loss: 0.3564137236603629\n",
      "Iteration 535, Training Loss: 0.3563982962687413\n",
      "Iteration 536, Training Loss: 0.35638940991600326\n",
      "Iteration 537, Training Loss: 0.356372663403534\n",
      "Iteration 538, Training Loss: 0.35636711942025495\n",
      "Iteration 539, Training Loss: 0.35634829305501714\n",
      "Iteration 540, Training Loss: 0.3563436505924076\n",
      "Iteration 541, Training Loss: 0.3563251638161267\n",
      "Iteration 542, Training Loss: 0.3563186494348211\n",
      "Iteration 543, Training Loss: 0.3563031488698854\n",
      "Iteration 544, Training Loss: 0.35629434526556564\n",
      "Iteration 545, Training Loss: 0.35628061693349916\n",
      "Iteration 546, Training Loss: 0.3562752211514989\n",
      "Iteration 547, Training Loss: 0.35625793894025565\n",
      "Iteration 548, Training Loss: 0.3562531797409306\n",
      "Iteration 549, Training Loss: 0.35623752240430584\n",
      "Iteration 550, Training Loss: 0.3562310098170829\n",
      "Iteration 551, Training Loss: 0.3562165076122819\n",
      "Iteration 552, Training Loss: 0.3562092257952413\n",
      "Iteration 553, Training Loss: 0.3561954075835023\n",
      "Iteration 554, Training Loss: 0.3561902638182918\n",
      "Iteration 555, Training Loss: 0.35617470806731666\n",
      "Iteration 556, Training Loss: 0.35616961060827124\n",
      "Iteration 557, Training Loss: 0.3561550531659318\n",
      "Iteration 558, Training Loss: 0.3561500222177326\n",
      "Iteration 559, Training Loss: 0.3561356835400145\n",
      "Iteration 560, Training Loss: 0.3561305470178644\n",
      "Iteration 561, Training Loss: 0.3561156760868561\n",
      "Iteration 562, Training Loss: 0.3561112856725111\n",
      "Iteration 563, Training Loss: 0.3560980025101115\n",
      "Iteration 564, Training Loss: 0.3560922333377365\n",
      "Iteration 565, Training Loss: 0.35607974590502395\n",
      "Iteration 566, Training Loss: 0.35607431157851877\n",
      "Iteration 567, Training Loss: 0.35606021511521996\n",
      "Iteration 568, Training Loss: 0.3560593375259927\n",
      "Iteration 569, Training Loss: 0.35604153011487155\n",
      "Iteration 570, Training Loss: 0.3560401965142067\n",
      "Iteration 571, Training Loss: 0.35602567390367423\n",
      "Iteration 572, Training Loss: 0.35602269490885347\n",
      "Iteration 573, Training Loss: 0.35600947593426907\n",
      "Iteration 574, Training Loss: 0.35600537934291243\n",
      "Iteration 575, Training Loss: 0.35599374808876566\n",
      "Iteration 576, Training Loss: 0.35598845035739324\n",
      "Iteration 577, Training Loss: 0.3559768679022152\n",
      "Iteration 578, Training Loss: 0.35597388903354965\n",
      "Iteration 579, Training Loss: 0.35595932669172\n",
      "Iteration 580, Training Loss: 0.3559591855449338\n",
      "Iteration 581, Training Loss: 0.35594434850206946\n",
      "Iteration 582, Training Loss: 0.3559420863034252\n",
      "Iteration 583, Training Loss: 0.35592998479007804\n",
      "Iteration 584, Training Loss: 0.3559270246820143\n",
      "Iteration 585, Training Loss: 0.3559154124519706\n",
      "Iteration 586, Training Loss: 0.35591267372345414\n",
      "Iteration 587, Training Loss: 0.3559002927932601\n",
      "Iteration 588, Training Loss: 0.35589920988584434\n",
      "Iteration 589, Training Loss: 0.3558841972112488\n",
      "Iteration 590, Training Loss: 0.3558845302220835\n",
      "Iteration 591, Training Loss: 0.3558725775486026\n",
      "Iteration 592, Training Loss: 0.3558700847913832\n",
      "Iteration 593, Training Loss: 0.3558588204624964\n",
      "Iteration 594, Training Loss: 0.35585573823033584\n",
      "Iteration 595, Training Loss: 0.35584675638543595\n",
      "Iteration 596, Training Loss: 0.35584251482830104\n",
      "Iteration 597, Training Loss: 0.3558326941762903\n",
      "Iteration 598, Training Loss: 0.35583266171017075\n",
      "Iteration 599, Training Loss: 0.3558183770657192\n",
      "Iteration 600, Training Loss: 0.3558205462484283\n",
      "Iteration 601, Training Loss: 0.35580655380180504\n",
      "Iteration 602, Training Loss: 0.35580488795518583\n",
      "Iteration 603, Training Loss: 0.35579593046828517\n",
      "Iteration 604, Training Loss: 0.35579293371699644\n",
      "Iteration 605, Training Loss: 0.3557848735559227\n",
      "Iteration 606, Training Loss: 0.35578084988440967\n",
      "Iteration 607, Training Loss: 0.3557717031879608\n",
      "Iteration 608, Training Loss: 0.3557716846190952\n",
      "Iteration 609, Training Loss: 0.3557594230591105\n",
      "Iteration 610, Training Loss: 0.3557617618687292\n",
      "Iteration 611, Training Loss: 0.35574884416306013\n",
      "Iteration 612, Training Loss: 0.3557505478501523\n",
      "Iteration 613, Training Loss: 0.3557387053251198\n",
      "Iteration 614, Training Loss: 0.3557394520289662\n",
      "Iteration 615, Training Loss: 0.3557295959834316\n",
      "Iteration 616, Training Loss: 0.35572830150918905\n",
      "Iteration 617, Training Loss: 0.3557200091287544\n",
      "Iteration 618, Training Loss: 0.3557198026623181\n",
      "Iteration 619, Training Loss: 0.355707200894302\n",
      "Iteration 620, Training Loss: 0.3557112031890679\n",
      "Iteration 621, Training Loss: 0.35569703348416476\n",
      "Iteration 622, Training Loss: 0.3556992019009461\n",
      "Iteration 623, Training Loss: 0.35568937709372367\n",
      "Iteration 624, Training Loss: 0.3556885002400248\n",
      "Iteration 625, Training Loss: 0.3556804317256555\n",
      "Iteration 626, Training Loss: 0.35567963309501344\n",
      "Iteration 627, Training Loss: 0.3556709306417059\n",
      "Iteration 628, Training Loss: 0.35567213016153554\n",
      "Iteration 629, Training Loss: 0.35566126190581815\n",
      "Iteration 630, Training Loss: 0.3556634350591694\n",
      "Iteration 631, Training Loss: 0.3556517971610693\n",
      "Iteration 632, Training Loss: 0.3556557203290946\n",
      "Iteration 633, Training Loss: 0.35564274358805226\n",
      "Iteration 634, Training Loss: 0.35564549237900595\n",
      "Iteration 635, Training Loss: 0.3556355858489849\n",
      "Iteration 636, Training Loss: 0.35563699590924075\n",
      "Iteration 637, Training Loss: 0.35562791480599626\n",
      "Iteration 638, Training Loss: 0.35562872488756936\n",
      "Iteration 639, Training Loss: 0.3556197363609331\n",
      "Iteration 640, Training Loss: 0.3556202793614541\n",
      "Iteration 641, Training Loss: 0.35561227681417834\n",
      "Iteration 642, Training Loss: 0.35561376885656654\n",
      "Iteration 643, Training Loss: 0.35560378759895\n",
      "Iteration 644, Training Loss: 0.35560681144682915\n",
      "Iteration 645, Training Loss: 0.3555953517476611\n",
      "Iteration 646, Training Loss: 0.355600467486606\n",
      "Iteration 647, Training Loss: 0.35558751581551556\n",
      "Iteration 648, Training Loss: 0.35558942133792354\n",
      "Iteration 649, Training Loss: 0.3555812566572542\n",
      "Iteration 650, Training Loss: 0.355583732452505\n",
      "Iteration 651, Training Loss: 0.3555744273798821\n",
      "Iteration 652, Training Loss: 0.35557768506408277\n",
      "Iteration 653, Training Loss: 0.3555673840835024\n",
      "Iteration 654, Training Loss: 0.3555703511933438\n",
      "Iteration 655, Training Loss: 0.35556118902865647\n",
      "Iteration 656, Training Loss: 0.35556233562396933\n",
      "Iteration 657, Training Loss: 0.35555494206077376\n",
      "Iteration 658, Training Loss: 0.3555557547535878\n",
      "Iteration 659, Training Loss: 0.3555483450707227\n",
      "Iteration 660, Training Loss: 0.3555499091476168\n",
      "Iteration 661, Training Loss: 0.3555409200225618\n",
      "Iteration 662, Training Loss: 0.3555454566214746\n",
      "Iteration 663, Training Loss: 0.3555334635003938\n",
      "Iteration 664, Training Loss: 0.3555384853207433\n",
      "Iteration 665, Training Loss: 0.35552794220532064\n",
      "Iteration 666, Training Loss: 0.3555310281830289\n",
      "Iteration 667, Training Loss: 0.35552278830973233\n",
      "Iteration 668, Training Loss: 0.3555245684902031\n",
      "Iteration 669, Training Loss: 0.3555176315214401\n",
      "Iteration 670, Training Loss: 0.35551946234606735\n",
      "Iteration 671, Training Loss: 0.35551162778207973\n",
      "Iteration 672, Training Loss: 0.3555150927713578\n",
      "Iteration 673, Training Loss: 0.3555057981465246\n",
      "Iteration 674, Training Loss: 0.3555088855483047\n",
      "Iteration 675, Training Loss: 0.3554997013025461\n",
      "Iteration 676, Training Loss: 0.3555033714780279\n",
      "Iteration 677, Training Loss: 0.3554935518582678\n",
      "Iteration 678, Training Loss: 0.3554975398842311\n",
      "Iteration 679, Training Loss: 0.3554897514727138\n",
      "Iteration 680, Training Loss: 0.35549083222722244\n",
      "Iteration 681, Training Loss: 0.3554861551824016\n",
      "Iteration 682, Training Loss: 0.35548584846924497\n",
      "Iteration 683, Training Loss: 0.3554808271109802\n",
      "Iteration 684, Training Loss: 0.3554827997480868\n",
      "Iteration 685, Training Loss: 0.3554746009520852\n",
      "Iteration 686, Training Loss: 0.35547802879601686\n",
      "Iteration 687, Training Loss: 0.3554696018985699\n",
      "Iteration 688, Training Loss: 0.35547374113779473\n",
      "Iteration 689, Training Loss: 0.3554639517344418\n",
      "Iteration 690, Training Loss: 0.3554687544936785\n",
      "Iteration 691, Training Loss: 0.35545881694739606\n",
      "Iteration 692, Training Loss: 0.3554631576483258\n",
      "Iteration 693, Training Loss: 0.35545645668528203\n",
      "Iteration 694, Training Loss: 0.3554564652185887\n",
      "Iteration 695, Training Loss: 0.3554524964984845\n",
      "Iteration 696, Training Loss: 0.35545386130169704\n",
      "Iteration 697, Training Loss: 0.3554481028576755\n",
      "Iteration 698, Training Loss: 0.35545090369723925\n",
      "Iteration 699, Training Loss: 0.35544191510093787\n",
      "Iteration 700, Training Loss: 0.3554465398373289\n",
      "Iteration 701, Training Loss: 0.35543766776716285\n",
      "Iteration 702, Training Loss: 0.35544351066527713\n",
      "Iteration 703, Training Loss: 0.355433041330018\n",
      "Iteration 704, Training Loss: 0.355436350865864\n",
      "Iteration 705, Training Loss: 0.35543235352606806\n",
      "Iteration 706, Training Loss: 0.3554322159415219\n",
      "Iteration 707, Training Loss: 0.3554272596762171\n",
      "Iteration 708, Training Loss: 0.35543096973114296\n",
      "Iteration 709, Training Loss: 0.3554217655091828\n",
      "Iteration 710, Training Loss: 0.3554281380592114\n",
      "Iteration 711, Training Loss: 0.3554180773048234\n",
      "Iteration 712, Training Loss: 0.355422885982308\n",
      "Iteration 713, Training Loss: 0.35541455319875126\n",
      "Iteration 714, Training Loss: 0.3554175525383764\n",
      "Iteration 715, Training Loss: 0.35541235940517063\n",
      "Iteration 716, Training Loss: 0.3554129491806351\n",
      "Iteration 717, Training Loss: 0.3554091448065135\n",
      "Iteration 718, Training Loss: 0.35541234463325805\n",
      "Iteration 719, Training Loss: 0.3554043712923706\n",
      "Iteration 720, Training Loss: 0.3554096600694857\n",
      "Iteration 721, Training Loss: 0.3554002445136525\n",
      "Iteration 722, Training Loss: 0.3554054024867411\n",
      "Iteration 723, Training Loss: 0.35539699476741426\n",
      "Iteration 724, Training Loss: 0.35540078079683146\n",
      "Iteration 725, Training Loss: 0.3553952036577368\n",
      "Iteration 726, Training Loss: 0.35539632308579233\n",
      "Iteration 727, Training Loss: 0.355391976067969\n",
      "Iteration 728, Training Loss: 0.35539570954695043\n",
      "Iteration 729, Training Loss: 0.35538836071230745\n",
      "Iteration 730, Training Loss: 0.35539300405216956\n",
      "Iteration 731, Training Loss: 0.35538433491941235\n",
      "Iteration 732, Training Loss: 0.3553900665655492\n",
      "Iteration 733, Training Loss: 0.35538241217476885\n",
      "Iteration 734, Training Loss: 0.3553850975961547\n",
      "Iteration 735, Training Loss: 0.3553808552431381\n",
      "Iteration 736, Training Loss: 0.3553828857946137\n",
      "Iteration 737, Training Loss: 0.3553771013016985\n",
      "Iteration 738, Training Loss: 0.35538160495913734\n",
      "Iteration 739, Training Loss: 0.3553742850778532\n",
      "Iteration 740, Training Loss: 0.35537918302436955\n",
      "Iteration 741, Training Loss: 0.3553704683852306\n",
      "Iteration 742, Training Loss: 0.35537585810957645\n",
      "Iteration 743, Training Loss: 0.3553680061557938\n",
      "Iteration 744, Training Loss: 0.35537071547873056\n",
      "Iteration 745, Training Loss: 0.35536769675226226\n",
      "Iteration 746, Training Loss: 0.3553678904456539\n",
      "Iteration 747, Training Loss: 0.355366051739664\n",
      "Iteration 748, Training Loss: 0.35536580648702687\n",
      "Iteration 749, Training Loss: 0.3553636479525808\n",
      "Iteration 750, Training Loss: 0.35536460653572327\n",
      "Iteration 751, Training Loss: 0.35535863308953075\n",
      "Iteration 752, Training Loss: 0.35536447537356936\n",
      "Iteration 753, Training Loss: 0.3553560355351992\n",
      "Iteration 754, Training Loss: 0.3553616266353884\n",
      "Iteration 755, Training Loss: 0.3553521255907927\n",
      "Iteration 756, Training Loss: 0.3553586627508135\n",
      "Iteration 757, Training Loss: 0.35534956433807086\n",
      "Iteration 758, Training Loss: 0.3553543248608175\n",
      "Iteration 759, Training Loss: 0.35534925042602283\n",
      "Iteration 760, Training Loss: 0.3553522816993117\n",
      "Iteration 761, Training Loss: 0.35534668210037773\n",
      "Iteration 762, Training Loss: 0.35535139241699754\n",
      "Iteration 763, Training Loss: 0.35534362275235004\n",
      "Iteration 764, Training Loss: 0.3553488378279201\n",
      "Iteration 765, Training Loss: 0.35534146867194827\n",
      "Iteration 766, Training Loss: 0.3553458068969597\n",
      "Iteration 767, Training Loss: 0.3553409309481992\n",
      "Iteration 768, Training Loss: 0.35534300255200096\n",
      "Iteration 769, Training Loss: 0.35533955306630743\n",
      "Iteration 770, Training Loss: 0.3553415913647968\n",
      "Iteration 771, Training Loss: 0.3553364843720801\n",
      "Iteration 772, Training Loss: 0.3553414094109085\n",
      "Iteration 773, Training Loss: 0.3553331113388722\n",
      "Iteration 774, Training Loss: 0.3553397152259\n",
      "Iteration 775, Training Loss: 0.3553306439675571\n",
      "Iteration 776, Training Loss: 0.3553359038331102\n",
      "Iteration 777, Training Loss: 0.35533100419374386\n",
      "Iteration 778, Training Loss: 0.35533403040075073\n",
      "Iteration 779, Training Loss: 0.3553298861371919\n",
      "Iteration 780, Training Loss: 0.35533270382153403\n",
      "Iteration 781, Training Loss: 0.3553276974692635\n",
      "Iteration 782, Training Loss: 0.3553309346617703\n",
      "Iteration 783, Training Loss: 0.35532639060427007\n",
      "Iteration 784, Training Loss: 0.3553299996640753\n",
      "Iteration 785, Training Loss: 0.3553234906571167\n",
      "Iteration 786, Training Loss: 0.3553280675359753\n",
      "Iteration 787, Training Loss: 0.355320893029333\n",
      "Iteration 788, Training Loss: 0.35532697408454694\n",
      "Iteration 789, Training Loss: 0.35531779613759346\n",
      "Iteration 790, Training Loss: 0.3553251838991194\n",
      "Iteration 791, Training Loss: 0.3553169411894052\n",
      "Iteration 792, Training Loss: 0.35532074420155646\n",
      "Iteration 793, Training Loss: 0.3553175247103165\n",
      "Iteration 794, Training Loss: 0.3553193025327629\n",
      "Iteration 795, Training Loss: 0.3553157984768114\n",
      "Iteration 796, Training Loss: 0.35531879456243426\n",
      "Iteration 797, Training Loss: 0.3553125365140398\n",
      "Iteration 798, Training Loss: 0.3553191936885035\n",
      "Iteration 799, Training Loss: 0.3553106218674549\n",
      "Iteration 800, Training Loss: 0.35531755019652506\n",
      "Iteration 801, Training Loss: 0.355310236187505\n",
      "Iteration 802, Training Loss: 0.3553137615021785\n",
      "Iteration 803, Training Loss: 0.355309351348694\n",
      "Iteration 804, Training Loss: 0.3553124955015119\n",
      "Iteration 805, Training Loss: 0.3553073173549919\n",
      "Iteration 806, Training Loss: 0.3553131423585403\n",
      "Iteration 807, Training Loss: 0.35530424200240546\n",
      "Iteration 808, Training Loss: 0.35531291514768937\n",
      "Iteration 809, Training Loss: 0.35530447438955504\n",
      "Iteration 810, Training Loss: 0.355307761551285\n",
      "Iteration 811, Training Loss: 0.355303670322081\n",
      "Iteration 812, Training Loss: 0.3553073666403879\n",
      "Iteration 813, Training Loss: 0.3553018936733715\n",
      "Iteration 814, Training Loss: 0.3553074988166104\n",
      "Iteration 815, Training Loss: 0.3553011960914528\n",
      "Iteration 816, Training Loss: 0.3553053485531567\n",
      "Iteration 817, Training Loss: 0.3553001807678499\n",
      "Iteration 818, Training Loss: 0.35530405892070926\n",
      "Iteration 819, Training Loss: 0.3552988919201466\n",
      "Iteration 820, Training Loss: 0.35530334661232593\n",
      "Iteration 821, Training Loss: 0.35529747372381504\n",
      "Iteration 822, Training Loss: 0.35530262116270833\n",
      "Iteration 823, Training Loss: 0.3552961304576348\n",
      "Iteration 824, Training Loss: 0.3553015928797197\n",
      "Iteration 825, Training Loss: 0.3552945897988843\n",
      "Iteration 826, Training Loss: 0.3553002540352178\n",
      "Iteration 827, Training Loss: 0.35529457077348003\n",
      "Iteration 828, Training Loss: 0.3552975911778001\n",
      "Iteration 829, Training Loss: 0.35529344490376874\n",
      "Iteration 830, Training Loss: 0.35529588111647803\n",
      "Iteration 831, Training Loss: 0.35529254123786813\n",
      "Iteration 832, Training Loss: 0.3552962277725983\n",
      "Iteration 833, Training Loss: 0.3552900173409406\n",
      "Iteration 834, Training Loss: 0.35529618009160635\n",
      "Iteration 835, Training Loss: 0.3552877790283849\n",
      "Iteration 836, Training Loss: 0.3552942124502241\n",
      "Iteration 837, Training Loss: 0.3552883121844569\n",
      "Iteration 838, Training Loss: 0.35529332118620316\n",
      "Iteration 839, Training Loss: 0.35528753011795755\n",
      "Iteration 840, Training Loss: 0.3552914433656543\n",
      "Iteration 841, Training Loss: 0.35528632201666144\n",
      "Iteration 842, Training Loss: 0.35529090764361104\n",
      "Iteration 843, Training Loss: 0.35528556831672264\n",
      "Iteration 844, Training Loss: 0.3552905878612327\n",
      "Iteration 845, Training Loss: 0.3552853435860516\n",
      "Iteration 846, Training Loss: 0.355287727068374\n",
      "Iteration 847, Training Loss: 0.3552854697192191\n",
      "Iteration 848, Training Loss: 0.3552872161429203\n",
      "Iteration 849, Training Loss: 0.3552840218709801\n",
      "Iteration 850, Training Loss: 0.35528833292207923\n",
      "Iteration 851, Training Loss: 0.3552815862205937\n",
      "Iteration 852, Training Loss: 0.35528845143896826\n",
      "Iteration 853, Training Loss: 0.35528028408317\n",
      "Iteration 854, Training Loss: 0.35528706934174886\n",
      "Iteration 855, Training Loss: 0.35527880787675187\n",
      "Iteration 856, Training Loss: 0.35528438521746697\n",
      "Iteration 857, Training Loss: 0.35527945163311786\n",
      "Iteration 858, Training Loss: 0.35528366654239185\n",
      "Iteration 859, Training Loss: 0.3552790118249946\n",
      "Iteration 860, Training Loss: 0.3552831073856134\n",
      "Iteration 861, Training Loss: 0.35527882505546843\n",
      "Iteration 862, Training Loss: 0.35528115564061014\n",
      "Iteration 863, Training Loss: 0.3552776241759074\n",
      "Iteration 864, Training Loss: 0.3552828835323629\n",
      "Iteration 865, Training Loss: 0.35527504008563554\n",
      "Iteration 866, Training Loss: 0.35528123207121914\n",
      "Iteration 867, Training Loss: 0.3552740536003328\n",
      "Iteration 868, Training Loss: 0.35528117337110293\n",
      "Iteration 869, Training Loss: 0.35527335465099147\n",
      "Iteration 870, Training Loss: 0.3552786774226382\n",
      "Iteration 871, Training Loss: 0.3552743510424754\n",
      "Iteration 872, Training Loss: 0.35527619402687755\n",
      "Iteration 873, Training Loss: 0.35527404790659656\n",
      "Iteration 874, Training Loss: 0.3552767578706601\n",
      "Iteration 875, Training Loss: 0.3552728028480731\n",
      "Iteration 876, Training Loss: 0.3552792591781368\n",
      "Iteration 877, Training Loss: 0.3552697696871883\n",
      "Iteration 878, Training Loss: 0.35527893808205396\n",
      "Iteration 879, Training Loss: 0.35527076072813185\n",
      "Iteration 880, Training Loss: 0.35527368247273794\n",
      "Iteration 881, Training Loss: 0.355271803207876\n",
      "Iteration 882, Training Loss: 0.35527328979547224\n",
      "Iteration 883, Training Loss: 0.35527175039467745\n",
      "Iteration 884, Training Loss: 0.35527320940706386\n",
      "Iteration 885, Training Loss: 0.35527085324899477\n",
      "Iteration 886, Training Loss: 0.3552729876510722\n",
      "Iteration 887, Training Loss: 0.35526917912597966\n",
      "Iteration 888, Training Loss: 0.3552738426988121\n",
      "Iteration 889, Training Loss: 0.35526712543517164\n",
      "Iteration 890, Training Loss: 0.3552751642597272\n",
      "Iteration 891, Training Loss: 0.3552661191024878\n",
      "Iteration 892, Training Loss: 0.35527259407781986\n",
      "Iteration 893, Training Loss: 0.3552652540985106\n",
      "Iteration 894, Training Loss: 0.35527072979600977\n",
      "Iteration 895, Training Loss: 0.35526725826215955\n",
      "Iteration 896, Training Loss: 0.35526867761867625\n",
      "Iteration 897, Training Loss: 0.3552664772796015\n",
      "Iteration 898, Training Loss: 0.3552706570004034\n",
      "Iteration 899, Training Loss: 0.3552646016597362\n",
      "Iteration 900, Training Loss: 0.355270981304871\n",
      "Iteration 901, Training Loss: 0.3552628836112878\n",
      "Iteration 902, Training Loss: 0.35527062673134124\n",
      "Iteration 903, Training Loss: 0.3552628625182354\n",
      "Iteration 904, Training Loss: 0.35526802308556815\n",
      "Iteration 905, Training Loss: 0.355264148613783\n",
      "Iteration 906, Training Loss: 0.35526698048443484\n",
      "Iteration 907, Training Loss: 0.35526289051443694\n",
      "Iteration 908, Training Loss: 0.355268299099905\n",
      "Iteration 909, Training Loss: 0.35526164052173054\n",
      "Iteration 910, Training Loss: 0.3552677628473088\n",
      "Iteration 911, Training Loss: 0.3552612171806023\n",
      "Iteration 912, Training Loss: 0.3552672779626093\n",
      "Iteration 913, Training Loss: 0.35526289463774824\n",
      "Iteration 914, Training Loss: 0.35526461185254704\n",
      "Iteration 915, Training Loss: 0.3552617061212057\n",
      "Iteration 916, Training Loss: 0.355267140293031\n",
      "Iteration 917, Training Loss: 0.355258927554946\n",
      "Iteration 918, Training Loss: 0.3552672063467751\n",
      "Iteration 919, Training Loss: 0.3552589720248856\n",
      "Iteration 920, Training Loss: 0.3552643828832693\n",
      "Iteration 921, Training Loss: 0.3552601647068729\n",
      "Iteration 922, Training Loss: 0.3552629130860135\n",
      "Iteration 923, Training Loss: 0.35525998886468746\n",
      "Iteration 924, Training Loss: 0.35526516542757897\n",
      "Iteration 925, Training Loss: 0.3552588037269447\n",
      "Iteration 926, Training Loss: 0.3552648555600021\n",
      "Iteration 927, Training Loss: 0.3552576932060359\n",
      "Iteration 928, Training Loss: 0.3552641466324603\n",
      "Iteration 929, Training Loss: 0.355258135590389\n",
      "Iteration 930, Training Loss: 0.35526204999095834\n",
      "Iteration 931, Training Loss: 0.35525860733781256\n",
      "Iteration 932, Training Loss: 0.3552611397072056\n",
      "Iteration 933, Training Loss: 0.355257706641406\n",
      "Iteration 934, Training Loss: 0.3552620163454608\n",
      "Iteration 935, Training Loss: 0.3552569819516779\n",
      "Iteration 936, Training Loss: 0.3552614390828495\n",
      "Iteration 937, Training Loss: 0.3552572316972142\n",
      "Iteration 938, Training Loss: 0.3552621827777157\n",
      "Iteration 939, Training Loss: 0.3552559344727406\n",
      "Iteration 940, Training Loss: 0.3552627623796861\n",
      "Iteration 941, Training Loss: 0.355254881114912\n",
      "Iteration 942, Training Loss: 0.35526099100172576\n",
      "Iteration 943, Training Loss: 0.355254524174537\n",
      "Iteration 944, Training Loss: 0.35525912991052644\n",
      "Iteration 945, Training Loss: 0.35525564139572596\n",
      "Iteration 946, Training Loss: 0.35525841373422373\n",
      "Iteration 947, Training Loss: 0.3552550279223999\n",
      "Iteration 948, Training Loss: 0.3552608696631983\n",
      "Iteration 949, Training Loss: 0.35525311801517395\n",
      "Iteration 950, Training Loss: 0.3552605510062832\n",
      "Iteration 951, Training Loss: 0.355252140366976\n",
      "Iteration 952, Training Loss: 0.3552594064646116\n",
      "Iteration 953, Training Loss: 0.355253605912433\n",
      "Iteration 954, Training Loss: 0.3552571245037619\n",
      "Iteration 955, Training Loss: 0.3552545377695447\n",
      "Iteration 956, Training Loss: 0.3552570998301703\n",
      "Iteration 957, Training Loss: 0.3552535915256791\n",
      "Iteration 958, Training Loss: 0.35525835240782916\n",
      "Iteration 959, Training Loss: 0.3552530237822264\n",
      "Iteration 960, Training Loss: 0.3552578803271976\n",
      "Iteration 961, Training Loss: 0.35525210717971006\n",
      "Early stopping at iteration 961\n",
      "Iteration 0, Training Loss: 1.0\n",
      "Iteration 1, Training Loss: 0.8876264674942123\n",
      "Iteration 2, Training Loss: 0.7871842230950977\n",
      "Iteration 3, Training Loss: 0.7059759546901003\n",
      "Iteration 4, Training Loss: 0.654930254283178\n",
      "Iteration 5, Training Loss: 0.6162301239871164\n",
      "Iteration 6, Training Loss: 0.5815859184239522\n",
      "Iteration 7, Training Loss: 0.5530970612228175\n",
      "Iteration 8, Training Loss: 0.5276099857505848\n",
      "Iteration 9, Training Loss: 0.5067474702878184\n",
      "Iteration 10, Training Loss: 0.48724958729507345\n",
      "Iteration 11, Training Loss: 0.4723360567910913\n",
      "Iteration 12, Training Loss: 0.45656097466914985\n",
      "Iteration 13, Training Loss: 0.4484150192355868\n",
      "Iteration 14, Training Loss: 0.43430151714802423\n",
      "Iteration 15, Training Loss: 0.4386447735708065\n",
      "Iteration 16, Training Loss: 0.4188938174795107\n",
      "Iteration 17, Training Loss: 0.4282247417549638\n",
      "Iteration 18, Training Loss: 0.4063096205045444\n",
      "Iteration 19, Training Loss: 0.41740615038640944\n",
      "Iteration 20, Training Loss: 0.3973402132019191\n",
      "Iteration 21, Training Loss: 0.41027142214516266\n",
      "Iteration 22, Training Loss: 0.39033664468339985\n",
      "Iteration 23, Training Loss: 0.40463330893041116\n",
      "Iteration 24, Training Loss: 0.38506303686313254\n",
      "Iteration 25, Training Loss: 0.40057369675358995\n",
      "Iteration 26, Training Loss: 0.38093563687494375\n",
      "Iteration 27, Training Loss: 0.39727928293442366\n",
      "Iteration 28, Training Loss: 0.37777612892621304\n",
      "Iteration 29, Training Loss: 0.3947766439858022\n",
      "Iteration 30, Training Loss: 0.37526529516547547\n",
      "Iteration 31, Training Loss: 0.3927579169127993\n",
      "Iteration 32, Training Loss: 0.3733614727553828\n",
      "Iteration 33, Training Loss: 0.39129615396077394\n",
      "Iteration 34, Training Loss: 0.37187052438953816\n",
      "Iteration 35, Training Loss: 0.3900965359731676\n",
      "Iteration 36, Training Loss: 0.37074392130365874\n",
      "Iteration 37, Training Loss: 0.38916545827135285\n",
      "Iteration 38, Training Loss: 0.3698449343455914\n",
      "Iteration 39, Training Loss: 0.38847252444608843\n",
      "Iteration 40, Training Loss: 0.36913300927101916\n",
      "Iteration 41, Training Loss: 0.38781205191769996\n",
      "Iteration 42, Training Loss: 0.36860154605662704\n",
      "Iteration 43, Training Loss: 0.38744793123462756\n",
      "Iteration 44, Training Loss: 0.36813999674999237\n",
      "Iteration 45, Training Loss: 0.38708026142474883\n",
      "Iteration 46, Training Loss: 0.36778231917114906\n",
      "Iteration 47, Training Loss: 0.38673760541250884\n",
      "Iteration 48, Training Loss: 0.36751818084152693\n",
      "Iteration 49, Training Loss: 0.3865891857615716\n",
      "Iteration 50, Training Loss: 0.36727152937912266\n",
      "Iteration 51, Training Loss: 0.3863532220709951\n",
      "Iteration 52, Training Loss: 0.3671060430048153\n",
      "Iteration 53, Training Loss: 0.38622097634458724\n",
      "Iteration 54, Training Loss: 0.3669801917111432\n",
      "Iteration 55, Training Loss: 0.38613641824752515\n",
      "Iteration 56, Training Loss: 0.36685365385723906\n",
      "Iteration 57, Training Loss: 0.38602868304275906\n",
      "Iteration 58, Training Loss: 0.36677658434842664\n",
      "Iteration 59, Training Loss: 0.38594634336856654\n",
      "Iteration 60, Training Loss: 0.3667062144950612\n",
      "Iteration 61, Training Loss: 0.3859386113245215\n",
      "Iteration 62, Training Loss: 0.36665568807684273\n",
      "Iteration 63, Training Loss: 0.3858471295306965\n",
      "Iteration 64, Training Loss: 0.36661237923750567\n",
      "Iteration 65, Training Loss: 0.3858493801249497\n",
      "Iteration 66, Training Loss: 0.36656952226749573\n",
      "Iteration 67, Training Loss: 0.3857426573412084\n",
      "Iteration 68, Training Loss: 0.3665468629402007\n",
      "Iteration 69, Training Loss: 0.3858270930456463\n",
      "Iteration 70, Training Loss: 0.3665174177220763\n",
      "Iteration 71, Training Loss: 0.3856908104634506\n",
      "Iteration 72, Training Loss: 0.36650277049139607\n",
      "Iteration 73, Training Loss: 0.38578248461998743\n",
      "Iteration 74, Training Loss: 0.366478721380755\n",
      "Iteration 75, Training Loss: 0.3856761452318184\n",
      "Iteration 76, Training Loss: 0.3664843638558714\n",
      "Iteration 77, Training Loss: 0.3857483292207809\n",
      "Iteration 78, Training Loss: 0.36646758443474087\n",
      "Iteration 79, Training Loss: 0.38568758864125724\n",
      "Iteration 80, Training Loss: 0.36645720415554256\n",
      "Iteration 81, Training Loss: 0.38573145570857353\n",
      "Iteration 82, Training Loss: 0.36645180247942566\n",
      "Iteration 83, Training Loss: 0.3856803584309519\n",
      "Iteration 84, Training Loss: 0.36644982761543754\n",
      "Iteration 85, Training Loss: 0.38570499895389176\n",
      "Iteration 86, Training Loss: 0.3664426995168172\n",
      "Iteration 87, Training Loss: 0.3856669509582023\n",
      "Iteration 88, Training Loss: 0.3664460891978543\n",
      "Iteration 89, Training Loss: 0.38573647432331454\n",
      "Iteration 90, Training Loss: 0.3664425556161913\n",
      "Iteration 91, Training Loss: 0.385660090918465\n",
      "Iteration 92, Training Loss: 0.3664427425449521\n",
      "Iteration 93, Training Loss: 0.38571403674135474\n",
      "Iteration 94, Training Loss: 0.36643622554471805\n",
      "Iteration 95, Training Loss: 0.3856644987843891\n",
      "Iteration 96, Training Loss: 0.36644344140831575\n",
      "Iteration 97, Training Loss: 0.3856993442354678\n",
      "Iteration 98, Training Loss: 0.3664386264854338\n",
      "Iteration 99, Training Loss: 0.38565212422463124\n",
      "Iteration 100, Training Loss: 0.366438437419454\n",
      "Iteration 101, Training Loss: 0.38569365736975153\n",
      "Iteration 102, Training Loss: 0.3664319661074427\n",
      "Iteration 103, Training Loss: 0.3856624012221454\n",
      "Iteration 104, Training Loss: 0.3664400345255136\n",
      "Iteration 105, Training Loss: 0.3857397337820671\n",
      "Iteration 106, Training Loss: 0.3664257269831989\n",
      "Iteration 107, Training Loss: 0.3856003904987459\n",
      "Iteration 108, Training Loss: 0.3664455037698186\n",
      "Iteration 109, Training Loss: 0.38570020893177237\n",
      "Iteration 110, Training Loss: 0.3664329838376508\n",
      "Iteration 111, Training Loss: 0.3856612053686059\n",
      "Iteration 112, Training Loss: 0.3664393251579367\n",
      "Iteration 113, Training Loss: 0.3857044575876983\n",
      "Iteration 114, Training Loss: 0.3664366377054528\n",
      "Iteration 115, Training Loss: 0.3856750737705406\n",
      "Iteration 116, Training Loss: 0.3664332603144409\n",
      "Early stopping at iteration 116\n",
      "Iteration 0, Training Loss: 1.0\n",
      "Iteration 1, Training Loss: 0.9887626467494214\n",
      "Iteration 2, Training Loss: 0.9777500405638542\n",
      "Iteration 3, Training Loss: 0.9669576865019981\n",
      "Iteration 4, Training Loss: 0.9563811795213796\n",
      "Iteration 5, Training Loss: 0.946016202680373\n",
      "Iteration 6, Training Loss: 0.9358585253761867\n",
      "Iteration 7, Training Loss: 0.9259040016180844\n",
      "Iteration 8, Training Loss: 0.9161485683351437\n",
      "Iteration 9, Training Loss: 0.9065882437178622\n",
      "Iteration 10, Training Loss: 0.8972191255929262\n",
      "Iteration 11, Training Loss: 0.8880373898304889\n",
      "Iteration 12, Training Loss: 0.8790392887833003\n",
      "Iteration 13, Training Loss: 0.8702217979394797\n",
      "Iteration 14, Training Loss: 0.8615870640537118\n",
      "Iteration 15, Training Loss: 0.853134733914386\n",
      "Iteration 16, Training Loss: 0.8448695969333841\n",
      "Iteration 17, Training Loss: 0.8367879273801727\n",
      "Iteration 18, Training Loss: 0.8288937600923895\n",
      "Iteration 19, Training Loss: 0.821189396716178\n",
      "Iteration 20, Training Loss: 0.813681412338202\n",
      "Iteration 21, Training Loss: 0.8063771102788482\n",
      "Iteration 22, Training Loss: 0.7992752703879771\n",
      "Iteration 23, Training Loss: 0.7923696681403637\n",
      "Iteration 24, Training Loss: 0.7856582792904464\n",
      "Iteration 25, Training Loss: 0.779129117164745\n",
      "Iteration 26, Training Loss: 0.7727880258469001\n",
      "Iteration 27, Training Loss: 0.7666345403473763\n",
      "Iteration 28, Training Loss: 0.7606812772393842\n",
      "Iteration 29, Training Loss: 0.7549147471905326\n",
      "Iteration 30, Training Loss: 0.749338278733847\n",
      "Iteration 31, Training Loss: 0.7439800178108034\n",
      "Iteration 32, Training Loss: 0.7388427256230061\n",
      "Iteration 33, Training Loss: 0.7339249493160415\n",
      "Iteration 34, Training Loss: 0.7292245498870578\n",
      "Iteration 35, Training Loss: 0.7247523983834708\n",
      "Iteration 36, Training Loss: 0.720489780980213\n",
      "Iteration 37, Training Loss: 0.7164130717776738\n",
      "Iteration 38, Training Loss: 0.7125189785592251\n",
      "Iteration 39, Training Loss: 0.7088015161966331\n",
      "Iteration 40, Training Loss: 0.7052346691967614\n",
      "Iteration 41, Training Loss: 0.7018131796860153\n",
      "Iteration 42, Training Loss: 0.6985235164311956\n",
      "Iteration 43, Training Loss: 0.6953572519684378\n",
      "Iteration 44, Training Loss: 0.6923063384968523\n",
      "Iteration 45, Training Loss: 0.6893589294985694\n",
      "Iteration 46, Training Loss: 0.6865126065450314\n",
      "Iteration 47, Training Loss: 0.6837605380382749\n",
      "Iteration 48, Training Loss: 0.6810912112125886\n",
      "Iteration 49, Training Loss: 0.6785060136231986\n",
      "Iteration 50, Training Loss: 0.6759930113428063\n",
      "Iteration 51, Training Loss: 0.6735492084253311\n",
      "Iteration 52, Training Loss: 0.6711702765973152\n",
      "Iteration 53, Training Loss: 0.6688571955973976\n",
      "Iteration 54, Training Loss: 0.6666094412546716\n",
      "Iteration 55, Training Loss: 0.6644208133692506\n",
      "Iteration 56, Training Loss: 0.6622874503188545\n",
      "Iteration 57, Training Loss: 0.6602101765441342\n",
      "Iteration 58, Training Loss: 0.6581850284981596\n",
      "Iteration 59, Training Loss: 0.6562114349670604\n",
      "Iteration 60, Training Loss: 0.6542848806212743\n",
      "Iteration 61, Training Loss: 0.6524045269186998\n",
      "Iteration 62, Training Loss: 0.6505713998137712\n",
      "Iteration 63, Training Loss: 0.6487810851628155\n",
      "Iteration 64, Training Loss: 0.6470318058904468\n",
      "Iteration 65, Training Loss: 0.6453230502998577\n",
      "Iteration 66, Training Loss: 0.6436549693696575\n",
      "Iteration 67, Training Loss: 0.6420255689629943\n",
      "Iteration 68, Training Loss: 0.6404358944745865\n",
      "Iteration 69, Training Loss: 0.6388874638995521\n",
      "Iteration 70, Training Loss: 0.6373761741165185\n",
      "Iteration 71, Training Loss: 0.6358990184752431\n",
      "Iteration 72, Training Loss: 0.6344556777118594\n",
      "Iteration 73, Training Loss: 0.6330426939519346\n",
      "Iteration 74, Training Loss: 0.6316664404835984\n",
      "Iteration 75, Training Loss: 0.6303204436724099\n",
      "Iteration 76, Training Loss: 0.6290063802121834\n",
      "Iteration 77, Training Loss: 0.6277233701054779\n",
      "Iteration 78, Training Loss: 0.6264684368810102\n",
      "Iteration 79, Training Loss: 0.6252417073377099\n",
      "Iteration 80, Training Loss: 0.6240432340827319\n",
      "Iteration 81, Training Loss: 0.62287084230648\n",
      "Iteration 82, Training Loss: 0.6217265457598463\n",
      "Iteration 83, Training Loss: 0.6206089363420075\n",
      "Iteration 84, Training Loss: 0.6195152759464584\n",
      "Iteration 85, Training Loss: 0.6184449914430368\n",
      "Iteration 86, Training Loss: 0.6173960561051482\n",
      "Iteration 87, Training Loss: 0.616372497996233\n",
      "Iteration 88, Training Loss: 0.615369087840653\n",
      "Iteration 89, Training Loss: 0.6143873307268175\n",
      "Iteration 90, Training Loss: 0.6134304738673995\n",
      "Iteration 91, Training Loss: 0.6124921354559041\n",
      "Iteration 92, Training Loss: 0.6115765005469654\n",
      "Iteration 93, Training Loss: 0.6106804498538078\n",
      "Iteration 94, Training Loss: 0.6098049767307997\n",
      "Iteration 95, Training Loss: 0.6089491424452279\n",
      "Iteration 96, Training Loss: 0.6081114723791436\n",
      "Iteration 97, Training Loss: 0.6072910485729068\n",
      "Iteration 98, Training Loss: 0.606489537041543\n",
      "Iteration 99, Training Loss: 0.6057070919492026\n",
      "Iteration 100, Training Loss: 0.6049412620690758\n",
      "Iteration 101, Training Loss: 0.6041909718961251\n",
      "Iteration 102, Training Loss: 0.6034570680441759\n",
      "Iteration 103, Training Loss: 0.6027379514758248\n",
      "Iteration 104, Training Loss: 0.6020348640029968\n",
      "Iteration 105, Training Loss: 0.6013467320865251\n",
      "Iteration 106, Training Loss: 0.6006748198907665\n",
      "Iteration 107, Training Loss: 0.6000160577388602\n",
      "Iteration 108, Training Loss: 0.5993721071384825\n",
      "Iteration 109, Training Loss: 0.5987439026281843\n",
      "Iteration 110, Training Loss: 0.598129386084963\n",
      "Iteration 111, Training Loss: 0.597527564389794\n",
      "Iteration 112, Training Loss: 0.5969395970641274\n",
      "Iteration 113, Training Loss: 0.5963625176005136\n",
      "Iteration 114, Training Loss: 0.5958000787704975\n",
      "Iteration 115, Training Loss: 0.595249967150929\n",
      "Iteration 116, Training Loss: 0.5947104597752675\n",
      "Iteration 117, Training Loss: 0.5941828165544559\n",
      "Iteration 118, Training Loss: 0.5936655487406758\n",
      "Iteration 119, Training Loss: 0.5931616851941364\n",
      "Iteration 120, Training Loss: 0.592667433990834\n",
      "Iteration 121, Training Loss: 0.5921836821262457\n",
      "Iteration 122, Training Loss: 0.5917108946931781\n",
      "Iteration 123, Training Loss: 0.5912473946319358\n",
      "Iteration 124, Training Loss: 0.5907942099901523\n",
      "Iteration 125, Training Loss: 0.5903488690989758\n",
      "Iteration 126, Training Loss: 0.5899124707732752\n",
      "Iteration 127, Training Loss: 0.5894853335787447\n",
      "Iteration 128, Training Loss: 0.5890670625181502\n",
      "Iteration 129, Training Loss: 0.5886577321175345\n",
      "Iteration 130, Training Loss: 0.5882576836215703\n",
      "Iteration 131, Training Loss: 0.5878656728820151\n",
      "Iteration 132, Training Loss: 0.5874822962697183\n",
      "Iteration 133, Training Loss: 0.5871063635590557\n",
      "Iteration 134, Training Loss: 0.5867394832030678\n",
      "Iteration 135, Training Loss: 0.5863787160665638\n",
      "Iteration 136, Training Loss: 0.5860261961023268\n",
      "Iteration 137, Training Loss: 0.5856820006048855\n",
      "Iteration 138, Training Loss: 0.5853447064282526\n",
      "Iteration 139, Training Loss: 0.5850136619381423\n",
      "Iteration 140, Training Loss: 0.5846907629945599\n",
      "Iteration 141, Training Loss: 0.5843743229564771\n",
      "Iteration 142, Training Loss: 0.5840634871634316\n",
      "Iteration 143, Training Loss: 0.5837596620191744\n",
      "Iteration 144, Training Loss: 0.5834624551856525\n",
      "Iteration 145, Training Loss: 0.5831729707122588\n",
      "Iteration 146, Training Loss: 0.5828898086855182\n",
      "Iteration 147, Training Loss: 0.5826125943152115\n",
      "Iteration 148, Training Loss: 0.582341177523976\n",
      "Iteration 149, Training Loss: 0.5820741965628655\n",
      "Iteration 150, Training Loss: 0.58181381499711\n",
      "Iteration 151, Training Loss: 0.5815581485076264\n",
      "Iteration 152, Training Loss: 0.581308603951982\n",
      "Iteration 153, Training Loss: 0.5810640413972756\n",
      "Iteration 154, Training Loss: 0.5808243875543129\n",
      "Iteration 155, Training Loss: 0.5805903096524492\n",
      "Iteration 156, Training Loss: 0.5803611612203424\n",
      "Iteration 157, Training Loss: 0.5801368546473993\n",
      "Iteration 158, Training Loss: 0.5799177899784774\n",
      "Iteration 159, Training Loss: 0.5797026238781195\n",
      "Iteration 160, Training Loss: 0.579493031827048\n",
      "Iteration 161, Training Loss: 0.5792861316956409\n",
      "Iteration 162, Training Loss: 0.5790851178909311\n",
      "Iteration 163, Training Loss: 0.57888842001991\n",
      "Iteration 164, Training Loss: 0.5786951286661913\n",
      "Iteration 165, Training Loss: 0.5785069957486435\n",
      "Iteration 166, Training Loss: 0.578322109740059\n",
      "Iteration 167, Training Loss: 0.5781401882793213\n",
      "Iteration 168, Training Loss: 0.577962914537376\n",
      "Iteration 169, Training Loss: 0.5777879205686989\n",
      "Iteration 170, Training Loss: 0.5776194404026164\n",
      "Iteration 171, Training Loss: 0.5774513186933603\n",
      "Iteration 172, Training Loss: 0.5772888348155109\n",
      "Iteration 173, Training Loss: 0.5771300982619063\n",
      "Iteration 174, Training Loss: 0.576972298345675\n",
      "Iteration 175, Training Loss: 0.5768201489095793\n",
      "Iteration 176, Training Loss: 0.5766690502526206\n",
      "Iteration 177, Training Loss: 0.576523995905198\n",
      "Iteration 178, Training Loss: 0.5763785657467135\n",
      "Iteration 179, Training Loss: 0.5762390635671657\n",
      "Iteration 180, Training Loss: 0.576100826976873\n",
      "Iteration 181, Training Loss: 0.5759661324775224\n",
      "Iteration 182, Training Loss: 0.575833360721877\n",
      "Iteration 183, Training Loss: 0.5757042748586377\n",
      "Iteration 184, Training Loss: 0.575577729127459\n",
      "Iteration 185, Training Loss: 0.5754519811395838\n",
      "Iteration 186, Training Loss: 0.5753317449627344\n",
      "Iteration 187, Training Loss: 0.5752103901371836\n",
      "Iteration 188, Training Loss: 0.5750967583848061\n",
      "Iteration 189, Training Loss: 0.5749803847942325\n",
      "Iteration 190, Training Loss: 0.5748706142334115\n",
      "Iteration 191, Training Loss: 0.5747602686759447\n",
      "Iteration 192, Training Loss: 0.5746536536289995\n",
      "Iteration 193, Training Loss: 0.5745491344989466\n",
      "Iteration 194, Training Loss: 0.5744447232290248\n",
      "Iteration 195, Training Loss: 0.5743478999714284\n",
      "Iteration 196, Training Loss: 0.5742475217509482\n",
      "Iteration 197, Training Loss: 0.5741526426959985\n",
      "Iteration 198, Training Loss: 0.5740576547242094\n",
      "Iteration 199, Training Loss: 0.5739661061087097\n",
      "Iteration 200, Training Loss: 0.5738750894690616\n",
      "Iteration 201, Training Loss: 0.5737874228790782\n",
      "Iteration 202, Training Loss: 0.573699968678472\n",
      "Iteration 203, Training Loss: 0.5736158128481251\n",
      "Iteration 204, Training Loss: 0.5735323148992257\n",
      "Iteration 205, Training Loss: 0.5734510077824937\n",
      "Iteration 206, Training Loss: 0.5733725555299005\n",
      "Iteration 207, Training Loss: 0.5732929194610671\n",
      "Iteration 208, Training Loss: 0.5732184089880132\n",
      "Iteration 209, Training Loss: 0.5731415851451263\n",
      "Iteration 210, Training Loss: 0.5730700795296836\n",
      "Iteration 211, Training Loss: 0.572996522809883\n",
      "Iteration 212, Training Loss: 0.572925904029645\n",
      "Iteration 213, Training Loss: 0.5728587130275444\n",
      "Iteration 214, Training Loss: 0.5727893533476152\n",
      "Iteration 215, Training Loss: 0.5727246356452613\n",
      "Iteration 216, Training Loss: 0.572659485996633\n",
      "Iteration 217, Training Loss: 0.572598119677202\n",
      "Iteration 218, Training Loss: 0.5725332271134023\n",
      "Iteration 219, Training Loss: 0.572475658806589\n",
      "Iteration 220, Training Loss: 0.5724137182113598\n",
      "Iteration 221, Training Loss: 0.5723562685472575\n",
      "Iteration 222, Training Loss: 0.5722994711166679\n",
      "Iteration 223, Training Loss: 0.5722423117081824\n",
      "Iteration 224, Training Loss: 0.5721875491158196\n",
      "Iteration 225, Training Loss: 0.5721351166813516\n",
      "Iteration 226, Training Loss: 0.5720795000872008\n",
      "Iteration 227, Training Loss: 0.572030019556633\n",
      "Iteration 228, Training Loss: 0.5719784801587485\n",
      "Iteration 229, Training Loss: 0.571929750457484\n",
      "Iteration 230, Training Loss: 0.5718782348724016\n",
      "Iteration 231, Training Loss: 0.5718332858648727\n",
      "Iteration 232, Training Loss: 0.5717849397300845\n",
      "Iteration 233, Training Loss: 0.5717413566608298\n",
      "Iteration 234, Training Loss: 0.5716943429290167\n",
      "Iteration 235, Training Loss: 0.5716533246219213\n",
      "Iteration 236, Training Loss: 0.5716075902786963\n",
      "Iteration 237, Training Loss: 0.5715680630964113\n",
      "Iteration 238, Training Loss: 0.5715242592386245\n",
      "Iteration 239, Training Loss: 0.571487124702136\n",
      "Iteration 240, Training Loss: 0.5714459779830717\n",
      "Iteration 241, Training Loss: 0.5714089059917363\n",
      "Iteration 242, Training Loss: 0.5713687953705031\n",
      "Iteration 243, Training Loss: 0.5713347631445566\n",
      "Iteration 244, Training Loss: 0.571294884310034\n",
      "Iteration 245, Training Loss: 0.5712625947957274\n",
      "Iteration 246, Training Loss: 0.5712254283480711\n",
      "Iteration 247, Training Loss: 0.5711932619811896\n",
      "Iteration 248, Training Loss: 0.5711582225879691\n",
      "Iteration 249, Training Loss: 0.5711266455145\n",
      "Iteration 250, Training Loss: 0.5710924532613276\n",
      "Iteration 251, Training Loss: 0.5710641884111625\n",
      "Iteration 252, Training Loss: 0.5710287356287973\n",
      "Iteration 253, Training Loss: 0.5710032911556003\n",
      "Iteration 254, Training Loss: 0.5709690227846534\n",
      "Iteration 255, Training Loss: 0.5709434951624834\n",
      "Iteration 256, Training Loss: 0.5709119483152153\n",
      "Iteration 257, Training Loss: 0.5708868042039265\n",
      "Iteration 258, Training Loss: 0.5708571437597457\n",
      "Iteration 259, Training Loss: 0.5708325900507\n",
      "Iteration 260, Training Loss: 0.5708042582108488\n",
      "Iteration 261, Training Loss: 0.570780782798643\n",
      "Iteration 262, Training Loss: 0.5707522118495805\n",
      "Iteration 263, Training Loss: 0.5707317848754543\n",
      "Iteration 264, Training Loss: 0.5707024788970668\n",
      "Iteration 265, Training Loss: 0.5706842699322673\n",
      "Iteration 266, Training Loss: 0.5706566597489343\n",
      "Iteration 267, Training Loss: 0.5706353644750286\n",
      "Iteration 268, Training Loss: 0.5706132499878523\n",
      "Iteration 269, Training Loss: 0.570591070071457\n",
      "Iteration 270, Training Loss: 0.5705710854464737\n",
      "Iteration 271, Training Loss: 0.5705474814615886\n",
      "Iteration 272, Training Loss: 0.5705314317221138\n",
      "Iteration 273, Training Loss: 0.5705058728286343\n",
      "Iteration 274, Training Loss: 0.5704918682817322\n",
      "Iteration 275, Training Loss: 0.5704678192151518\n",
      "Iteration 276, Training Loss: 0.5704523288516309\n",
      "Iteration 277, Training Loss: 0.5704311360209439\n",
      "Iteration 278, Training Loss: 0.5704155805044943\n",
      "Iteration 279, Training Loss: 0.5703953627993342\n",
      "Iteration 280, Training Loss: 0.5703805814940247\n",
      "Iteration 281, Training Loss: 0.5703597745473865\n",
      "Iteration 282, Training Loss: 0.5703479525515176\n",
      "Iteration 283, Training Loss: 0.5703265722988562\n",
      "Iteration 284, Training Loss: 0.570315866985136\n",
      "Iteration 285, Training Loss: 0.5702946121341508\n",
      "Iteration 286, Training Loss: 0.5702843759351514\n",
      "Iteration 287, Training Loss: 0.5702647460780327\n",
      "Iteration 288, Training Loss: 0.5702535584359053\n",
      "Iteration 289, Training Loss: 0.5702363244639863\n",
      "Iteration 290, Training Loss: 0.5702271969298983\n",
      "Iteration 291, Training Loss: 0.5702077411193952\n",
      "Iteration 292, Training Loss: 0.5701984486272645\n",
      "Iteration 293, Training Loss: 0.5701805493597647\n",
      "Iteration 294, Training Loss: 0.5701730643048456\n",
      "Iteration 295, Training Loss: 0.5701559300979324\n",
      "Iteration 296, Training Loss: 0.5701453819247473\n",
      "Iteration 297, Training Loss: 0.5701310706780861\n",
      "Iteration 298, Training Loss: 0.5701240557847438\n",
      "Iteration 299, Training Loss: 0.5701053867345278\n",
      "Iteration 300, Training Loss: 0.5701013943657033\n",
      "Iteration 301, Training Loss: 0.5700844167675024\n",
      "Iteration 302, Training Loss: 0.5700748203072526\n",
      "Iteration 303, Training Loss: 0.570062429640564\n",
      "Iteration 304, Training Loss: 0.5700553057037626\n",
      "Iteration 305, Training Loss: 0.570041258776322\n",
      "Iteration 306, Training Loss: 0.5700362970891639\n",
      "Iteration 307, Training Loss: 0.5700203929627912\n",
      "Iteration 308, Training Loss: 0.5700168437300525\n",
      "Iteration 309, Training Loss: 0.5700018071182447\n",
      "Iteration 310, Training Loss: 0.5699956497037144\n",
      "Iteration 311, Training Loss: 0.5699825769428174\n",
      "Iteration 312, Training Loss: 0.5699797803914309\n",
      "Iteration 313, Training Loss: 0.5699662505996345\n",
      "Iteration 314, Training Loss: 0.5699592633553594\n",
      "Iteration 315, Training Loss: 0.5699479014295288\n",
      "Iteration 316, Training Loss: 0.5699460810127321\n",
      "Iteration 317, Training Loss: 0.569931701925386\n",
      "Iteration 318, Training Loss: 0.5699276366826699\n",
      "Iteration 319, Training Loss: 0.5699169044284189\n",
      "Iteration 320, Training Loss: 0.5699117042491435\n",
      "Iteration 321, Training Loss: 0.5699020268150536\n",
      "Iteration 322, Training Loss: 0.5698971265063207\n",
      "Iteration 323, Training Loss: 0.5698872557236996\n",
      "Iteration 324, Training Loss: 0.569884832483485\n",
      "Iteration 325, Training Loss: 0.5698717072004713\n",
      "Iteration 326, Training Loss: 0.5698709041459274\n",
      "Iteration 327, Training Loss: 0.5698600419727581\n",
      "Iteration 328, Training Loss: 0.5698549173150333\n",
      "Iteration 329, Training Loss: 0.5698466547989004\n",
      "Iteration 330, Training Loss: 0.5698468675959881\n",
      "Iteration 331, Training Loss: 0.5698339663010685\n",
      "Iteration 332, Training Loss: 0.5698331091214943\n",
      "Iteration 333, Training Loss: 0.5698222592637741\n",
      "Iteration 334, Training Loss: 0.5698196989187633\n",
      "Iteration 335, Training Loss: 0.5698114022780261\n",
      "Iteration 336, Training Loss: 0.5698094902850221\n",
      "Iteration 337, Training Loss: 0.5698003663083425\n",
      "Iteration 338, Training Loss: 0.5697992263660494\n",
      "Iteration 339, Training Loss: 0.5697898023808945\n",
      "Iteration 340, Training Loss: 0.5697908617417672\n",
      "Iteration 341, Training Loss: 0.569779345606259\n",
      "Iteration 342, Training Loss: 0.569778614864395\n",
      "Iteration 343, Training Loss: 0.5697703523660228\n",
      "Iteration 344, Training Loss: 0.5697687987335625\n",
      "Iteration 345, Training Loss: 0.5697610138904483\n",
      "Iteration 346, Training Loss: 0.5697614223537611\n",
      "Iteration 347, Training Loss: 0.5697504830809499\n",
      "Iteration 348, Training Loss: 0.5697538143419993\n",
      "Iteration 349, Training Loss: 0.5697432815615887\n",
      "Iteration 350, Training Loss: 0.5697422858321215\n",
      "Iteration 351, Training Loss: 0.5697340271948578\n",
      "Iteration 352, Training Loss: 0.5697369592863357\n",
      "Iteration 353, Training Loss: 0.569727527721891\n",
      "Iteration 354, Training Loss: 0.5697243327007864\n",
      "Iteration 355, Training Loss: 0.5697214366805792\n",
      "Iteration 356, Training Loss: 0.5697163348479877\n",
      "Iteration 357, Training Loss: 0.5697171220202716\n",
      "Iteration 358, Training Loss: 0.5697085666582843\n",
      "Iteration 359, Training Loss: 0.5697112683607836\n",
      "Iteration 360, Training Loss: 0.5697023610558205\n",
      "Iteration 361, Training Loss: 0.5697009045549183\n",
      "Iteration 362, Training Loss: 0.5696954693004589\n",
      "Iteration 363, Training Loss: 0.5696969386455814\n",
      "Iteration 364, Training Loss: 0.569688579521813\n",
      "Iteration 365, Training Loss: 0.5696933860146725\n",
      "Iteration 366, Training Loss: 0.5696833113451754\n",
      "Iteration 367, Training Loss: 0.5696824922392587\n",
      "Iteration 368, Training Loss: 0.5696771699925832\n",
      "Iteration 369, Training Loss: 0.5696790046587374\n",
      "Iteration 370, Training Loss: 0.569671744143351\n",
      "Iteration 371, Training Loss: 0.5696733752264241\n",
      "Iteration 372, Training Loss: 0.5696657374764514\n",
      "Iteration 373, Training Loss: 0.5696700621846423\n",
      "Iteration 374, Training Loss: 0.5696614410530608\n",
      "Iteration 375, Training Loss: 0.5696620688025135\n",
      "Iteration 376, Training Loss: 0.5696556667538705\n",
      "Iteration 377, Training Loss: 0.5696606551054526\n",
      "Iteration 378, Training Loss: 0.5696519902448892\n",
      "Iteration 379, Training Loss: 0.5696518169270961\n",
      "Iteration 380, Training Loss: 0.5696463524916445\n",
      "Iteration 381, Training Loss: 0.56965006856795\n",
      "Iteration 382, Training Loss: 0.5696428772263832\n",
      "Iteration 383, Training Loss: 0.5696415673721913\n",
      "Iteration 384, Training Loss: 0.5696398547880629\n",
      "Iteration 385, Training Loss: 0.5696373804552319\n",
      "Iteration 386, Training Loss: 0.5696369776265399\n",
      "Iteration 387, Training Loss: 0.5696333031350468\n",
      "Iteration 388, Training Loss: 0.5696352602335316\n",
      "Iteration 389, Training Loss: 0.5696281225233059\n",
      "Iteration 390, Training Loss: 0.5696336662727639\n",
      "Iteration 391, Training Loss: 0.5696240178031549\n",
      "Iteration 392, Training Loss: 0.5696279078043037\n",
      "Iteration 393, Training Loss: 0.5696229330952893\n",
      "Iteration 394, Training Loss: 0.5696205687909676\n",
      "Iteration 395, Training Loss: 0.5696212583324982\n",
      "Iteration 396, Training Loss: 0.5696161699778092\n",
      "Iteration 397, Training Loss: 0.5696206924215494\n",
      "Iteration 398, Training Loss: 0.5696130648802149\n",
      "Iteration 399, Training Loss: 0.5696171927807429\n",
      "Iteration 400, Training Loss: 0.5696106685254656\n",
      "Iteration 401, Training Loss: 0.5696110319882661\n",
      "Iteration 402, Training Loss: 0.5696073866738582\n",
      "Iteration 403, Training Loss: 0.5696110993991934\n",
      "Iteration 404, Training Loss: 0.5696044641790069\n",
      "Iteration 405, Training Loss: 0.5696084444703319\n",
      "Iteration 406, Training Loss: 0.5696010787174983\n",
      "Iteration 407, Training Loss: 0.5696047121673119\n",
      "Iteration 408, Training Loss: 0.569599669827647\n",
      "Iteration 409, Training Loss: 0.5696007511524981\n",
      "Iteration 410, Training Loss: 0.5695968428231751\n",
      "Iteration 411, Training Loss: 0.569601775214497\n",
      "Iteration 412, Training Loss: 0.5695937479737729\n",
      "Iteration 413, Training Loss: 0.5695989801378556\n",
      "Iteration 414, Training Loss: 0.5695925779566577\n",
      "Iteration 415, Training Loss: 0.5695923143855058\n",
      "Iteration 416, Training Loss: 0.569590070889625\n",
      "Iteration 417, Training Loss: 0.5695928497417287\n",
      "Iteration 418, Training Loss: 0.5695873144289518\n",
      "Iteration 419, Training Loss: 0.5695912178266402\n",
      "Iteration 420, Training Loss: 0.5695846955092694\n",
      "Iteration 421, Training Loss: 0.5695898371191992\n",
      "Iteration 422, Training Loss: 0.5695830876756531\n",
      "Iteration 423, Training Loss: 0.5695872826325981\n",
      "Iteration 424, Training Loss: 0.5695818813174311\n",
      "Iteration 425, Training Loss: 0.5695830775513214\n",
      "Iteration 426, Training Loss: 0.5695802607565368\n",
      "Iteration 427, Training Loss: 0.5695835264437522\n",
      "Iteration 428, Training Loss: 0.5695786704872807\n",
      "Iteration 429, Training Loss: 0.5695826875654114\n",
      "Iteration 430, Training Loss: 0.5695740772003564\n",
      "Iteration 431, Training Loss: 0.5695824607753189\n",
      "Iteration 432, Training Loss: 0.5695763705497148\n",
      "Iteration 433, Training Loss: 0.5695749420943288\n",
      "Iteration 434, Training Loss: 0.5695747728540438\n",
      "Iteration 435, Training Loss: 0.5695743653991362\n",
      "Iteration 436, Training Loss: 0.5695737179550605\n",
      "Iteration 437, Training Loss: 0.5695728315511353\n",
      "Iteration 438, Training Loss: 0.5695712000843159\n",
      "Iteration 439, Training Loss: 0.56957486632279\n",
      "Iteration 440, Training Loss: 0.5695686965055701\n",
      "Iteration 441, Training Loss: 0.5695756981551371\n",
      "Iteration 442, Training Loss: 0.5695687199391778\n",
      "Iteration 443, Training Loss: 0.5695699716943583\n",
      "Iteration 444, Training Loss: 0.5695676412162507\n",
      "Iteration 445, Training Loss: 0.569569137856473\n",
      "Iteration 446, Training Loss: 0.5695660904158326\n",
      "Iteration 447, Training Loss: 0.5695706271033324\n",
      "Iteration 448, Training Loss: 0.5695632697686531\n",
      "Iteration 449, Training Loss: 0.5695703414912369\n",
      "Iteration 450, Training Loss: 0.5695634823341625\n",
      "Iteration 451, Training Loss: 0.569566090935684\n",
      "Iteration 452, Training Loss: 0.5695623300002589\n",
      "Iteration 453, Training Loss: 0.5695662088333047\n",
      "Iteration 454, Training Loss: 0.5695616889332689\n",
      "Iteration 455, Training Loss: 0.5695637958326204\n",
      "Iteration 456, Training Loss: 0.5695596052177918\n",
      "Iteration 457, Training Loss: 0.5695665234470904\n",
      "Iteration 458, Training Loss: 0.5695587519497608\n",
      "Iteration 459, Training Loss: 0.5695619187072021\n",
      "Iteration 460, Training Loss: 0.5695575008450724\n",
      "Iteration 461, Training Loss: 0.5695642358101015\n",
      "Iteration 462, Training Loss: 0.5695582460770848\n",
      "Iteration 463, Training Loss: 0.5695584128096864\n",
      "Iteration 464, Training Loss: 0.5695583485981455\n",
      "Iteration 465, Training Loss: 0.5695560384323238\n",
      "Iteration 466, Training Loss: 0.5695612420722297\n",
      "Iteration 467, Training Loss: 0.569554574422389\n",
      "Iteration 468, Training Loss: 0.5695603601795257\n",
      "Iteration 469, Training Loss: 0.5695549835093667\n",
      "Iteration 470, Training Loss: 0.5695579615333879\n",
      "Iteration 471, Training Loss: 0.5695543662471373\n",
      "Iteration 472, Training Loss: 0.5695574157709045\n",
      "Iteration 473, Training Loss: 0.5695530784301005\n",
      "Iteration 474, Training Loss: 0.5695573583937897\n",
      "Iteration 475, Training Loss: 0.5695522802847325\n",
      "Iteration 476, Training Loss: 0.5695575850787302\n",
      "Iteration 477, Training Loss: 0.5695507391910285\n",
      "Iteration 478, Training Loss: 0.5695581134014682\n",
      "Iteration 479, Training Loss: 0.5695520018375507\n",
      "Iteration 480, Training Loss: 0.5695532938855612\n",
      "Iteration 481, Training Loss: 0.5695502922557809\n",
      "Iteration 482, Training Loss: 0.5695574185082546\n",
      "Iteration 483, Training Loss: 0.569551055522418\n",
      "Iteration 484, Training Loss: 0.5695523817486037\n",
      "Iteration 485, Training Loss: 0.5695501709635934\n",
      "Iteration 486, Training Loss: 0.5695537993085321\n",
      "Iteration 487, Training Loss: 0.5695497883567666\n",
      "Iteration 488, Training Loss: 0.5695523834741208\n",
      "Iteration 489, Training Loss: 0.5695481666223237\n",
      "Iteration 490, Training Loss: 0.5695550780505133\n",
      "Iteration 491, Training Loss: 0.5695485069215757\n",
      "Iteration 492, Training Loss: 0.5695506046914206\n",
      "Iteration 493, Training Loss: 0.569548436094676\n",
      "Iteration 494, Training Loss: 0.5695508247513006\n",
      "Iteration 495, Training Loss: 0.5695471419739687\n",
      "Iteration 496, Training Loss: 0.5695525535913275\n",
      "Iteration 497, Training Loss: 0.5695468112863298\n",
      "Iteration 498, Training Loss: 0.5695509634710196\n",
      "Iteration 499, Training Loss: 0.5695465132408314\n",
      "Iteration 500, Training Loss: 0.5695502106493056\n",
      "Iteration 501, Training Loss: 0.5695467749435873\n",
      "Iteration 502, Training Loss: 0.5695489428983691\n",
      "Iteration 503, Training Loss: 0.569545012775894\n",
      "Iteration 504, Training Loss: 0.5695519833864344\n",
      "Iteration 505, Training Loss: 0.5695437406029815\n",
      "Iteration 506, Training Loss: 0.5695524622407659\n",
      "Iteration 507, Training Loss: 0.5695454512797964\n",
      "Iteration 508, Training Loss: 0.5695461597922735\n",
      "Iteration 509, Training Loss: 0.5695455667458864\n",
      "Iteration 510, Training Loss: 0.5695460078080807\n",
      "Iteration 511, Training Loss: 0.5695454251848893\n",
      "Iteration 512, Training Loss: 0.5695473562459253\n",
      "Iteration 513, Training Loss: 0.5695437348683356\n",
      "Iteration 514, Training Loss: 0.5695487411733248\n",
      "Iteration 515, Training Loss: 0.5695430795403166\n",
      "Iteration 516, Training Loss: 0.5695495608192348\n",
      "Iteration 517, Training Loss: 0.5695441372763489\n",
      "Iteration 518, Training Loss: 0.5695455913763583\n",
      "Iteration 519, Training Loss: 0.5695422739346576\n",
      "Iteration 520, Training Loss: 0.5695508078054696\n",
      "Iteration 521, Training Loss: 0.5695423103403764\n",
      "Iteration 522, Training Loss: 0.569545056722129\n",
      "Iteration 523, Training Loss: 0.5695419918045748\n",
      "Iteration 524, Training Loss: 0.5695490565196651\n",
      "Iteration 525, Training Loss: 0.5695426151729112\n",
      "Iteration 526, Training Loss: 0.5695448592445574\n",
      "Iteration 527, Training Loss: 0.5695427914801903\n",
      "Iteration 528, Training Loss: 0.5695452783221797\n",
      "Iteration 529, Training Loss: 0.5695416901992677\n",
      "Iteration 530, Training Loss: 0.5695477344635231\n",
      "Iteration 531, Training Loss: 0.569541089898128\n",
      "Iteration 532, Training Loss: 0.5695465723459536\n",
      "Iteration 533, Training Loss: 0.5695417120273776\n",
      "Iteration 534, Training Loss: 0.5695442592923329\n",
      "Iteration 535, Training Loss: 0.56954194066545\n",
      "Iteration 536, Training Loss: 0.5695437051100023\n",
      "Iteration 537, Training Loss: 0.5695406664315202\n",
      "Iteration 538, Training Loss: 0.5695479768879759\n",
      "Iteration 539, Training Loss: 0.5695402872371791\n",
      "Iteration 540, Training Loss: 0.5695465683729702\n",
      "Iteration 541, Training Loss: 0.569541232582722\n",
      "Iteration 542, Training Loss: 0.5695417545847711\n",
      "Iteration 543, Training Loss: 0.5695420058408628\n",
      "Iteration 544, Training Loss: 0.569542007821863\n",
      "Iteration 545, Training Loss: 0.5695427374200592\n",
      "Iteration 546, Training Loss: 0.5695407337148973\n",
      "Iteration 547, Training Loss: 0.5695455376394345\n",
      "Iteration 548, Training Loss: 0.5695399413143749\n",
      "Iteration 549, Training Loss: 0.5695452238816499\n",
      "Iteration 550, Training Loss: 0.5695396303539209\n",
      "Iteration 551, Training Loss: 0.5695452337127163\n",
      "Iteration 552, Training Loss: 0.5695403913200775\n",
      "Iteration 553, Training Loss: 0.5695426620696202\n",
      "Iteration 554, Training Loss: 0.569539386018243\n",
      "Iteration 555, Training Loss: 0.5695462073771171\n",
      "Iteration 556, Training Loss: 0.5695390789112074\n",
      "Iteration 557, Training Loss: 0.5695443912310485\n",
      "Iteration 558, Training Loss: 0.5695398052951378\n",
      "Iteration 559, Training Loss: 0.5695431539619557\n",
      "Iteration 560, Training Loss: 0.5695406118100819\n",
      "Iteration 561, Training Loss: 0.5695406392873748\n",
      "Iteration 562, Training Loss: 0.5695424120700442\n",
      "Iteration 563, Training Loss: 0.569539890802403\n",
      "Iteration 564, Training Loss: 0.5695441961108008\n",
      "Iteration 565, Training Loss: 0.5695381226478342\n",
      "Iteration 566, Training Loss: 0.5695457381950381\n",
      "Iteration 567, Training Loss: 0.5695383618803669\n",
      "Iteration 568, Training Loss: 0.5695419571340731\n",
      "Iteration 569, Training Loss: 0.5695384419148943\n",
      "Iteration 570, Training Loss: 0.5695452775555387\n",
      "Iteration 571, Training Loss: 0.569538681811255\n",
      "Iteration 572, Training Loss: 0.5695415183768371\n",
      "Iteration 573, Training Loss: 0.5695387776341199\n",
      "Iteration 574, Training Loss: 0.5695435775760276\n",
      "Iteration 575, Training Loss: 0.5695382908705906\n",
      "Early stopping at iteration 575\n",
      "Iteration 0, Training Loss: 1.0\n",
      "Iteration 1, Training Loss: 0.8876264674942123\n",
      "Iteration 2, Training Loss: 0.7981772334956132\n",
      "Iteration 3, Training Loss: 0.7312145532326775\n",
      "Iteration 4, Training Loss: 0.6891312568013277\n",
      "Iteration 5, Training Loss: 0.6643177033290867\n",
      "Iteration 6, Training Loss: 0.6424807906705797\n",
      "Iteration 7, Training Loss: 0.6274755341369712\n",
      "Iteration 8, Training Loss: 0.6144591844271635\n",
      "Iteration 9, Training Loss: 0.6055171068076427\n",
      "Iteration 10, Training Loss: 0.5973431950870082\n",
      "Iteration 11, Training Loss: 0.5922181195255746\n",
      "Iteration 12, Training Loss: 0.5867024749376586\n",
      "Iteration 13, Training Loss: 0.5840318216082456\n",
      "Iteration 14, Training Loss: 0.579995228406123\n",
      "Iteration 15, Training Loss: 0.5791002495797845\n",
      "Iteration 16, Training Loss: 0.5756656464711982\n",
      "Iteration 17, Training Loss: 0.5762513879732226\n",
      "Iteration 18, Training Loss: 0.5727685048073381\n",
      "Iteration 19, Training Loss: 0.5747762628486505\n",
      "Iteration 20, Training Loss: 0.5707001587897425\n",
      "Iteration 21, Training Loss: 0.5744434533018599\n",
      "Iteration 22, Training Loss: 0.5691993127365981\n",
      "Iteration 23, Training Loss: 0.5749990749383144\n",
      "Iteration 24, Training Loss: 0.5681332348772009\n",
      "Iteration 25, Training Loss: 0.5760494010099892\n",
      "Iteration 26, Training Loss: 0.5675463396910131\n",
      "Iteration 27, Training Loss: 0.5770134175085837\n",
      "Iteration 28, Training Loss: 0.5673245235393879\n",
      "Iteration 29, Training Loss: 0.5774016608010626\n",
      "Iteration 30, Training Loss: 0.5672817509702954\n",
      "Iteration 31, Training Loss: 0.577558842136557\n",
      "Iteration 32, Training Loss: 0.5672789722390709\n",
      "Iteration 33, Training Loss: 0.5775451082289736\n",
      "Iteration 34, Training Loss: 0.567315760497467\n",
      "Iteration 35, Training Loss: 0.5774093869410273\n",
      "Iteration 36, Training Loss: 0.5673235529686476\n",
      "Iteration 37, Training Loss: 0.577287723012423\n",
      "Iteration 38, Training Loss: 0.567323469344041\n",
      "Iteration 39, Training Loss: 0.577236142235224\n",
      "Iteration 40, Training Loss: 0.5673181907108665\n",
      "Iteration 41, Training Loss: 0.5772324648341806\n",
      "Iteration 42, Training Loss: 0.5673013588869612\n",
      "Early stopping at iteration 42\n",
      "Iteration 0, Training Loss: 1.0\n",
      "Iteration 1, Training Loss: 0.9887626467494214\n",
      "Iteration 2, Training Loss: 0.9779747876288656\n",
      "Iteration 3, Training Loss: 0.9676184428731323\n",
      "Iteration 4, Training Loss: 0.9576763519076282\n",
      "Iteration 5, Training Loss: 0.9481319445807441\n",
      "Iteration 6, Training Loss: 0.9389693135469356\n",
      "Iteration 7, Training Loss: 0.9301731877544795\n",
      "Iteration 8, Training Loss: 0.9217289069937215\n",
      "Iteration 9, Training Loss: 0.9136223974633939\n",
      "Iteration 10, Training Loss: 0.9058401483142795\n",
      "Iteration 11, Training Loss: 0.8983691891311294\n",
      "Iteration 12, Training Loss: 0.8911970683153055\n",
      "Iteration 13, Training Loss: 0.8843118323321144\n",
      "Iteration 14, Training Loss: 0.8777020057882511\n",
      "Iteration 15, Training Loss: 0.8713570335866206\n",
      "Iteration 16, Training Loss: 0.8652696242480998\n",
      "Iteration 17, Training Loss: 0.8594315159782779\n",
      "Iteration 18, Training Loss: 0.8538331849543804\n",
      "Iteration 19, Training Loss: 0.8484694234538943\n",
      "Iteration 20, Training Loss: 0.8433282761499509\n",
      "Iteration 21, Training Loss: 0.838402120780221\n",
      "Iteration 22, Training Loss: 0.8336865562162534\n",
      "Iteration 23, Training Loss: 0.8291730174426889\n",
      "Iteration 24, Training Loss: 0.8248548787405514\n",
      "Iteration 25, Training Loss: 0.820728508594322\n",
      "Iteration 26, Training Loss: 0.816782766490078\n",
      "Iteration 27, Training Loss: 0.8130127846698196\n",
      "Iteration 28, Training Loss: 0.809417599665528\n",
      "Iteration 29, Training Loss: 0.8059836206363534\n",
      "Iteration 30, Training Loss: 0.8027090988756422\n",
      "Iteration 31, Training Loss: 0.799578500254863\n",
      "Iteration 32, Training Loss: 0.7965932083171543\n",
      "Iteration 33, Training Loss: 0.7937417809365188\n",
      "Iteration 34, Training Loss: 0.7910217814709002\n",
      "Iteration 35, Training Loss: 0.7884283194185482\n",
      "Iteration 36, Training Loss: 0.7859495381720204\n",
      "Iteration 37, Training Loss: 0.7835836376259588\n",
      "Iteration 38, Training Loss: 0.7813204967242726\n",
      "Iteration 39, Training Loss: 0.7791587433116784\n",
      "Iteration 40, Training Loss: 0.777095065526663\n",
      "Iteration 41, Training Loss: 0.7751270842368112\n",
      "Iteration 42, Training Loss: 0.773249122167476\n",
      "Iteration 43, Training Loss: 0.7714598831330464\n",
      "Iteration 44, Training Loss: 0.7697501363359193\n",
      "Iteration 45, Training Loss: 0.7681194861452337\n",
      "Iteration 46, Training Loss: 0.7665610919617313\n",
      "Iteration 47, Training Loss: 0.7650736261740372\n",
      "Iteration 48, Training Loss: 0.7636538199458346\n",
      "Iteration 49, Training Loss: 0.7623016268747695\n",
      "Iteration 50, Training Loss: 0.7610112004991816\n",
      "Iteration 51, Training Loss: 0.7597797834557478\n",
      "Iteration 52, Training Loss: 0.7586089860040006\n",
      "Iteration 53, Training Loss: 0.7574924215306905\n",
      "Iteration 54, Training Loss: 0.7564297163714683\n",
      "Iteration 55, Training Loss: 0.7554169002456571\n",
      "Iteration 56, Training Loss: 0.7544507455228391\n",
      "Iteration 57, Training Loss: 0.7535273472390998\n",
      "Iteration 58, Training Loss: 0.7526464850489358\n",
      "Iteration 59, Training Loss: 0.7518074059938631\n",
      "Iteration 60, Training Loss: 0.7510036614030768\n",
      "Iteration 61, Training Loss: 0.7502385380237335\n",
      "Iteration 62, Training Loss: 0.7495072699037082\n",
      "Iteration 63, Training Loss: 0.7488085313433271\n",
      "Iteration 64, Training Loss: 0.7481436878414472\n",
      "Iteration 65, Training Loss: 0.7475087725175539\n",
      "Iteration 66, Training Loss: 0.7469056407228668\n",
      "Iteration 67, Training Loss: 0.7463314068302901\n",
      "Iteration 68, Training Loss: 0.7457812318429172\n",
      "Iteration 69, Training Loss: 0.7452562431462159\n",
      "Iteration 70, Training Loss: 0.7447543850796274\n",
      "Iteration 71, Training Loss: 0.7442767725011061\n",
      "Iteration 72, Training Loss: 0.7438203758511734\n",
      "Iteration 73, Training Loss: 0.743385864372184\n",
      "Iteration 74, Training Loss: 0.7429708350920177\n",
      "Iteration 75, Training Loss: 0.7425750090507792\n",
      "Iteration 76, Training Loss: 0.7421960596942188\n",
      "Iteration 77, Training Loss: 0.7418333173449176\n",
      "Iteration 78, Training Loss: 0.7414866392780466\n",
      "Iteration 79, Training Loss: 0.7411564044907765\n",
      "Iteration 80, Training Loss: 0.7408409143624821\n",
      "Iteration 81, Training Loss: 0.7405390851288187\n",
      "Iteration 82, Training Loss: 0.7402498518739575\n",
      "Iteration 83, Training Loss: 0.7399721815251035\n",
      "Iteration 84, Training Loss: 0.7397066416589692\n",
      "Iteration 85, Training Loss: 0.7394522385689568\n",
      "Iteration 86, Training Loss: 0.7392090376291862\n",
      "Iteration 87, Training Loss: 0.7389770985283974\n",
      "Iteration 88, Training Loss: 0.738754953616612\n",
      "Iteration 89, Training Loss: 0.7385416991811113\n",
      "Iteration 90, Training Loss: 0.7383385249341208\n",
      "Iteration 91, Training Loss: 0.7381429522913533\n",
      "Iteration 92, Training Loss: 0.7379552152611532\n",
      "Iteration 93, Training Loss: 0.7377755085318594\n",
      "Iteration 94, Training Loss: 0.7376045097173984\n",
      "Iteration 95, Training Loss: 0.7374398577959953\n",
      "Iteration 96, Training Loss: 0.73728328935318\n",
      "Iteration 97, Training Loss: 0.7371319928808439\n",
      "Iteration 98, Training Loss: 0.7369872393364466\n",
      "Iteration 99, Training Loss: 0.7368487807515872\n",
      "Iteration 100, Training Loss: 0.7367153702399633\n",
      "Iteration 101, Training Loss: 0.7365883066470547\n",
      "Iteration 102, Training Loss: 0.736466328440731\n",
      "Iteration 103, Training Loss: 0.7363497360030896\n",
      "Iteration 104, Training Loss: 0.7362373136816739\n",
      "Iteration 105, Training Loss: 0.7361298827386231\n",
      "Iteration 106, Training Loss: 0.736027268377272\n",
      "Iteration 107, Training Loss: 0.735929254634761\n",
      "Iteration 108, Training Loss: 0.7358351679954145\n",
      "Iteration 109, Training Loss: 0.7357443433043213\n",
      "Iteration 110, Training Loss: 0.735658150844004\n",
      "Iteration 111, Training Loss: 0.7355744078755095\n",
      "Iteration 112, Training Loss: 0.7354945130479636\n",
      "Iteration 113, Training Loss: 0.7354178269500428\n",
      "Iteration 114, Training Loss: 0.735344704909056\n",
      "Iteration 115, Training Loss: 0.7352750154087896\n",
      "Iteration 116, Training Loss: 0.7352070945198995\n",
      "Iteration 117, Training Loss: 0.7351419006297886\n",
      "Iteration 118, Training Loss: 0.7350803191741647\n",
      "Iteration 119, Training Loss: 0.7350201919569073\n",
      "Iteration 120, Training Loss: 0.7349629660765187\n",
      "Iteration 121, Training Loss: 0.7349075441047667\n",
      "Iteration 122, Training Loss: 0.7348553225442906\n",
      "Iteration 123, Training Loss: 0.7348031952936116\n",
      "Iteration 124, Training Loss: 0.7347556637657425\n",
      "Iteration 125, Training Loss: 0.7347090407677825\n",
      "Iteration 126, Training Loss: 0.7346647673798834\n",
      "Iteration 127, Training Loss: 0.7346232843238774\n",
      "Iteration 128, Training Loss: 0.734580937152164\n",
      "Iteration 129, Training Loss: 0.7345422860984452\n",
      "Iteration 130, Training Loss: 0.7345036830879904\n",
      "Iteration 131, Training Loss: 0.7344691370613609\n",
      "Iteration 132, Training Loss: 0.7344329579706322\n",
      "Iteration 133, Training Loss: 0.7343997279276993\n",
      "Iteration 134, Training Loss: 0.7343673299905998\n",
      "Iteration 135, Training Loss: 0.734336729326715\n",
      "Iteration 136, Training Loss: 0.7343063533749503\n",
      "Iteration 137, Training Loss: 0.7342796983189311\n",
      "Iteration 138, Training Loss: 0.7342510943338829\n",
      "Iteration 139, Training Loss: 0.7342246446059095\n",
      "Iteration 140, Training Loss: 0.7341992393212282\n",
      "Iteration 141, Training Loss: 0.734175877658169\n",
      "Iteration 142, Training Loss: 0.7341524253776025\n",
      "Iteration 143, Training Loss: 0.7341294187195087\n",
      "Iteration 144, Training Loss: 0.7341093441654988\n",
      "Iteration 145, Training Loss: 0.7340875573823763\n",
      "Iteration 146, Training Loss: 0.7340686485926926\n",
      "Iteration 147, Training Loss: 0.7340494939497374\n",
      "Iteration 148, Training Loss: 0.7340316062897285\n",
      "Iteration 149, Training Loss: 0.7340129213359361\n",
      "Iteration 150, Training Loss: 0.7339970044196499\n",
      "Iteration 151, Training Loss: 0.7339802123144457\n",
      "Iteration 152, Training Loss: 0.7339651031618735\n",
      "Iteration 153, Training Loss: 0.7339510911658649\n",
      "Iteration 154, Training Loss: 0.7339361412350646\n",
      "Iteration 155, Training Loss: 0.7339227821392799\n",
      "Iteration 156, Training Loss: 0.733908463199935\n",
      "Iteration 157, Training Loss: 0.7338982372468513\n",
      "Iteration 158, Training Loss: 0.7338858959681092\n",
      "Iteration 159, Training Loss: 0.7338740480192295\n",
      "Iteration 160, Training Loss: 0.7338631818545072\n",
      "Iteration 161, Training Loss: 0.7338527583765465\n",
      "Iteration 162, Training Loss: 0.7338432404854581\n",
      "Iteration 163, Training Loss: 0.7338315957678533\n",
      "Iteration 164, Training Loss: 0.7338244370247179\n",
      "Iteration 165, Training Loss: 0.7338140479307488\n",
      "Iteration 166, Training Loss: 0.7338065843220255\n",
      "Iteration 167, Training Loss: 0.733797909749549\n",
      "Iteration 168, Training Loss: 0.7337900923307321\n",
      "Iteration 169, Training Loss: 0.7337825844695467\n",
      "Iteration 170, Training Loss: 0.7337748706336841\n",
      "Iteration 171, Training Loss: 0.7337689854513606\n",
      "Iteration 172, Training Loss: 0.733761322094785\n",
      "Iteration 173, Training Loss: 0.7337554536833569\n",
      "Iteration 174, Training Loss: 0.7337483294075641\n",
      "Iteration 175, Training Loss: 0.7337439954406435\n",
      "Iteration 176, Training Loss: 0.7337368209023487\n",
      "Iteration 177, Training Loss: 0.7337319533752238\n",
      "Iteration 178, Training Loss: 0.7337272661801872\n",
      "Iteration 179, Training Loss: 0.7337202581671544\n",
      "Iteration 180, Training Loss: 0.7337170564465115\n",
      "Iteration 181, Training Loss: 0.7337109608654068\n",
      "Iteration 182, Training Loss: 0.733708635812422\n",
      "Iteration 183, Training Loss: 0.7337028888885202\n",
      "Iteration 184, Training Loss: 0.7336993593974906\n",
      "Iteration 185, Training Loss: 0.7336944703817143\n",
      "Iteration 186, Training Loss: 0.7336923098579027\n",
      "Iteration 187, Training Loss: 0.733687702497123\n",
      "Iteration 188, Training Loss: 0.7336847978711584\n",
      "Iteration 189, Training Loss: 0.7336814899187558\n",
      "Iteration 190, Training Loss: 0.7336773227858597\n",
      "Iteration 191, Training Loss: 0.7336758444228354\n",
      "Iteration 192, Training Loss: 0.7336718975996689\n",
      "Iteration 193, Training Loss: 0.733669109263509\n",
      "Iteration 194, Training Loss: 0.7336659351974627\n",
      "Iteration 195, Training Loss: 0.7336644060139813\n",
      "Iteration 196, Training Loss: 0.7336604159893344\n",
      "Iteration 197, Training Loss: 0.7336585952747899\n",
      "Iteration 198, Training Loss: 0.7336573538944737\n",
      "Iteration 199, Training Loss: 0.7336536466112484\n",
      "Iteration 200, Training Loss: 0.733652595448363\n",
      "Iteration 201, Training Loss: 0.7336485812266446\n",
      "Iteration 202, Training Loss: 0.7336492487671866\n",
      "Iteration 203, Training Loss: 0.733646365518157\n",
      "Iteration 204, Training Loss: 0.7336441030835767\n",
      "Iteration 205, Training Loss: 0.7336429286840904\n",
      "Iteration 206, Training Loss: 0.7336403114931934\n",
      "Iteration 207, Training Loss: 0.7336403050164164\n",
      "Iteration 208, Training Loss: 0.7336372798968722\n",
      "Iteration 209, Training Loss: 0.7336363760038351\n",
      "Iteration 210, Training Loss: 0.7336340255111876\n",
      "Iteration 211, Training Loss: 0.7336347665980862\n",
      "Iteration 212, Training Loss: 0.7336319571748922\n",
      "Iteration 213, Training Loss: 0.7336307790207767\n",
      "Iteration 214, Training Loss: 0.7336311515216458\n",
      "Iteration 215, Training Loss: 0.733627991301729\n",
      "Iteration 216, Training Loss: 0.7336289769019608\n",
      "Iteration 217, Training Loss: 0.7336259087424443\n",
      "Iteration 218, Training Loss: 0.7336259768993135\n",
      "Iteration 219, Training Loss: 0.7336240267500131\n",
      "Iteration 220, Training Loss: 0.7336231518041344\n",
      "Iteration 221, Training Loss: 0.7336223320647549\n",
      "Iteration 222, Training Loss: 0.7336220361303845\n",
      "Iteration 223, Training Loss: 0.7336202378272315\n",
      "Iteration 224, Training Loss: 0.7336195257624543\n",
      "Iteration 225, Training Loss: 0.7336198465838131\n",
      "Iteration 226, Training Loss: 0.7336176438074863\n",
      "Iteration 227, Training Loss: 0.7336185352528853\n",
      "Iteration 228, Training Loss: 0.7336163836539883\n",
      "Iteration 229, Training Loss: 0.7336168279267377\n",
      "Iteration 230, Training Loss: 0.7336152398998931\n",
      "Iteration 231, Training Loss: 0.733615235192001\n",
      "Iteration 232, Training Loss: 0.7336147179131652\n",
      "Iteration 233, Training Loss: 0.7336137189891566\n",
      "Iteration 234, Training Loss: 0.7336142742063241\n",
      "Iteration 235, Training Loss: 0.7336117934055307\n",
      "Iteration 236, Training Loss: 0.7336134271443242\n",
      "Iteration 237, Training Loss: 0.7336114781799361\n",
      "Iteration 238, Training Loss: 0.7336126149881074\n",
      "Iteration 239, Training Loss: 0.7336106994199844\n",
      "Iteration 240, Training Loss: 0.7336108691756102\n",
      "Iteration 241, Training Loss: 0.7336105293884112\n",
      "Iteration 242, Training Loss: 0.733610715399631\n",
      "Iteration 243, Training Loss: 0.7336088658838658\n",
      "Iteration 244, Training Loss: 0.7336096169420815\n",
      "Iteration 245, Training Loss: 0.7336083175141046\n",
      "Iteration 246, Training Loss: 0.7336095966430746\n",
      "Iteration 247, Training Loss: 0.7336087965788554\n",
      "Iteration 248, Training Loss: 0.7336075394091196\n",
      "Iteration 249, Training Loss: 0.7336093437997451\n",
      "Iteration 250, Training Loss: 0.7336070506200346\n",
      "Iteration 251, Training Loss: 0.733607371253555\n",
      "Iteration 252, Training Loss: 0.7336076669575379\n",
      "Iteration 253, Training Loss: 0.7336064550369272\n",
      "Iteration 254, Training Loss: 0.7336072967054642\n",
      "Iteration 255, Training Loss: 0.7336055915121169\n",
      "Iteration 256, Training Loss: 0.7336069780360568\n",
      "Iteration 257, Training Loss: 0.7336057932165072\n",
      "Iteration 258, Training Loss: 0.7336061457254692\n",
      "Iteration 259, Training Loss: 0.7336049859948667\n",
      "Iteration 260, Training Loss: 0.7336043906771377\n",
      "Iteration 261, Training Loss: 0.733606320781508\n",
      "Iteration 262, Training Loss: 0.7336041578350193\n",
      "Iteration 263, Training Loss: 0.7336055928168793\n",
      "Iteration 264, Training Loss: 0.7336034571455611\n",
      "Iteration 265, Training Loss: 0.7336049231129408\n",
      "Iteration 266, Training Loss: 0.7336038156281948\n",
      "Iteration 267, Training Loss: 0.7336042594712943\n",
      "Iteration 268, Training Loss: 0.7336036779439434\n",
      "Iteration 269, Training Loss: 0.733603625010029\n",
      "Iteration 270, Training Loss: 0.7336030718789399\n",
      "Iteration 271, Training Loss: 0.7336045656486794\n",
      "Iteration 272, Training Loss: 0.7336034640049058\n",
      "Iteration 273, Training Loss: 0.7336034277837882\n",
      "Iteration 274, Training Loss: 0.7336033727868904\n",
      "Iteration 275, Training Loss: 0.7336023420312956\n",
      "Iteration 276, Training Loss: 0.7336043506613525\n",
      "Iteration 277, Training Loss: 0.7336032611823593\n",
      "Iteration 278, Training Loss: 0.7336027230698585\n",
      "Iteration 279, Training Loss: 0.7336027121322299\n",
      "Iteration 280, Training Loss: 0.7336027003155895\n",
      "Iteration 281, Training Loss: 0.7336036955218812\n",
      "Iteration 282, Training Loss: 0.7336016320770887\n",
      "Iteration 283, Training Loss: 0.7336031657735576\n",
      "Iteration 284, Training Loss: 0.7336011294183062\n",
      "Iteration 285, Training Loss: 0.7336036949920323\n",
      "Iteration 286, Training Loss: 0.7336026319874087\n",
      "Iteration 287, Training Loss: 0.7336021191098198\n",
      "Iteration 288, Training Loss: 0.7336026279418251\n",
      "Iteration 289, Training Loss: 0.7336016180288945\n",
      "Iteration 290, Training Loss: 0.7336026558126764\n",
      "Iteration 291, Training Loss: 0.7336016387278479\n",
      "Iteration 292, Training Loss: 0.7336026709779917\n",
      "Iteration 293, Training Loss: 0.7336011528420575\n",
      "Iteration 294, Training Loss: 0.7336027058514226\n",
      "Early stopping at iteration 294\n",
      "Iteration 0, Training Loss: 1.0\n",
      "Iteration 1, Training Loss: 0.8876264674942123\n",
      "Iteration 2, Training Loss: 0.8203583740345015\n",
      "Iteration 3, Training Loss: 0.7813870191588618\n",
      "Iteration 4, Training Loss: 0.7600947517563279\n",
      "Iteration 5, Training Loss: 0.7485173249083731\n",
      "Iteration 6, Training Loss: 0.7422413588269464\n",
      "Iteration 7, Training Loss: 0.7387285520055711\n",
      "Iteration 8, Training Loss: 0.7366574835522116\n",
      "Iteration 9, Training Loss: 0.7354102861062403\n",
      "Iteration 10, Training Loss: 0.7347023168025664\n",
      "Iteration 11, Training Loss: 0.7342473413535417\n",
      "Iteration 12, Training Loss: 0.7339994055524867\n",
      "Iteration 13, Training Loss: 0.7338205380433451\n",
      "Iteration 14, Training Loss: 0.7337584750427744\n",
      "Iteration 15, Training Loss: 0.7336659417039111\n",
      "Iteration 16, Training Loss: 0.7336657150710222\n",
      "Iteration 17, Training Loss: 0.7336152850855182\n",
      "Iteration 18, Training Loss: 0.733640400247771\n",
      "Iteration 19, Training Loss: 0.7335900542474441\n",
      "Iteration 20, Training Loss: 0.7336201841658355\n",
      "Iteration 21, Training Loss: 0.7335929945471557\n",
      "Iteration 22, Training Loss: 0.7336219567922067\n",
      "Iteration 23, Training Loss: 0.7335790002390943\n",
      "Iteration 24, Training Loss: 0.7336135457248932\n",
      "Iteration 25, Training Loss: 0.7335890125369302\n",
      "Iteration 26, Training Loss: 0.7336195665203653\n",
      "Iteration 27, Training Loss: 0.7335775676396121\n",
      "Iteration 28, Training Loss: 0.7336126853829471\n",
      "Iteration 29, Training Loss: 0.733588496468405\n",
      "Iteration 30, Training Loss: 0.733619256741135\n",
      "Iteration 31, Training Loss: 0.7335773819747193\n",
      "Iteration 32, Training Loss: 0.7336125738826306\n",
      "Iteration 33, Training Loss: 0.7335884295868903\n",
      "Iteration 34, Training Loss: 0.7336242936481558\n",
      "Iteration 35, Training Loss: 0.7335803861579535\n",
      "Iteration 36, Training Loss: 0.7336143850394096\n",
      "Iteration 37, Training Loss: 0.7335895141645243\n",
      "Iteration 38, Training Loss: 0.7336198693768963\n",
      "Iteration 39, Training Loss: 0.7335777472416336\n",
      "Iteration 40, Training Loss: 0.7336127941581083\n",
      "Iteration 41, Training Loss: 0.7335885614793412\n",
      "Early stopping at iteration 41\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "svm = LinearSVM(metric=f_score)\n",
    "num_samples = 100000\n",
    "train_losses, val_losses = svm.train(x_train[:num_samples], y_train[:num_samples], verbose=True)\n",
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fcab370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0, 0.01)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm._lambda, svm.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a3f88f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.8322518214927646\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "y_pred = svm.predict(x_train[num_samples:2*num_samples], scores=True)\n",
    "print(\"AUC-ROC:\", auc_roc(y_pred, y_train[num_samples:2*num_samples]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1673b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([15063., 30484., 19165., 13745.,  8859.,  5855.,  3884.,  2077.,\n",
       "          770.,    98.]),\n",
       " array([0.23306723, 0.29769956, 0.36233189, 0.42696422, 0.49159655,\n",
       "        0.55622888, 0.62086121, 0.68549354, 0.75012587, 0.8147582 ,\n",
       "        0.87939053]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7db9a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.00, F1 Score: 0.1600\n",
      "Threshold: 0.01, F1 Score: 0.1600\n",
      "Threshold: 0.02, F1 Score: 0.1600\n",
      "Threshold: 0.03, F1 Score: 0.1600\n",
      "Threshold: 0.04, F1 Score: 0.1600\n",
      "Threshold: 0.05, F1 Score: 0.1600\n",
      "Threshold: 0.06, F1 Score: 0.1600\n",
      "Threshold: 0.07, F1 Score: 0.1600\n",
      "Threshold: 0.08, F1 Score: 0.1600\n",
      "Threshold: 0.09, F1 Score: 0.1600\n",
      "Threshold: 0.10, F1 Score: 0.1600\n",
      "Threshold: 0.11, F1 Score: 0.1600\n",
      "Threshold: 0.12, F1 Score: 0.1600\n",
      "Threshold: 0.13, F1 Score: 0.1600\n",
      "Threshold: 0.14, F1 Score: 0.1600\n",
      "Threshold: 0.15, F1 Score: 0.1600\n",
      "Threshold: 0.16, F1 Score: 0.1600\n",
      "Threshold: 0.17, F1 Score: 0.1600\n",
      "Threshold: 0.18, F1 Score: 0.1600\n",
      "Threshold: 0.19, F1 Score: 0.1600\n",
      "Threshold: 0.20, F1 Score: 0.1600\n",
      "Threshold: 0.21, F1 Score: 0.1600\n",
      "Threshold: 0.22, F1 Score: 0.1600\n",
      "Threshold: 0.23, F1 Score: 0.1601\n",
      "Threshold: 0.24, F1 Score: 0.1602\n",
      "Threshold: 0.25, F1 Score: 0.1609\n",
      "Threshold: 0.26, F1 Score: 0.1634\n",
      "Threshold: 0.27, F1 Score: 0.1689\n",
      "Threshold: 0.28, F1 Score: 0.1777\n",
      "Threshold: 0.29, F1 Score: 0.1880\n",
      "Threshold: 0.30, F1 Score: 0.1991\n",
      "Threshold: 0.31, F1 Score: 0.2102\n",
      "Threshold: 0.32, F1 Score: 0.2211\n",
      "Threshold: 0.33, F1 Score: 0.2310\n",
      "Threshold: 0.34, F1 Score: 0.2413\n",
      "Threshold: 0.35, F1 Score: 0.2515\n",
      "Threshold: 0.36, F1 Score: 0.2617\n",
      "Threshold: 0.37, F1 Score: 0.2712\n",
      "Threshold: 0.38, F1 Score: 0.2802\n",
      "Threshold: 0.39, F1 Score: 0.2895\n",
      "Threshold: 0.40, F1 Score: 0.2986\n",
      "Threshold: 0.41, F1 Score: 0.3083\n",
      "Threshold: 0.42, F1 Score: 0.3178\n",
      "Threshold: 0.43, F1 Score: 0.3257\n",
      "Threshold: 0.44, F1 Score: 0.3346\n",
      "Threshold: 0.45, F1 Score: 0.3407\n",
      "Threshold: 0.46, F1 Score: 0.3469\n",
      "Threshold: 0.47, F1 Score: 0.3529\n",
      "Threshold: 0.48, F1 Score: 0.3576\n",
      "Threshold: 0.49, F1 Score: 0.3628\n",
      "Threshold: 0.50, F1 Score: 0.3662\n",
      "Threshold: 0.51, F1 Score: 0.3683\n",
      "Threshold: 0.52, F1 Score: 0.3697\n",
      "Threshold: 0.53, F1 Score: 0.3724\n",
      "Threshold: 0.54, F1 Score: 0.3736\n",
      "Threshold: 0.55, F1 Score: 0.3732\n",
      "Threshold: 0.56, F1 Score: 0.3721\n",
      "Threshold: 0.57, F1 Score: 0.3687\n",
      "Threshold: 0.58, F1 Score: 0.3651\n",
      "Threshold: 0.59, F1 Score: 0.3614\n",
      "Threshold: 0.60, F1 Score: 0.3564\n",
      "Threshold: 0.61, F1 Score: 0.3476\n",
      "Threshold: 0.62, F1 Score: 0.3379\n",
      "Threshold: 0.63, F1 Score: 0.3268\n",
      "Threshold: 0.64, F1 Score: 0.3143\n",
      "Threshold: 0.65, F1 Score: 0.3036\n",
      "Threshold: 0.66, F1 Score: 0.2914\n",
      "Threshold: 0.67, F1 Score: 0.2777\n",
      "Threshold: 0.68, F1 Score: 0.2629\n",
      "Threshold: 0.69, F1 Score: 0.2481\n",
      "Threshold: 0.70, F1 Score: 0.2299\n",
      "Threshold: 0.71, F1 Score: 0.2100\n",
      "Threshold: 0.72, F1 Score: 0.1912\n",
      "Threshold: 0.73, F1 Score: 0.1734\n",
      "Threshold: 0.74, F1 Score: 0.1513\n",
      "Threshold: 0.75, F1 Score: 0.1323\n",
      "Threshold: 0.76, F1 Score: 0.1143\n",
      "Threshold: 0.77, F1 Score: 0.0981\n",
      "Threshold: 0.78, F1 Score: 0.0773\n",
      "Threshold: 0.79, F1 Score: 0.0622\n",
      "Threshold: 0.80, F1 Score: 0.0481\n",
      "Threshold: 0.81, F1 Score: 0.0365\n",
      "Threshold: 0.82, F1 Score: 0.0273\n",
      "Threshold: 0.83, F1 Score: 0.0177\n",
      "Threshold: 0.84, F1 Score: 0.0123\n",
      "Threshold: 0.85, F1 Score: 0.0069\n",
      "Threshold: 0.86, F1 Score: 0.0039\n",
      "Threshold: 0.87, F1 Score: 0.0016\n",
      "Threshold: 0.88, F1 Score: 0.0007\n",
      "Threshold: 0.89, F1 Score: 0.0002\n",
      "Threshold: 0.90, F1 Score: 0.0000\n",
      "Threshold: 0.91, F1 Score: 0.0000\n",
      "Threshold: 0.92, F1 Score: 0.0000\n",
      "Threshold: 0.93, F1 Score: 0.0000\n",
      "Threshold: 0.94, F1 Score: 0.0000\n",
      "Threshold: 0.95, F1 Score: 0.0000\n",
      "Threshold: 0.96, F1 Score: 0.0000\n",
      "Threshold: 0.97, F1 Score: 0.0000\n",
      "Threshold: 0.98, F1 Score: 0.0000\n",
      "Threshold: 0.99, F1 Score: 0.0000\n",
      "Threshold: 1.00, F1 Score: 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHFCAYAAAAOmtghAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABaQklEQVR4nO3deVhUZf8G8HtgGIZ1EJBVBFxQ1NzABc3MXTEtW9Q0dyvTcnvLMn9vastrq5mVmvvyupVb9mYWuW+lIi6puSKLggjIIussz+8PZHIElMGZOcxwf65rLpznLPOdg3nunvM858iEEAJERERENsJO6gKIiIiITInhhoiIiGwKww0RERHZFIYbIiIisikMN0RERGRTGG6IiIjIpjDcEBERkU1huCEiIiKbwnBDRERENoXhhqgKVq5cCZlMpn/J5XL4+/tj8ODBuHTpUrnbqNVqLFy4EFFRUVCpVHByckJ4eDjeeecdZGRklLuNTqfDmjVr0L17d3h7e8PBwQE+Pj546qmn8NNPP0Gn0z201qKiInzzzTd4/PHHUatWLSgUCgQGBmLgwIHYt2/fIx0HqY0ePRq9e/fWv7927Zr+d7Jhw4Yy68+aNQsymQzp6emWLFMvJCTE4O+Nq6sr2rVrh9WrV0tST2XJZDLMmjXL6O2WLVuGwMBA5OXlmb4oogdguCF6BCtWrMCRI0fw+++/4/XXX8f27dvx+OOP4/bt2wbr5efno0ePHnjjjTfQqlUrrF+/Hjt27MCwYcOwePFitGrVChcuXDDYprCwENHR0RgxYgR8fHywcOFC7N69G4sWLUJAQABeeOEF/PTTTw+sLz09HR07dsTUqVPRrFkzrFy5Ert27cIXX3wBe3t7dOvWDadOnTL5cbGEuLg4rFq1Ch9++GG5y2fMmAG1Wm3hqh6uY8eOOHLkCI4cOaIPySNGjMDChQulLs3kRowYARcXF3z66adSl0I1jSAio61YsUIAEMeOHTNonz17tgAgli9fbtD+yiuvCABiw4YNZfZ14cIFoVKpRNOmTYVGo9G3v/baawKAWLVqVbk1XLx4UZw6deqBdfbp00fI5XKxa9eucpcfPXpUJCQkPHAflZWfn2+S/VTWwIEDRfv27Q3a4uPjBQDRp08fAUDMnz/fYPnMmTMFAHHr1i1LlqoXHBws+vbta9B2+/Zt4e7uLho0aCBJTZUBQMycObNK237++edCpVKJvLw80xZF9ADsuSEyocjISADAzZs39W2pqalYvnw5evXqhUGDBpXZJiwsDG+//TbOnj2Lbdu26bdZunQpevXqheHDh5f7WQ0bNkTz5s0rrCU2Nha//PILxowZg65du5a7Tps2bVC3bl0A/1yyuV9p78K1a9f0bSEhIXjqqaewZcsWtGrVCkqlErNnz0arVq3QqVOnMvvQarUIDAzEs88+q28rLi7Ghx9+iMaNG8PR0RG1a9fGqFGjcOvWrQq/U6mbN29i69atGDZsWLnLu3btil69euGDDz5Abm7uA/cVEhKCkSNHlml/8skn8eSTT+rf7927FzKZDOvWrcPbb78Nf39/uLq6ol+/frh58yZyc3PxyiuvwNvbG97e3hg1ahTu3Lnz0O/i4eGBRo0aISEhAQAwZswYeHp6Ij8/v9zv1bRp04fuc/ny5WjRogWUSiU8PT0xYMAAnD9/3mCdkSNHwtXVFZcvX0Z0dDRcXV0RFBSEf/3rXygqKqpw39euXYNcLsecOXPKLNu/fz9kMhl++OEHfdvQoUORk5NT7mVCInNhuCEyofj4eAAlgaXUnj17oNFo8Mwzz1S4XemymJgY/TZqtfqB2zzMb7/9ZrBvUztx4gTeeustTJw4ETt37sRzzz2HUaNG4eDBg2XGHf3222+4ceMGRo0aBaBkLNHTTz+Njz/+GEOGDMHPP/+Mjz/+GDExMXjyySdRUFDw0O+mVqvRpUuXCtf55JNPkJ6ejs8+++zRv+w93n33XaSlpWHlypX44osvsHfvXrz44ot47rnnoFKpsH79ekybNg1r1qzBu++++9D9qdVqJCQkoHbt2gCASZMm4fbt21i3bp3BeufOncOePXswYcKEB+5vzpw5GDNmDJo2bYotW7bgq6++wunTpxEVFVXm96JWq9G/f39069YNP/74I0aPHo0vv/wSn3zySYX7DwkJQf/+/bFo0SJotVqDZd988w0CAgIwYMAAfZufnx8aN26Mn3/++aHHgshkpO46IrJGpZel/vjjD6FWq0Vubq7YuXOn8PPzE0888YRQq9X6dT/++GMBQOzcubPC/RUUFOgvp1R2m4cZN26cACD+/vvvSq1fesnmfqXfNT4+Xt8WHBws7O3txYULFwzWTU9PFwqFQrz77rsG7QMHDhS+vr7647J+/XoBQGzevNlgvWPHjgkAYsGCBQ+s9bXXXhNOTk5Cp9MZtJdelvrss8+EEEIMHTpUuLi4iJSUFIPveO9lqeDgYDFixIgyn9G5c2fRuXNn/fs9e/YIAKJfv34G602ePFkAEBMnTjRof+aZZ4Snp6dBW3BwsIiOjhZqtVqo1WoRHx8vRowYIQCIt956y+CzW7ZsWeY7u7u7i9zc3AqPy+3bt4WTk5OIjo42aE9MTBSOjo5iyJAh+rbSz/3+++8N1o2OjhaNGjUyaMN9l6VKj8XWrVv1bdevXxdyuVzMnj27TF1Dhw4Vvr6+FdZNZGrsuSF6BO3bt4eDgwPc3NzQu3dv1KpVCz/++CPkcnmV9lfeZaHqqnnz5gY9VADg5eWFfv36YdWqVfqZXLdv38aPP/6I4cOH64/L//73P3h4eKBfv37QaDT6V8uWLeHn54e9e/c+8LNv3LiB2rVrP/R4ffjhh1Cr1Zg9e3bVv+h9nnrqKYP34eHhAIC+ffuWac/MzCxzaWrHjh1wcHCAg4MDQkND8f333+ONN94wGBg9adIknDx5EocOHQIA5OTkYM2aNRgxYgRcXV0rrO3IkSMoKCgoc5ktKCgIXbt2xa5duwzaZTIZ+vXrZ9DWvHlz/SWyijz55JNo0aIFvv32W33bokWLIJPJ8Morr5RZ38fHB2lpadBoNA/cL5GpMNwQPYLVq1fj2LFj2L17N1599VWcP38eL774osE6pWNaSi9Zlad0WVBQUKW3eRhT7ONB/P39y20fPXo0rl+/rr/Etn79ehQVFRmccG/evImsrCwoFAr9ib70lZqa+tCp2gUFBVAqlQ+tMSQkBOPHj8fSpUsrnKJvLE9PT4P3CoXige2FhYUG7Y8//jiOHTuG48eP49y5c8jKysL8+fP16wPA008/jZCQEH14WLlyJfLy8h56Sar0lgLl/W4CAgLK3HLA2dm5zHF0dHQsU3N5Jk6ciF27duHChQtQq9VYsmQJnn/+efj5+ZVZV6lUQghRqf0SmQLDDdEjCA8PR2RkJLp06YJFixZh7Nix2LlzJzZt2qRfp0uXLpDL5frBwuUpXdajRw/9Ng4ODg/c5mF69eplsO+HKT3J3T+YtKKgUVGvSa9evRAQEIAVK1YAKJku365dOzRp0kS/jre3N7y8vHDs2LFyXwsWLHhgrd7e3sjMzKzU9/q///s/ODs7Vzj+RalUljuA1lz3wlGpVIiMjERERATCw8MNQk0pOzs7TJgwAZs2bUJKSgoWLFiAbt26oVGjRg/ct5eXFwAgJSWlzLIbN27A29vbNF8CwJAhQ+Dl5YVvv/0WP/zwA1JTUysMX5mZmXB0dHxgrxORKTHcEJnQp59+ilq1auG9997TX5bx8/PD6NGj8euvv2Ljxo1ltrl48SI++eQTNG3aVD/418/PD2PHjsWvv/5a4Q3erly5gtOnT1dYS+vWrdGnTx8sW7YMu3fvLned48ePIzExEUBJLweAMvt82L107mdvb49hw4Zh27ZtOHDgAI4fP47Ro0cbrPPUU08hIyMDWq0WkZGRZV4PO4k3btwYGRkZyM7Ofmg9Xl5eePvtt7Fp0yYcPXq0zPKQkJAy3/nixYtl7jtkaWPHjoVCocDQoUNx4cIFvP766w/dJioqCk5OTvjvf/9r0J6cnIzdu3ejW7duJqtPqVTilVdewapVqzB37ly0bNkSHTt2LHfdq1evGoRbInNjuCEyoVq1amH69Ok4f/68wWyXuXPnonPnznjppZcwYcIE7Ny5E3v27MGcOXMQFRUFNzc3bN68Gfb29gbb9OrVCyNHjsTQoUOxadMmHDhwAFu3bsX48ePRrFmzh15yWr16NVq0aIE+ffrgtddew/bt23HgwAF8//33GDZsGNq3b6+/4WB0dDQ8PT0xZswYbNu2Df/73//w/PPPIykpyejjMHr0aBQVFWHIkCFwcnIqMwV+8ODB6NOnD6Kjo/H+++9j586d2LVrF1atWoWRI0di69atD9z/k08+CSEE/vzzz0rVM3nyZAQEBOCXX34ps2zYsGE4d+4cxo8fj127dmH58uXo37+/fvaSVDw8PDB8+HDs2bMHwcHBZcbGVLTNv//9b2zfvh3Dhw/HL7/8gv/+97/o0qULlEolZs6cadIax48fj/z8fMTGxlYYvnQ6HY4ePfrAmW1EJif1iGYia1TRTfyEKJn5VLduXdGwYUODm/IVFxeLb7/9VrRr1064uroKR0dH0ahRIzFt2jSRnp5e7udoNBqxatUq0bVrV+Hp6SnkcrmoXbu26NOnj1i3bp3QarUPrbWgoEDMnz9fREVFCXd3dyGXy0VAQIB49tlnxc8//2yw7tGjR0WHDh2Ei4uLCAwMFDNnzhRLly4td7bU/Teju1+HDh0EADF06NByl6vVavH555+LFi1aCKVSKVxdXUXjxo3Fq6++Ki5duvTAfWu1WhESEiLGjx9v0H7/bKl7LV68WAAoM1tKp9OJTz/9VNSrV08olUoRGRkpdu/eXeFsqR9++MFgvxX9XahoZtbDjtu99u7dKwCIjz/+uNLbCCHE0qVLRfPmzYVCoRAqlUo8/fTT4uzZswbrjBgxQri4uJTZtrxZc3jATfyefPJJ4enpWeFNHHft2iUAiNjYWKO+A9GjkAkhhCSpiojoEXzxxRf46KOPcP36dTg5OUldjln861//wsKFC5GUlKQfT1OdpKWlITg4GG+88UaFj1gYNmwYrl69qp/5RWQJvCxFRFZpwoQJUKlUBtORbcUff/yB1atXY8GCBXjllVeqXbBJTk7G/v37MWbMGNjZ2WHSpEnlrnflyhVs3LjxgTcFJDKHqt2Mg4hIYkqlEmvWrEFcXJzUpZhcVFQUnJ2d8dRTT1X4YFApLV26FO+//z5CQkKwdu1aBAYGlrteYmKi/on0RJbEy1JERERkU3hZioiIiGwKww0RERHZFIYbIiIisik1bkCxTqfDjRs34ObmZlUPKSQiIqrJhBDIzc1FQEAA7Owe3DdT48LNjRs39A8nJCIiIuuSlJSEOnXqPHCdGhdu3NzcAJQcHHd3d4mrISIiosrIyclBUFCQ/jz+IDUu3JReinJ3d2e4ISIisjKVGVLCAcVERERkUxhuiIiIyKYw3BAREZFNYbghIiIim8JwQ0RERDaF4YaIiIhsCsMNERER2RSGGyIiIrIpDDdERERkUxhuiIiIyKZIGm7279+Pfv36ISAgADKZDNu2bXvoNvv27UNERASUSiXq1auHRYsWmb9QIiIishqShpu8vDy0aNEC33zzTaXWj4+PR3R0NDp16oS4uDi8++67mDhxIjZv3mzmSomIiMhaSPrgzD59+qBPnz6VXn/RokWoW7cu5s2bBwAIDw/H8ePH8fnnn+O5554zU5VEREQ1m04noBMCOoG7PwW0upL34u6ftaU/dQL2djL4q5wkq9eqngp+5MgR9OzZ06CtV69eWLZsGdRqNRwcHMpsU1RUhKKiIv37nJwcs9dJRET0MBqtDmqtQLFGh2JtyUut0UGt1aHo7s9izd11tFoUa4R+nWLtP8tL2krWMdjf3X2o736ORquDRleyXKMT+s/X6HTQaAXUpT/vtqk1OqjvrqcTxn03HzdHHJ3R3TwHrhKsKtykpqbC19fXoM3X1xcajQbp6enw9/cvs82cOXMwe/ZsS5VIRERWoFCtRW6hBjmFauQWapB792eRRns3HAh90FBrS94bhgWdQdhQaw0Dh1orDNrU5QQTYwODNXCwl0Emk8HRQdr5SlYVbgBAJpMZvBdClNteavr06Zg6dar+fU5ODoKCgsxXIBERmZVaqzMIJDmFauQUGL6/f3nJ+5K2nAINirU6qb9GGQq5HRT2dlDI7eBgL7v78962kj876NeTlby/d/ndZQ737UduZwe5vQwO9jLI7UqWO9jLILe3g4NdyU+5vQwO968nL1luf/dlZyeDnUwGOxnu/rznz3bln4elYFXhxs/PD6mpqQZtaWlpkMvl8PLyKncbR0dHODo6WqI8IiKqpEK1FtkFamTlq+/+LEZ2gVr/Km3PLlDrw0lOQcnPArXWJDXIZICroxzuSge4KUt+KhX2UNjL7p78S0PCPyHCQR8yZIahwt4ODvJ/trs3kDjcv+7d0OFgdzeMyO0gt5NV+D/pZDyrCjdRUVH46aefDNp+++03REZGljvehoiIzEerE8gpUCOroJyAkn9vuxrZBcUGoaVI8+g9J84Ke30wKXk5wN3pn/fuSge469tLfpa2uynlcFHIq1VvA5mOpOHmzp07uHz5sv59fHw8Tp48CU9PT9StWxfTp0/H9evXsXr1agDAuHHj8M0332Dq1Kl4+eWXceTIESxbtgzr16+X6isQEVm9Io3WIJBk5f8TVLLy1cgqKDboSSldnlOoeaTPtZMBHs4KqJwc9C8P57s/nUqCiuruz3t7V9yd5HB1lENuz/vQUvkkDTfHjx9Hly5d9O9Lx8aMGDECK1euREpKChITE/XLQ0NDsWPHDkyZMgXffvstAgICMH/+fE4DJyIqR3aBGldv3UF8eh7i0/OQfqfobjC526uSX4ysAjXyix/tMo+ro7xMQPFwLgklHk4Kg9By7zqujnJeiiGzkInSEbk1RE5ODlQqFbKzs+Hu7i51OUREj6RIo0ViRj6upufh6q08xKffufszDxl5xZXej0wGfY+JylkBj9KQcv97ZweonBT6Ze5ODnBgDwpZgDHnb6sac0NEVBPpdAKpOYWIT8/D1Vt37gkyeUi+nf/AKcW+7o4I9XZBqLcr/FXKfy77GAQYBdyUHH9CtoPhhoioGihUa5F8uwBJt/ORfLsAyZn5SLqdj/j0fFxLz3vgDCFXRznq1Xa5G2JcUK+2K+p5uyDE2wWujvxnnmoe/q0nIrIAtVaHlKxCJN3OR9Ld4JJ8u+DunwtwK7fogdvL7WSo6+WMenfDS6i3C+p5uyC0tgtquzpy7ArRPRhuiIhMpFCtxdVbebiUlov49DwkZRYg+W6ISckueOgdaV0d5ahTywl1ajkjyLPkZ6i3M0K9XRFUy4mzg4gqieGGiMhIeUUaXLl1B5du3sGltDu4nJaLy2l3kJj54PEvjnI71KnlhCBP55KftZwN/uzh7MAeGCITYLghIqpAdr4al2/l3hNiSl7Xswoq3Ebl5ICGPq6oV9sFdT1Lw4szgmo5wdvVkYN2iSyA4YaIajwhSmYjnUrKxunkLJy5no0LqblIe8A4GG9XRzT0cUUDH1c09C352cDHleNfiKoBhhsiqnFu5xXjVHIWTieXhJlTydkVDugNUCnRwNcNDWqXhJjSQOPhrLBw1URUWQw3RGTT7hRp8Nf1f0LM6eQsJGWWvaxkbydDmK8bWtRRoXkdDzQJcEf92i5wU/K5dUTWhuGGiGzK7bxi7LmQhkOXM3AqOQtXbt1Befdhr+ftguZ3g0yLIBWa+KvgpLC3fMFEZHIMN0Rk1YQQuHIrD7vO38Su82k4npBZZsZSgEqJ5nU80DxIhRZ1PNAsUAWVE3tkiGwVww0RWR21Vofj127j9/M3sev8TVzLyDdY3tjPDV0a+yAyuBaa1/FAbTdHiSolIikw3BCRVcguUGPvhTTsOp+GvRfSkFOo0S9zsJehfT0vdA/3RdfGPgjydJawUiKSGsMNEVVbSZn5+PVsKnadT8Oxa5nQ3HO9qZazA7o09kH3cF90aujNgb9EpMdwQ0TVSna+Gv87cwNbT1zH8YTbBssa+LiiW3hJoGldtxbseUM8IioHww0RSa5Yo8OeC2nYeuI6dv+dhmKtDgAgkwHtQj3Ro4kfuof7INjLReJKicgaMNwQkSSEEDiReBtbTlzHz2dSkJWv1i9r7OeGAa0C8XTLQPiplBJWSUTWiOGGiCzqWnoetsZdx7aT15FwzywnX3dHPN0yEANaBSLc313CConI2jHcEJHZ5Raqse3kDWw5kYy4xCx9u7PCHr2b+eHZVnUQVd+LY2iIyCQYbojIbM6n5GDNHwnYFncd+cVaAICdDOjUsDYGtApEz6a+cFbwnyEiMi3+q0JEJlWk0WLnX6lYcyTBYLZTQx9XDGoThP4tA+DjxnE0RGQ+DDdEZBLJt/Ox7s9EbDyWhIy8YgCA3E6GXs38MKx9MNqFekIm42UnIjI/hhsiqjKdTmDfpVtY+0cCdv2dpn9ApZ+7EkPa1cXgNkHwcWcvDRFZFsMNERktM68YPxxPwto/E5GY+c+Mp04NvTG0XTC6h/tAbm8nYYVEVJMx3BBRpcWn52Hx/qvYfCIZxZqSG+25K+V4ITIIQ9vVRb3arhJXSETEcENElXA6OQuL9l3BL3+l6i89NQt0x/D2IejXIgBOCntpCyQiugfDDRGVSwiBA5fSsWjfFRy+kqFv79rYB+M610ebkFocIExE1RLDDREZ0Gh12PFXKr7bdwVnb+QAKJn11L9FAF7tXB+N/NwkrpCI6MEYbogIAFCo1uKH40lYfOAqkjILAABODvYY3DYIYzvVQ6CHk8QVEhFVDsMNUQ2XlV+M1UcSsOrwNf39aTxdFBgRFYLhUcGo5aKQuEIiIuMw3BDVUJl5xVh28CpWHU7AnSINAKBOLSe83KkeBkYGcZAwEVkthhuiGib9ThGWHLiKNUcS9M97auznhteerI++j/nz/jREZPUYbohqiLTcQizZfxX//SMRBeqSUNM0wB0TuzVEj3Bf2PGJ3ERkIxhuiGzczZxCLNp3Bev+TETR3RvvNa+jwqRuDdG1sQ+ncxORzWG4IbJRKdkFWLj3CjYcS9LfTbhVXQ9M6tYQncNqM9QQkc1iuCGyMcm387Fw7xX8cDwZxdqSUBMZXAuTujfE4w28GWqIyOYx3BDZiLwiDb7ZcxnLDsTrQ027UE9M6t4QUfW8GGqIqMZguCGycjqdwLaT1/HxL38jLbcIANC+nicmdw9D+3peEldHRGR5DDdEVuxUUhZm/XQWcYlZAIBgL2f8X98m6B7OgcJEVHMx3BBZobTcQny68wI2xSYDAJwV9ni9awOMeTwUjnLefI+IajaGGyIrUqzRYcWheHy9+7L+rsLPtg7E270bw9ddKXF1RETVA8MNkRUQQmD332n48OfziE/PAwC0qKPCzP5N0bpuLYmrIyKqXhhuiKq5K7fu4P2fzmHfxVsAAG9XR7zduxGea12HdxUmIioHww1RNSWEwH//SMAHP59HsUYHB3sZRj8eite7NICb0kHq8oiIqi2GG6JqKCu/GG9vPo1fz94EADwRVhuz+zdFqLeLxJUREVV/DDdE1czxa5mYuD4ON7IL4WAvwzt9wjG6YwindhMRVRLDDVE1odUJLNx7GV/+fglanUCIlzO+frE1Hqujkro0IiKrwnBDVA2k5RRi8saTOHwlAwDwTMsAfDjgMbg68j9RIiJj8V9OIontuZCGN78/hYy8Yjg52OODZ5rhudaBvAxFRFRFDDdEEinW6PD5bxeweP9VAEC4vzu+GdIK9Wu7SlwZEZF1Y7ghkkBiRj7eWH8Cp5KzAQAjooIxPTocSgc+OoGI6FEx3BBZ2E+nbuDdLWeQW6SByskBnz7fHL2a+kldFhGRzWC4IbKQgmItZv90FhuOJQEAIoNr4asXWyHQw0niyoiIbAvDDZEF/J2ag9fXxeFy2h3IZMAbXRpgYreGkNvbSV0aEZHNYbghMiMhBNb+mYgP/ncORRodfNwcMW9wS3So7y11aURENovhhshMsgvUeGfzafzyVyoAoEuj2vj8hRbwcnWUuDIiItsmeZ/4ggULEBoaCqVSiYiICBw4cOCB669duxYtWrSAs7Mz/P39MWrUKGRkZFioWqLKiU24jeivDuCXv1LhYC/D//UNx7IRbRhsiIgsQNJws3HjRkyePBkzZsxAXFwcOnXqhD59+iAxMbHc9Q8ePIjhw4djzJgxOHv2LH744QccO3YMY8eOtXDlROXT6QQW7L2Mgd8dwfWsAgR7OWPzax0wtlM92NnxpnxERJYgE0IIqT68Xbt2aN26NRYuXKhvCw8PxzPPPIM5c+aUWf/zzz/HwoULceXKFX3b119/jU8//RRJSUmV+sycnByoVCpkZ2fD3d390b8E0V23coswZeNJHLycDgDo3yIAHw1oBjelg8SVERFZP2PO35L13BQXFyM2NhY9e/Y0aO/ZsycOHz5c7jYdOnRAcnIyduzYASEEbt68iU2bNqFv374Vfk5RURFycnIMXkSmdj2rAM8vOoyDl9Ph5GCPT59vjq8Gt2SwISKSgGThJj09HVqtFr6+vgbtvr6+SE1NLXebDh06YO3atRg0aBAUCgX8/Pzg4eGBr7/+usLPmTNnDlQqlf4VFBRk0u9BlJSZj0HfHUFCRj6CPJ3w0xsdMTAyiM+GIiKSiOQDiu8/AQghKjwpnDt3DhMnTsR7772H2NhY7Ny5E/Hx8Rg3blyF+58+fTqys7P1r8peviKqjGvpeRj03REk3y5AqLcLvn81Cg183KQui4ioRpNsKri3tzfs7e3L9NKkpaWV6c0pNWfOHHTs2BFvvfUWAKB58+ZwcXFBp06d8OGHH8Lf37/MNo6OjnB05AwVMr0rt+5gyJI/cDOnCPVru2D9y+3h466UuiwiohpPsp4bhUKBiIgIxMTEGLTHxMSgQ4cO5W6Tn58POzvDku3tSx40KOG4aKqBLt7MxaDvSoJNI183bHglisGGiKiakPQmflOnTsWwYcMQGRmJqKgoLF68GImJifrLTNOnT8f169exevVqAEC/fv3w8ssvY+HChejVqxdSUlIwefJktG3bFgEBAVJ+FapBzt3IwUvL/kRmXjGa+Lvjv2PbwdNFIXVZRER0l6ThZtCgQcjIyMD777+PlJQUNGvWDDt27EBwcDAAICUlxeCeNyNHjkRubi6++eYb/Otf/4KHhwe6du2KTz75RKqvQDXMX9ez8dKyP5GVr0bzOiqsHt0WHs4MNkRE1Ymk97mRAu9zQ1V1MikLw5f9iZxCDVrV9cCq0W3hzqneREQWYcz5m8+WIqqE49cyMXLFMdwp0qBNSC2sGNUWro78z4eIqDriv85ED3HsWiZGLD+K/GItoup5YdnISDgr+J8OEVF1xX+hiR7g+LVMjLwbbDo19MbiYZFwUthLXRYRET0Aww1RBY7f7bHJuxtslgyPhNKBwYaIqLqT/A7FRNVRbMI/webxBgw2RETWhOGG6D6xCbcxYvkx5BVr0aG+F4MNEZGVYbghuseJxNsYsfwo7hRpSgYPj2jDMTZERFaG4YborrjE2xixrCTYtK/niWUjOXiYiMgaMdwQofQGfUeRW6RBu1BPLB/ZhtO9iYisFMMN1XinkrIwbNmfyC3SoG2oJ1aMYrAhIrJmDDdUo5U+BDO3UIO2IZ5YwR4bIiKrx3BDNVZiRj6GLz+K3EINIoNrYcWoNnDhIxWIiKweww3VSGm5hXhp2Z9Iv1OEcH93LBvJYENEZCsYbqjGySlUY8TyY0jMzEddT2esGt0GKic+3ZuIyFYw3FCNUqjWYuyq4zifkgNvV0esGdMWPm5KqcsiIiITYrihGkOj1eGN9XE4Gp8JN0c5Vo1ug2AvF6nLIiIiE2O4oRpBCIF3t55BzLmbUMjtsGREJJoGqKQui4iIzIDhhmqET3ZewPfHk2EnA75+sRXa1/OSuiQiIjIThhuyeUv2X8WifVcAAB8/2xy9mvpJXBEREZkTww3ZtE2xyfhox3kAwNu9G2NgmyCJKyIiInNjuCGbdehyOt7efBoA8HKnUIzrXE/iioiIyBIYbsgmxafnYfzaE9DqBJ5uGYDpfcIhk8mkLouIiCyA4YZsTnaBGmNWHUN2gRqt6nrgk+eaw86OwYaIqKZguCGbotHq8Pq6E7h6Kw8BKiW+GxYBpYO91GUREZEFMdyQTfngf+dw4FI6nBzssWREJO8+TERUAzHckM347x8JWHUkAQDw5aCWvEkfEVENxXBDNuHw5XTM3H4WAPBWr0bo3Yz3siEiqqkYbsjqxafn4bW7M6OeaRmA8U/Wl7okIiKSEMMNWbV7Z0a1DPLAx88155RvIqIajuGGrNb9M6MWD+fMKCIiYrghK3bvzKjFwzkzioiISjDckFVac+SawcyoZoGcGUVERCUYbsjqHLh0C7N+OgcAmNabM6OIiMhQlcKNRqPB77//ju+++w65ubkAgBs3buDOnTsmLY7ofpfT7uifGfVs60C81pkzo4iIyJDc2A0SEhLQu3dvJCYmoqioCD169ICbmxs+/fRTFBYWYtGiReaokwi384oxZtUx5BZqEBlcC3OefYwzo4iIqAyje24mTZqEyMhI3L59G05OTvr2AQMGYNeuXSYtjqhUsUaHcf+NRUJGPurUcsJ3wyLgKOfMKCIiKsvonpuDBw/i0KFDUCgUBu3BwcG4fv26yQojKiWEwHs//oU/4zPh6ijHshFt4OXqKHVZRERUTRndc6PT6aDVasu0Jycnw83NzSRFEd1r2cF4bDiWBDsZ8PWLrdDIj3/PiIioYkaHmx49emDevHn69zKZDHfu3MHMmTMRHR1tytqIsOfvNHy04zwAYEbfJujS2EfiioiIqLoz+rLUl19+iS5duqBJkyYoLCzEkCFDcOnSJXh7e2P9+vXmqJFqqJTsAkz5/iSEAF5sG4TRHUOkLomIiKyA0eEmICAAJ0+exIYNGxAbGwudTocxY8Zg6NChBgOMiR6FVicwecNJZOWr0SzQHbP6N+XMKCIiqhSZEEIYs8H+/fvRoUMHyOWGuUij0eDw4cN44oknTFqgqeXk5EClUiE7Oxvu7u5Sl0MV+Or3S/jy94twUdjjfxM7IdTbReqSiIhIQsacv40ec9OlSxdkZmaWac/OzkaXLl2M3R1RGceuZeKrXRcBAB8804zBhoiIjGJ0uBFClHt5ICMjAy4uPAnRo8nKL8ak9XHQCeDZVoF4tnUdqUsiIiIrU+kxN88++yyAktlRI0eOhKPjP/cZ0Wq1OH36NDp06GD6CqnGEELg7c2ncSO7ECFeznj/mWZSl0RERFao0uFGpSp56rIQAm5ubgaDhxUKBdq3b4+XX37Z9BVSjfHfPxPx69mbcLCX4esXW8PV0ejx7kRERJUPNytWrAAAhISE4M033+QlKDKpv1Nz8MH/Sp70/XbvxnisjkriioiIyFoZ/b/GM2fONEcdVIMVqrV4Y10cijU6dGlUG2MeD5W6JCIismJV6vfftGkTvv/+eyQmJqK4uNhg2YkTJ0xSGNUcX/5+EZfS7qC2myM+f6EF72dDRESPxOjZUvPnz8eoUaPg4+ODuLg4tG3bFl5eXrh69Sr69OljjhrJhp1OzsKS/VcBAP8Z8BgfiElERI/M6HCzYMECLF68GN988w0UCgWmTZuGmJgYTJw4EdnZ2eaokWxUsUaHaZtOQyeA/i0C0KOJr9QlERGRDTA63CQmJuqnfDs5OSE3NxcAMGzYMD5bioyyYO9l/J2aC08XBWb2ayJ1OUREZCOMDjd+fn7IyMgAAAQHB+OPP/4AAMTHx8PIJzlQDXYhNRff7rkMAJjVvykvRxERkckYHW66du2Kn376CQAwZswYTJkyBT169MCgQYMwYMAAkxdItkej1WHaplNQawW6h/uiX3N/qUsiIiIbYvRsqcWLF0On0wEAxo0bB09PTxw8eBD9+vXDuHHjTF4g2Z5lB+NxKjkbbko5PhrQjLOjiIjIpIx+KviDXL9+HYGBgabanVnwqeDSunrrDvp8dQBFGh0+fa45BrYJkrokIiKyAmZ9Knh5UlNT8cYbb6BBgwZGb7tgwQKEhoZCqVQiIiICBw4ceOD6RUVFmDFjBoKDg+Ho6Ij69etj+fLlVS2dLEinE3hn8xkUaXTo1NAbL0TyoZhERGR6lQ43WVlZGDp0KGrXro2AgADMnz8fOp0O7733HurVq4c//vjD6JCxceNGTJ48GTNmzEBcXBw6deqEPn36IDExscJtBg4ciF27dmHZsmW4cOEC1q9fj8aNGxv1uSSNtUcTcfRaJpwV9vjPgMd4OYqIiMyi0pelxo8fj59++gmDBg3Czp07cf78efTq1QuFhYWYOXMmOnfubPSHt2vXDq1bt8bChQv1beHh4XjmmWcwZ86cMuvv3LkTgwcPxtWrV+Hp6Wn05wG8LCWVmzmF6P7FPuQWaTCrXxOM7MhHLBARUeWZ5bLUzz//jBUrVuDzzz/H9u3bIYRAWFgYdu/eXaVgU1xcjNjYWPTs2dOgvWfPnjh8+HC522zfvh2RkZH49NNPERgYiLCwMLz55psoKCio8HOKioqQk5Nj8CLLe/9/55BbpEGLOioMiwqRuhwiIrJhlZ4tdePGDTRpUnKjtXr16kGpVGLs2LFV/uD09HRotVr4+hreldbX1xepqanlbnP16lUcPHgQSqUSW7duRXp6OsaPH4/MzMwKL4nNmTMHs2fPrnKd9Oj2XkjDz6dTYCcDPhrwGOzteDmKiIjMp9I9NzqdDg4ODvr39vb2cHFxeeQC7h93IYSocCyGTqeDTCbD2rVr0bZtW0RHR2Pu3LlYuXJlhb0306dPR3Z2tv6VlJT0yDVT5RWqtXjvx7MAgJEdQtEsUCVxRUREZOsq3XMjhMDIkSPh6FhyJ9nCwkKMGzeuTMDZsmVLpfbn7e0Ne3v7Mr00aWlpZXpzSvn7+yMwMBAq1T8nyPDwcAghkJycjIYNG5bZxtHRUV8zWd43uy8jMTMf/iolpvYMk7ocIiKqASrdczNixAj4+PhApVJBpVLhpZdeQkBAgP596auyFAoFIiIiEBMTY9AeExOjf3bV/Tp27IgbN27gzp07+raLFy/Czs4OdepwWnF1czktF9/tvwIAmNmvKVwdjb5nJBERkdFMehM/Y23cuBHDhg3DokWLEBUVhcWLF2PJkiU4e/YsgoODMX36dFy/fh2rV68GANy5cwfh4eFo3749Zs+ejfT0dIwdOxadO3fGkiVLKvWZnC1lGUIIDFr8B47GZ6JbYx8sHRHJqd9ERFRlxpy/Jf1f6UGDBiEjIwPvv/8+UlJS0KxZM+zYsQPBwcEAgJSUFIN73ri6uiImJgZvvPEGIiMj4eXlhYEDB+LDDz+U6itQBTbFJuNofCacHOwx++mmDDZERGQxkvbcSIE9N+aXmVeMbl/sxe18Nab3aYxXO9eXuiQiIrJyFn/8AtG9Pvz5HG7nq9HYzw2jH+fN+oiIyLIYbsikfjubii0nruvvaeNgz79iRERkWTzzkMlk5hXj3a1nAAAvP1EPEcG1JK6IiIhqoiqFmzVr1qBjx44ICAhAQkICAGDevHn48ccfTVocWQ8hBP5v2xmk3ylGmK8rpnTnPW2IiEgaRoebhQsXYurUqYiOjkZWVha0Wi0AwMPDA/PmzTN1fWQlfjqdgh1nUiG3k+GLF1pC6WAvdUlERFRDGR1uvv76ayxZsgQzZsyAvf0/J7DIyEicOXPGpMWRdUjLKcS/t/0FAHi9awM8VoePWCAiIukYHW7i4+PRqlWrMu2Ojo7Iy8szSVFkPYQQmL7lDLIL1GgW6I4JXRpIXRIREdVwRoeb0NBQnDx5skz7L7/8on9qONUcP8QmY9ffaVDY22HuwJacHUVERJIz+g7Fb731FiZMmIDCwkIIIXD06FGsX78ec+bMwdKlS81RI1VTN3MK8cFP5wAAU3uGIczXTeKKiIiIqhBuRo0aBY1Gg2nTpiE/Px9DhgxBYGAgvvrqKwwePNgcNVI19eHP55FbpEHLIA+83Kme1OUQEREBeMTHL6Snp0On08HHx8eUNZkVH79gGocvp2PI0j9hJwO2v/44mgVyEDEREZmPWR+/MHv2bFy5cgUA4O3tbVXBhkyjWKPDv38smR01rH0wgw0REVUrRoebzZs3IywsDO3bt8c333yDW7dumaMuqsaWHYzHlVt58HZVYGrPRlKXQ0REZMDocHP69GmcPn0aXbt2xdy5cxEYGIjo6GisW7cO+fn55qiRqpEbWQWYv+sSAGB6n3ConBwkroiIiMhQlebtNm3aFP/5z39w9epV7NmzB6GhoZg8eTL8/PxMXR9VMx/87xwK1Fq0CamFZ1sHSl0OERFRGY98UxIXFxc4OTlBoVBArVaboiaqpvZdvIVf/kqFvZ0M7z/dDDKZTOqSiIiIyqhSuImPj8dHH32EJk2aIDIyEidOnMCsWbOQmppq6vqomijSaDFr+1kAwIioEIT7c6YZERFVT0bf5yYqKgpHjx7FY489hlGjRunvc0O2bc2RBMSn56G2myOm9GgodTlEREQVMjrcdOnSBUuXLkXTpk3NUQ9VQ9kFanyz5zIA4K2ejeCm5CBiIiKqvowON//5z3/MUQdVY4v2XUFWvhoNfVw5iJiIiKq9SoWbqVOn4oMPPoCLiwumTp36wHXnzp1rksKoekjJLsDyg/EAgLd7N4acD8YkIqJqrlLhJi4uTj8TKi4uzqwFUfUyL+YSijQ6tAmphW7hvBs1ERFVf5UKN3v27Cn3z2TbLt3MxQ+xSQCAd/qEc+o3ERFZBaOvMYwePRq5ubll2vPy8jB69GiTFEXVwyc7L0AngN5N/RARXEvqcoiIiCrF6HCzatUqFBQUlGkvKCjA6tWrTVIUSe/YtUz8fv4m7O1keKs3nx9FRETWo9KzpXJyciCEgBACubm5UCqV+mVarRY7duzgE8JthBACc3acBwAMahOE+rVdJa6IiIio8iodbjw8PCCTySCTyRAWFlZmuUwmw+zZs01aHEnj9/NpOJGYBScHe0zuxhv2ERGRdal0uNmzZw+EEOjatSs2b94MT09P/TKFQoHg4GAEBASYpUiyHJ1O4IvfLgAARnYMgY+78iFbEBERVS+VDjedO3cGUPJcqbp163LmjI366fQN/J2aCzelHK8+UU/qcoiIiIxWqXBz+vRpNGvWDHZ2dsjOzsaZM2cqXLd58+YmK44sS63V4cuYiwCAV5+oBw9nhcQVERERGa9S4aZly5ZITU2Fj48PWrZsCZlMBiFEmfVkMhm0Wq3JiyTL2BybjGsZ+fByUWBUx1CpyyEiIqqSSoWb+Ph41K5dW/9nsj1FGi3m77oEABjfpQFcHI1+7BgREVG1UKkzWHBwcLl/Jtux7s9E3MguhL9KiaHt6kpdDhERUZVV6SZ+P//8s/79tGnT4OHhgQ4dOiAhIcGkxZFl5Bdr8O2eywCAid0aQulgL3FFREREVWd0uPnPf/4DJycnAMCRI0fwzTff4NNPP4W3tzemTJli8gLJ/NYcSUD6nWIEeznj+Yg6UpdDRET0SIweWJGUlIQGDRoAALZt24bnn38er7zyCjp27Ignn3zS1PWRmRVrdFh+qGQc1etdGsDB3ui8S0REVK0YfSZzdXVFRkYGAOC3335D9+7dAQBKpbLcZ05R9bb91A3czCmCr7sjnm4ZKHU5REREj8zonpsePXpg7NixaNWqFS5evIi+ffsCAM6ePYuQkBBT10dmJITAkv1XAQCjOoZCIWevDRERWT+jz2bffvstoqKicOvWLWzevBleXl4AgNjYWLz44osmL5DMZ9/FW7hwMxcuCnu82JYzpIiIyDbIRHl347NhOTk5UKlUyM7Ohru7u9TlSGro0j9w6HIGxjwein8/1UTqcoiIiCpkzPm7Sndqy8rKwrJly3D+/HnIZDKEh4djzJgxUKlUVSqYLO+v69k4dDkD9nYyjOoYInU5REREJmP0Zanjx4+jfv36+PLLL5GZmYn09HR8+eWXqF+/Pk6cOGGOGskMlhwoGWvT9zF/1KnlLHE1REREpmN0z82UKVPQv39/LFmyBHJ5yeYajQZjx47F5MmTsX//fpMXSaZ1PasA/zudAgB4hU/+JiIiG2N0uDl+/LhBsAEAuVyOadOmITIy0qTFkXksPxgPrU6gQ30vNAvkpUQiIrItRl+Wcnd3R2JiYpn2pKQkuLm5maQoMp/sAjU2HC35/b3MXhsiIrJBRoebQYMGYcyYMdi4cSOSkpKQnJyMDRs2YOzYsZwKbgXW/pmAvGItGvm64cmw2lKXQ0REZHJGX5b6/PPPIZPJMHz4cGg0GgCAg4MDXnvtNXz88ccmL5BMp0ijxYpD1wCU9NrIZDJpCyIiIjKDKt/nJj8/H1euXIEQAg0aNICzs3XMuKnJ97nZeCwRb28+Az93JfZP68I7EhMRkdUw5vxd6bNbfn4+JkyYgMDAQPj4+GDs2LHw9/dH8+bNrSbY1GQ6ncDiu49aGP14CIMNERHZrEqf4WbOnImVK1eib9++GDx4MGJiYvDaa6+ZszYyod1/p+HKrTy4Ocr5qAUiIrJplR5zs2XLFixbtgyDBw8GALz00kvo2LEjtFot7O3tzVYgmUZpr82Q9nXhpnSQuBoiIiLzqXTPTVJSEjp16qR/37ZtW8jlcty4ccMshZHpnEi8jaPXMuFgL8PojqFSl0NERGRWlQ43Wq0WCoXCoE0ul+tnTFH1tXhfSa/N0y0D4euulLgaIiIi86r0ZSkhBEaOHAlHR0d9W2FhIcaNGwcXFxd925YtW0xbIT2S+PQ8/HouFQAftUBERDVDpcPNiBEjyrS99NJLJi2GTG/pgasQAujSqDbCfHkHaSIisn2VDjcrVqwwZx1kBul3irApNhkA8MoT9SWuhoiIyDIkv9nJggULEBoaCqVSiYiICBw4cKBS2x06dAhyuRwtW7Y0b4FWbPWRBBRpdGheR4X29TylLoeIiMgiJA03GzduxOTJkzFjxgzExcWhU6dO6NOnT7kP5rxXdnY2hg8fjm7dulmoUutTUKzFmiPXAJSMteGjFoiIqKaQNNzMnTsXY8aMwdixYxEeHo558+YhKCgICxcufOB2r776KoYMGYKoqCgLVWp9NsUm4Xa+GkGeTujd1E/qcoiIiCxGsnBTXFyM2NhY9OzZ06C9Z8+eOHz4cIXbrVixAleuXMHMmTPNXaLV0uoElh6MBwCMfbwe5PaSX30kIiKyGKOfCm4q6enp0Gq18PX1NWj39fVFampqudtcunQJ77zzDg4cOAC5vHKlFxUVoaioSP8+Jyen6kVbiV/PpiIhIx8ezg54IbKO1OUQERFZVJX+l37NmjXo2LEjAgICkJCQAACYN28efvzxR6P3df9YECFEueNDtFothgwZgtmzZyMsLKzS+58zZw5UKpX+FRQUZHSN1mbFoZJem2Htg+GskCy/EhERScLocLNw4UJMnToV0dHRyMrKglarBQB4eHhg3rx5ld6Pt7c37O3ty/TSpKWllenNAYDc3FwcP34cr7/+OuRyOeRyOd5//32cOnUKcrkcu3fvLvdzpk+fjuzsbP0rKSmp8l/WCp1PycGxa7cht5PhpfbBUpdDRERkcUaHm6+//hpLlizBjBkzDB6YGRkZiTNnzlR6PwqFAhEREYiJiTFoj4mJQYcOHcqs7+7ujjNnzuDkyZP617hx49CoUSOcPHkS7dq1K/dzHB0d4e7ubvCyZauPlPSk9Wrqx0ctEBFRjWT0NYv4+Hi0atWqTLujoyPy8vKM2tfUqVMxbNgwREZGIioqCosXL0ZiYiLGjRsHoKTX5fr161i9ejXs7OzQrFkzg+19fHygVCrLtNdUOYVqbIu7DgAYFsVeGyIiqpmMDjehoaE4efIkgoMNT56//PILmjRpYtS+Bg0ahIyMDLz//vtISUlBs2bNsGPHDv2+U1JSHnrPG/rH5thkFKi1CPN1RbtQ3rSPiIhqJpkQQhizwYoVK/Dvf/8bX3zxBcaMGYOlS5fiypUrmDNnDpYuXYrBgwebq1aTyMnJgUqlQnZ2tk1dohJCoNvcfbh6Kw8fPN0Uw6JCpC6JiIjIZIw5fxvdczNq1ChoNBpMmzYN+fn5GDJkCAIDA/HVV19V+2Bjyw5fycDVW3lwdZRjQGtO/yYiopqrSvOEX375Zbz88stIT0+HTqeDj4+PqesiI/33j5KBxM+2DoSrI6d/ExFRzfVIZ0Fvb29T1UGPILtAjV3n0wAAg9vUlbgaIiIiaVVpQPGDHsJ49erVRyqIjPfb2VQUa3Vo6OOKcH83qcshIiKSlNHhZvLkyQbv1Wo14uLisHPnTrz11lumqouM8NPpFABA/xYBfPo3ERHVeEaHm0mTJpXb/u233+L48eOPXBAZJ+NOEQ5dTgcAPNUiQOJqiIiIpGeyx0X36dMHmzdvNtXuqJJ2/JUKrU7gsUAVQr1dpC6HiIhIciYLN5s2bYKnJ28cZ2k/nboBAOjXwl/iSoiIiKoHoy9LtWrVymBchxACqampuHXrFhYsWGDS4ujBUrILcOxaJgCgb3NekiIiIgKqEG6eeeYZg/d2dnaoXbs2nnzySTRu3NhUdVEl/Hw6BUIAbUJqIdDDSepyiIiIqgWjwo1Go0FISAh69eoFPz8/c9VElVQ6S+op9toQERHpGTXmRi6X47XXXkNRUZG56qFKSr6dj1NJWZDJgD6PMWgSERGVMnpAcbt27RAXF2eOWsgIv5xJBQC0DfGEj5tS4mqIiIiqD6PH3IwfPx7/+te/kJycjIiICLi4GE4/bt68ucmKo4rt+KvkklT0Y5wlRUREdK9Kh5vRo0dj3rx5GDRoEABg4sSJ+mUymQxCCMhkMmi1WtNXSQZuZBUgLvHuJalmvCRFRER0r0qHm1WrVuHjjz9GfHy8OeuhSthxpqTXpk2wJ3zceUmKiIjoXpUON0IIAEBwcLDZiqHKKQ030RxITEREVIZRA4r5UEbppWQX4ETpJSmOtyEiIirDqAHFYWFhDw04mZmZj1QQPVjpLKnI4Frw5SUpIiKiMowKN7Nnz4ZKpTJXLVQJO8+WhJvezdhrQ0REVB6jws3gwYPh4+NjrlroIdLvFOH43WdJ9eYsKSIionJVeswNx9tI7/dzN6ETwGOBKj5LioiIqAKVDjels6VIOr/qL0mx14aIiKgilb4spdPpzFkHPUROoRqHLmcAAHo19ZW4GiIiourL6GdLkTT2/J2GYq0O9Wu7oIGPm9TlEBERVVsMN1bit7M3AQC9mvKSFBER0YMw3FiBQrUWey6kAeB4GyIioodhuLEChy6nI79YiwCVEo8F8j5DRERED8JwYwVKL0n1bOrHKflEREQPwXBTzWl1Ar+fvxtumnCWFBER0cMw3FRzJxJvIyOvGConB7QJ9ZS6HCIiomqP4aaa++3ujfu6NfaBgz1/XURERA/Ds2U1JoTAb+dKLkn14CUpIiKiSmG4qcYu3ryDhIx8KOR2eCKsttTlEBERWQWGm2qs9JJUpwbecHE06gHuRERENRbDTTX267mScMNLUkRERJXHcFNNJWbk46/rObC3kzHcEBERGYHhppra8VcKAKB9PU94uTpKXA0REZH1YLippnacKQk3fZr5S1wJERGRdWG4qYaSMvNxOjkbdjI+BZyIiMhYDDfV0C93L0m1DfVEbTdekiIiIjIGw001tONMySypvo/xkhQREZGxGG6qmRtZBTiZlAUZL0kRERFVCcNNNbPzr5JemzbBnvBxV0pcDRERkfVhuKlmdt69K3GvZuy1ISIiqgqGm2rkVm4Rjl3LBAD0ZrghIiKqEoabaiTm3E0IATSvo0Kgh5PU5RAREVklhptqpHQKOHttiIiIqo7hpprIzlfjyJUMAEBvzpIiIiKqMoabamLPhTRodAJhvq6oV9tV6nKIiIisFsNNNbHv4i0AQLdwPgGciIjoUTDcVAM6ncD+u+HmiYa1Ja6GiIjIujHcVAPnUnKQkVcMF4U9IoJrSV0OERGRVWO4qQZKL0l1aOANhZy/EiIiokfBM2k1UBpungjjJSkiIqJHxXAjsdxCNU4k3AYAdOZ4GyIiokcmebhZsGABQkNDoVQqERERgQMHDlS47pYtW9CjRw/Url0b7u7uiIqKwq+//mrBak3v8JUMaHQCod4uqOvlLHU5REREVk/ScLNx40ZMnjwZM2bMQFxcHDp16oQ+ffogMTGx3PX379+PHj16YMeOHYiNjUWXLl3Qr18/xMXFWbhy0ym9JNWZl6SIiIhMQiaEEFJ9eLt27dC6dWssXLhQ3xYeHo5nnnkGc+bMqdQ+mjZtikGDBuG9996r1Po5OTlQqVTIzs6Gu7t7leo2FSEEHv9kD65nFWDFqDbo0shH0nqIiIiqK2PO35L13BQXFyM2NhY9e/Y0aO/ZsycOHz5cqX3odDrk5ubC09PTHCWa3ZVbebieVQCF3A7tQ72kLoeIiMgmyKX64PT0dGi1Wvj6Gt6R19fXF6mpqZXaxxdffIG8vDwMHDiwwnWKiopQVFSkf5+Tk1O1gs2g9JJUu1BPOCnsJa6GiIjINkg+oFgmkxm8F0KUaSvP+vXrMWvWLGzcuBE+PhVfzpkzZw5UKpX+FRQU9Mg1mwrH2xAREZmeZOHG29sb9vb2ZXpp0tLSyvTm3G/jxo0YM2YMvv/+e3Tv3v2B606fPh3Z2dn6V1JS0iPXbgoFxVr8ebXkKeAMN0RERKYjWbhRKBSIiIhATEyMQXtMTAw6dOhQ4Xbr16/HyJEjsW7dOvTt2/ehn+Po6Ah3d3eDV3Vw4NItFGl0CPRwQgMfPgWciIjIVCQbcwMAU6dOxbBhwxAZGYmoqCgsXrwYiYmJGDduHICSXpfr169j9erVAEqCzfDhw/HVV1+hffv2+l4fJycnqFQqyb5HVfx+/iYAoEcT30pdhiMiIqLKkTTcDBo0CBkZGXj//feRkpKCZs2aYceOHQgODgYApKSkGNzz5rvvvoNGo8GECRMwYcIEffuIESOwcuVKS5dfZVqdwK7zaQBKwg0RERGZjqT3uZFCdbjPTWxCJp5beARuSjlO/LsHHOwlH9dNRERUrVnFfW5qsphzJb02XRr5MNgQERGZGM+sErh3vA0RERGZFsONhaVmF+Jy2h3YyYAnOAWciIjI5BhuLOzP+JJ72zQNUEHl5CBxNURERLaH4cbCjsZnAgDahlrn87CIiIiqO4YbC/vzbrhpx3BDRERkFgw3FpR+pwiX0+4AANqEMNwQERGZA8ONBR2722vTyNcNtVwUEldDRERkmxhuLOhPjrchIiIyO4YbCzqewHBDRERkbgw3FpJbqMa5GzkAgMiQWhJXQ0REZLsYbiwkLjELOgHUqeUEf5WT1OUQERHZLIYbCzl+7e4lKc6SIiIiMiuGGws5du02ACCS4YaIiMisGG4sQK3VIS6pJNy04XgbIiIis2K4sYBzN3JQqNZB5eSA+rVdpS6HiIjIpjHcWMCJxJJem9Z1PWBnJ5O4GiIiItvGcGMBsQkl4SYimJekiIiIzI3hxgLiErMAAK3rMtwQERGZG8ONmaVmF+J6VgHsZECLIA+pyyEiIrJ5DDdmVjreprGfO1wc5RJXQ0REZPsYbszs+DWOtyEiIrIkhhszi737sEw+T4qIiMgyGG7MqKBYi7N3H5bJwcRERESWwXBjRqeSs6DRCfi6O6JOLT4sk4iIyBIYbszoZFIWgJJeG5mMN+8jIiKyBIYbM7p08w4AINzfXeJKiIiIag6GGzO6nJYLAGjow+dJERERWQrDjZkIIXA5raTnpqEvww0REZGlMNyYSUp2IfKKtZDbyRDs5SJ1OURERDUGw42ZlPbahHi7wMGeh5mIiMhSeNY1k79TS+5vE8ZLUkRERBbFcGMmpTfvaxqgkrgSIiKimoXhxkzO3Q03TTgNnIiIyKIYbsygUK3FlVslY26aBjDcEBERWRLDjRlcunkHOgF4uShQ281R6nKIiIhqFIYbM7iaXtJrU6+2Cx+7QEREZGEMN2ZwLT0fABDC+9sQERFZHMONGVzLyANQco8bIiIisiyGGzO4ml4SbkIZboiIiCyO4cbEhBC4fJMPzCQiIpIKw42J3ftMKV6WIiIisjyGGxO7dPeZUqF8phQREZEkePY1saTMkplSfBI4ERGRNBhuTOxGVgEAINBDKXElRERENRPDjYmVhpsADyeJKyEiIqqZGG5M7EZWIQCGGyIiIqkw3JhY4t0xN3VqMdwQERFJgeHGhAqKtUjNKem54Q38iIiIpMFwY0IJmSV3JlY5OcDDWSFxNURERDUTw40JJWeWDCYO8uQlKSIiIqkw3JjQjezSaeAMN0RERFJhuDGh0plS/iqGGyIiIqkw3JhQWm5JuPF15w38iIiIpMJwY0K3cosAAN6uHExMREQkFYYbE8rMKwYAeLs5SlwJERFRzcVwY0JZ+WoAQC1OAyciIpKM5OFmwYIFCA0NhVKpREREBA4cOPDA9fft24eIiAgolUrUq1cPixYtslClD5dbWBJuXB3lEldCRERUc0kabjZu3IjJkydjxowZiIuLQ6dOndCnTx8kJiaWu358fDyio6PRqVMnxMXF4d1338XEiROxefNmC1dellYnkFukAQC4OzHcEBERSUUmhBBSfXi7du3QunVrLFy4UN8WHh6OZ555BnPmzCmz/ttvv43t27fj/Pnz+rZx48bh1KlTOHLkSKU+MycnByqVCtnZ2XB3d3/0L3HXnSINms38FQBw/v3ecFLYm2zfRERENZ0x52/Jem6Ki4sRGxuLnj17GrT37NkThw8fLnebI0eOlFm/V69eOH78ONRqdbnbFBUVIScnx+BlDoVqrf7PjnLJr/YRERHVWJKdhdPT06HVauHr62vQ7uvri9TU1HK3SU1NLXd9jUaD9PT0creZM2cOVCqV/hUUFGSaL3AfnRBwlNvBycEednYys3wGERERPZzkXQwymWEQEEKUaXvY+uW1l5o+fTqys7P1r6SkpEesuHw+bkpc+LAPzn/Q2yz7JyIiosqRbOSrt7c37O3ty/TSpKWllemdKeXn51fu+nK5HF5eXuVu4+joCEdH3neGiIioppCs50ahUCAiIgIxMTEG7TExMejQoUO520RFRZVZ/7fffkNkZCQcHBzMVisRERFZD0kvS02dOhVLly7F8uXLcf78eUyZMgWJiYkYN24cgJJLSsOHD9evP27cOCQkJGDq1Kk4f/48li9fjmXLluHNN9+U6isQERFRNSPpDVkGDRqEjIwMvP/++0hJSUGzZs2wY8cOBAcHAwBSUlIM7nkTGhqKHTt2YMqUKfj2228REBCA+fPn47nnnpPqKxAREVE1I+l9bqRgrvvcEBERkflYxX1uiIiIiMyB4YaIiIhsCsMNERER2RSGGyIiIrIpDDdERERkUxhuiIiIyKYw3BAREZFNYbghIiIim8JwQ0RERDZF0scvSKH0hsw5OTkSV0JERESVVXrersyDFWpcuMnNzQUABAUFSVwJERERGSs3NxcqleqB69S4Z0vpdDrcuHEDbm5ukMlkJt13Tk4OgoKCkJSUxOdWmRGPs2XwOFsGj7Pl8FhbhrmOsxACubm5CAgIgJ3dg0fV1LieGzs7O9SpU8esn+Hu7s7/cCyAx9kyeJwtg8fZcnisLcMcx/lhPTalOKCYiIiIbArDDREREdkUhhsTcnR0xMyZM+Ho6Ch1KTaNx9kyeJwtg8fZcnisLaM6HOcaN6CYiIiIbBt7boiIiMimMNwQERGRTWG4ISIiIpvCcENEREQ2heHGSAsWLEBoaCiUSiUiIiJw4MCBB66/b98+REREQKlUol69eli0aJGFKrVuxhznLVu2oEePHqhduzbc3d0RFRWFX3/91YLVWi9j/z6XOnToEORyOVq2bGneAm2Esce5qKgIM2bMQHBwMBwdHVG/fn0sX77cQtVaL2OP89q1a9GiRQs4OzvD398fo0aNQkZGhoWqtU779+9Hv379EBAQAJlMhm3btj10G0nOg4IqbcOGDcLBwUEsWbJEnDt3TkyaNEm4uLiIhISEcte/evWqcHZ2FpMmTRLnzp0TS5YsEQ4ODmLTpk0Wrty6GHucJ02aJD755BNx9OhRcfHiRTF9+nTh4OAgTpw4YeHKrYuxx7lUVlaWqFevnujZs6do0aKFZYq1YlU5zv379xft2rUTMTExIj4+Xvz555/i0KFDFqza+hh7nA8cOCDs7OzEV199Ja5evSoOHDggmjZtKp555hkLV25dduzYIWbMmCE2b94sAIitW7c+cH2pzoMMN0Zo27atGDdunEFb48aNxTvvvFPu+tOmTRONGzc2aHv11VdF+/btzVajLTD2OJenSZMmYvbs2aYuzaZU9TgPGjRI/N///Z+YOXMmw00lGHucf/nlF6FSqURGRoYlyrMZxh7nzz77TNSrV8+gbf78+aJOnTpmq9HWVCbcSHUe5GWpSiouLkZsbCx69uxp0N6zZ08cPny43G2OHDlSZv1evXrh+PHjUKvVZqvVmlXlON9Pp9MhNzcXnp6e5ijRJlT1OK9YsQJXrlzBzJkzzV2iTajKcd6+fTsiIyPx6aefIjAwEGFhYXjzzTdRUFBgiZKtUlWOc4cOHZCcnIwdO3ZACIGbN29i06ZN6Nu3ryVKrjGkOg/WuAdnVlV6ejq0Wi18fX0N2n19fZGamlruNqmpqeWur9FokJ6eDn9/f7PVa62qcpzv98UXXyAvLw8DBw40R4k2oSrH+dKlS3jnnXdw4MAByOX8p6MyqnKcr169ioMHD0KpVGLr1q1IT0/H+PHjkZmZyXE3FajKce7QoQPWrl2LQYMGobCwEBqNBv3798fXX39tiZJrDKnOg+y5MZJMJjN4L4Qo0/aw9ctrJ0PGHudS69evx6xZs7Bx40b4+PiYqzybUdnjrNVqMWTIEMyePRthYWGWKs9mGPP3WafTQSaTYe3atWjbti2io6Mxd+5crFy5kr03D2HMcT537hwmTpyI9957D7Gxsdi5cyfi4+Mxbtw4S5Rao0hxHuT/flWSt7c37O3ty/xfQFpaWplUWsrPz6/c9eVyOby8vMxWqzWrynEutXHjRowZMwY//PADunfvbs4yrZ6xxzk3NxfHjx9HXFwcXn/9dQAlJ2EhBORyOX777Td07drVIrVbk6r8ffb390dgYCBUKpW+LTw8HEIIJCcno2HDhmat2RpV5TjPmTMHHTt2xFtvvQUAaN68OVxcXNCpUyd8+OGH7Fk3EanOg+y5qSSFQoGIiAjExMQYtMfExKBDhw7lbhMVFVVm/d9++w2RkZFwcHAwW63WrCrHGSjpsRk5ciTWrVvHa+aVYOxxdnd3x5kzZ3Dy5En9a9y4cWjUqBFOnjyJdu3aWap0q1KVv88dO3bEjRs3cOfOHX3bxYsXYWdnhzp16pi1XmtVleOcn58POzvDU6C9vT2Af3oW6NFJdh4063BlG1M61XDZsmXi3LlzYvLkycLFxUVcu3ZNCCHEO++8I4YNG6Zfv3QK3JQpU8S5c+fEsmXLOBW8Eow9zuvWrRNyuVx8++23IiUlRf/KysqS6itYBWOP8/04W6pyjD3Oubm5ok6dOuL5558XZ8+eFfv27RMNGzYUY8eOleorWAVjj/OKFSuEXC4XCxYsEFeuXBEHDx4UkZGRom3btlJ9BauQm5sr4uLiRFxcnAAg5s6dK+Li4vRT7qvLeZDhxkjffvutCA4OFgqFQrRu3Vrs27dPv2zEiBGic+fOBuvv3btXtGrVSigUChESEiIWLlxo4YqtkzHHuXPnzgJAmdeIESMsX7iVMfbv870YbirP2ON8/vx50b17d+Hk5CTq1Kkjpk6dKvLz8y1ctfUx9jjPnz9fNGnSRDg5OQl/f38xdOhQkZycbOGqrcuePXse+O9tdTkPyoRg/xsRERHZDo65ISIiIpvCcENEREQ2heGGiIiIbArDDREREdkUhhsiIiKyKQw3REREZFMYboiIiMimMNwQkYGVK1fCw8ND6jKqLCQkBPPmzXvgOrNmzULLli0tUg8RWR7DDZENGjlyJGQyWZnX5cuXpS4NK1euNKjJ398fAwcORHx8vEn2f+zYMbzyyiv69zKZDNu2bTNY580338SuXbtM8nkVuf97+vr6ol+/fjh79qzR+7HmsEkkBYYbIhvVu3dvpKSkGLxCQ0OlLgtAyYM4U1JScOPGDaxbtw4nT55E//79odVqH3nftWvXhrOz8wPXcXV1NesTiUvd+z1//vln5OXloW/fviguLjb7ZxPVZAw3RDbK0dERfn5+Bi97e3vMnTsXjz32GFxcXBAUFITx48cbPIH6fqdOnUKXLl3g5uYGd3d3RERE4Pjx4/rlhw8fxhNPPAEnJycEBQVh4sSJyMvLe2BtMpkMfn5+8Pf3R5cuXTBz5kz89ddf+p6lhQsXon79+lAoFGjUqBHWrFljsP2sWbNQt25dODo6IiAgABMnTtQvu/eyVEhICABgwIABkMlk+vf3Xpb69ddfoVQqkZWVZfAZEydOROfOnU32PSMjIzFlyhQkJCTgwoUL+nUe9PvYu3cvRo0ahezsbH0P0KxZswAAxcXFmDZtGgIDA+Hi4oJ27dph7969D6yHqKZguCGqYezs7DB//nz89ddfWLVqFXbv3o1p06ZVuP7QoUNRp04dHDt2DLGxsXjnnXfg4OAAADhz5gx69eqFZ599FqdPn8bGjRtx8OBBvP7660bV5OTkBABQq9XYunUrJk2ahH/961/466+/8Oqrr2LUqFHYs2cPAGDTpk348ssv8d133+HSpUvYtm0bHnvssXL3e+zYMQDAihUrkJKSon9/r+7du8PDwwObN2/Wt2m1Wnz//fcYOnSoyb5nVlYW1q1bBwD64wc8+PfRoUMHzJs3T98DlJKSgjfffBMAMGrUKBw6dAgbNmzA6dOn8cILL6B37964dOlSpWsisllmfzQnEVnciBEjhL29vXBxcdG/nn/++XLX/f7774WXl5f+/YoVK4RKpdK/d3NzEytXrix322HDholXXnnFoO3AgQPCzs5OFBQUlLvN/ftPSkoS7du3F3Xq1BFFRUWiQ4cO4uWXXzbY5oUXXhDR0dFCCCG++OILERYWJoqLi8vdf3BwsPjyyy/17wGIrVu3Gqxz/xPNJ06cKLp27ap//+uvvwqFQiEyMzMf6XsCEC4uLsLZ2Vn/9OT+/fuXu36ph/0+hBDi8uXLQiaTievXrxu0d+vWTUyfPv2B+yeqCeTSRisiMpcuXbpg4cKF+vcuLi4AgD179uA///kPzp07h5ycHGg0GhQWFiIvL0+/zr2mTp2KsWPHYs2aNejevTteeOEF1K9fHwAQGxuLy5cvY+3atfr1hRDQ6XSIj49HeHh4ubVlZ2fD1dUVQgjk5+ejdevW2LJlCxQKBc6fP28wIBgAOnbsiK+++goA8MILL2DevHmoV68eevfujejoaPTr1w9yedX/ORs6dCiioqJw48YNBAQEYO3atYiOjkatWrUe6Xu6ubnhxIkT0Gg02LdvHz777DMsWrTIYB1jfx8AcOLECQghEBYWZtBeVFRkkbFERNUdww2RjXJxcUGDBg0M2hISEhAdHY1x48bhgw8+gKenJw4ePIgxY8ZArVaXu59Zs2ZhyJAh+Pnnn/HLL79g5syZ2LBhAwYMGACdTodXX33VYMxLqbp161ZYW+lJ387ODr6+vmVO4jKZzOC9EELfFhQUhAsXLiAmJga///47xo8fj88++wz79u0zuNxjjLZt26J+/frYsGEDXnvtNWzduhUrVqzQL6/q97Szs9P/Dho3bozU1FQMGjQI+/fvB1C130dpPfb29oiNjYW9vb3BMldXV6O+O5EtYrghqkGOHz8OjUaDL774AnZ2JUPuvv/++4duFxYWhrCwMEyZMgUvvvgiVqxYgQEDBqB169Y4e/ZsmRD1MPee9O8XHh6OgwcPYvjw4fq2w4cPG/SOODk5oX///ujfvz8mTJiAxo0b48yZM2jdunWZ/Tk4OFRqFtaQIUOwdu1a1KlTB3Z2dujbt69+WVW/5/2mTJmCuXPnYuvWrRgwYEClfh8KhaJM/a1atYJWq0VaWho6der0SDUR2SIOKCaqQerXrw+NRoOvv/4aV69exZo1a8pcJrlXQUEBXn/9dezduxcJCQk4dOgQjh07pg8ab7/9No4cOYIJEybg5MmTuHTpErZv34433nijyjW+9dZbWLlyJRYtWoRLly5h7ty52LJli34g7cqVK7Fs2TL89ddf+u/g5OSE4ODgcvcXEhKCXbt2ITU1Fbdv367wc4cOHYoTJ07go48+wvPPPw+lUqlfZqrv6e7ujrFjx2LmzJkQQlTq9xESEoI7d+5g165dSE9PR35+PsLCwjB06FAMHz4cW7ZsQXx8PI4dO4ZPPvkEO3bsMKomIpsk5YAfIjKPESNGiKeffrrcZXPnzhX+/v7CyclJ9OrVS6xevVoAELdv3xZCGA5gLSoqEoMHDxZBQUFCoVCIgIAA8frrrxsMoj169Kjo0aOHcHV1FS4uLqJ58+bio48+qrC28gbI3m/BggWiXr16wsHBQYSFhYnVq1frl23dulW0a9dOuLu7CxcXF9G+fXvx+++/65ffP6B4+/btokGDBkIul4vg4GAhRNkBxaXatGkjAIjdu3eXWWaq75mQkCDkcrnYuHGjEOLhvw8hhBg3bpzw8vISAMTMmTOFEEIUFxeL9957T4SEhAgHBwfh5+cnBgwYIE6fPl1hTUQ1hUwIIaSNV0RERESmw8tSREREZFMYboiIiMimMNwQERGRTWG4ISIiIpvCcENEREQ2heGGiIiIbArDDREREdkUhhsiIiKyKQw3REREZFMYboiIiMimMNwQERGRTWG4ISIiIpvy/4bcu3Ld8msxAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc(y_pred, y_train[num_samples:2*num_samples], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fffdf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.3661413856853596\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm.predict(x_train[num_samples:2*num_samples])\n",
    "print(\"F1 Score:\", f_score(y_pred, y_train[num_samples:2*num_samples]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
