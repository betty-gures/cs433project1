{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff5bbcc",
   "metadata": {},
   "source": [
    "# 010: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9352bc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from metrics import f_score, auc_roc\n",
    "from models import LinearSVM\n",
    "from visualizations import plot_roc, plot_losses\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed563196",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load(\"../data/dataset_prep/train.npz\")\n",
    "x_train = train[\"x_train\"]\n",
    "y_train = train[\"y_train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a694d222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Training Loss: 1.0\n",
      "Iteration 1, Training Loss: 0.9987270632938003\n",
      "Iteration 2, Training Loss: 0.9974573089293663\n",
      "Iteration 3, Training Loss: 0.9961907289508434\n",
      "Iteration 4, Training Loss: 0.9949273154222665\n",
      "Iteration 5, Training Loss: 0.9936670604275114\n",
      "Iteration 6, Training Loss: 0.9924099560702431\n",
      "Iteration 7, Training Loss: 0.9911559944738678\n",
      "Iteration 8, Training Loss: 0.9899051677814835\n",
      "Iteration 9, Training Loss: 0.9886574681558304\n",
      "Iteration 10, Training Loss: 0.9874128877792412\n",
      "Iteration 11, Training Loss: 0.9861714188535934\n",
      "Iteration 12, Training Loss: 0.9849330536002598\n",
      "Iteration 13, Training Loss: 0.9836977842600597\n",
      "Iteration 14, Training Loss: 0.9824656030932098\n",
      "Iteration 15, Training Loss: 0.9812365023792773\n",
      "Iteration 16, Training Loss: 0.9800104744171296\n",
      "Iteration 17, Training Loss: 0.9787875115248871\n",
      "Iteration 18, Training Loss: 0.9775676060398754\n",
      "Iteration 19, Training Loss: 0.9763507503185761\n",
      "Iteration 20, Training Loss: 0.9751369367365801\n",
      "Iteration 21, Training Loss: 0.9739265656113115\n",
      "Iteration 22, Training Loss: 0.9727305321458336\n",
      "Iteration 23, Training Loss: 0.9715374887640194\n",
      "Iteration 24, Training Loss: 0.9703474279906598\n",
      "Iteration 25, Training Loss: 0.9691633470208184\n",
      "Iteration 26, Training Loss: 0.9680127524396867\n",
      "Iteration 27, Training Loss: 0.9669092900046148\n",
      "Iteration 28, Training Loss: 0.9658443100291879\n",
      "Iteration 29, Training Loss: 0.9648025784867078\n",
      "Iteration 30, Training Loss: 0.9637910386991783\n",
      "Iteration 31, Training Loss: 0.9628293397433589\n",
      "Iteration 32, Training Loss: 0.9619310240618356\n",
      "Iteration 33, Training Loss: 0.9610740841906388\n",
      "Iteration 34, Training Loss: 0.9602808213270319\n",
      "Iteration 35, Training Loss: 0.9595320283203951\n",
      "Iteration 36, Training Loss: 0.9588268273136046\n",
      "Iteration 37, Training Loss: 0.9581662143022659\n",
      "Iteration 38, Training Loss: 0.957540716280055\n",
      "Iteration 39, Training Loss: 0.956936227970582\n",
      "Iteration 40, Training Loss: 0.9563603156510687\n",
      "Iteration 41, Training Loss: 0.9558027427860816\n",
      "Iteration 42, Training Loss: 0.9552683969237908\n",
      "Iteration 43, Training Loss: 0.9547510970067713\n",
      "Iteration 44, Training Loss: 0.9542419279243339\n",
      "Iteration 45, Training Loss: 0.9537562967750502\n",
      "Iteration 46, Training Loss: 0.9532993425611034\n",
      "Iteration 47, Training Loss: 0.952855761199958\n",
      "Iteration 48, Training Loss: 0.9524312904623183\n",
      "Iteration 49, Training Loss: 0.9520190207371522\n",
      "Iteration 50, Training Loss: 0.9516115310841704\n",
      "Iteration 51, Training Loss: 0.9512166148045879\n",
      "Iteration 52, Training Loss: 0.9508361404924468\n",
      "Iteration 53, Training Loss: 0.9504649767159542\n",
      "Iteration 54, Training Loss: 0.9501003111617414\n",
      "Iteration 55, Training Loss: 0.9497410869527837\n",
      "Iteration 56, Training Loss: 0.9493929858659138\n",
      "Iteration 57, Training Loss: 0.9490553587834432\n",
      "Iteration 58, Training Loss: 0.948725995681954\n",
      "Iteration 59, Training Loss: 0.9484169490833234\n",
      "Iteration 60, Training Loss: 0.9481193560262104\n",
      "Iteration 61, Training Loss: 0.9478282324734179\n",
      "Iteration 62, Training Loss: 0.9475452638191505\n",
      "Iteration 63, Training Loss: 0.9472685774285406\n",
      "Iteration 64, Training Loss: 0.9470000527795362\n",
      "Iteration 65, Training Loss: 0.9467379436422948\n",
      "Iteration 66, Training Loss: 0.9464807498314333\n",
      "Iteration 67, Training Loss: 0.9462279964403771\n",
      "Iteration 68, Training Loss: 0.9459785295943381\n",
      "Iteration 69, Training Loss: 0.9457334501723689\n",
      "Iteration 70, Training Loss: 0.9454964478092152\n",
      "Iteration 71, Training Loss: 0.9452651863286591\n",
      "Iteration 72, Training Loss: 0.945034917526692\n",
      "Iteration 73, Training Loss: 0.9448109234431915\n",
      "Iteration 74, Training Loss: 0.944594388966822\n",
      "Iteration 75, Training Loss: 0.9443809612401438\n",
      "Iteration 76, Training Loss: 0.9441729542343669\n",
      "Iteration 77, Training Loss: 0.9439658602040469\n",
      "Iteration 78, Training Loss: 0.9437603804305804\n",
      "Iteration 79, Training Loss: 0.9435567634029339\n",
      "Iteration 80, Training Loss: 0.9433578732610074\n",
      "Iteration 81, Training Loss: 0.9431625211195187\n",
      "Iteration 82, Training Loss: 0.9429748450949117\n",
      "Iteration 83, Training Loss: 0.9427912337110628\n",
      "Iteration 84, Training Loss: 0.9426097794641941\n",
      "Iteration 85, Training Loss: 0.9424303296353147\n",
      "Iteration 86, Training Loss: 0.9422537810603308\n",
      "Iteration 87, Training Loss: 0.9420826695620684\n",
      "Iteration 88, Training Loss: 0.941912978666515\n",
      "Iteration 89, Training Loss: 0.9417466279261447\n",
      "Iteration 90, Training Loss: 0.9415816909945698\n",
      "Iteration 91, Training Loss: 0.9414218272997928\n",
      "Iteration 92, Training Loss: 0.9412644200105926\n",
      "Iteration 93, Training Loss: 0.9411096669612155\n",
      "Iteration 94, Training Loss: 0.9409555689916727\n",
      "Iteration 95, Training Loss: 0.9408037594672847\n",
      "Iteration 96, Training Loss: 0.9406545540834186\n",
      "Iteration 97, Training Loss: 0.9405069209838748\n",
      "Iteration 98, Training Loss: 0.9403631701645928\n",
      "Iteration 99, Training Loss: 0.9402225488139249\n",
      "Iteration 100, Training Loss: 0.9400830248387895\n",
      "Iteration 101, Training Loss: 0.9399442475901721\n",
      "Iteration 102, Training Loss: 0.9398069332616276\n",
      "Iteration 103, Training Loss: 0.939670840461311\n",
      "Iteration 104, Training Loss: 0.9395396143242517\n",
      "Iteration 105, Training Loss: 0.9394096623723299\n",
      "Iteration 106, Training Loss: 0.9392811454365081\n",
      "Iteration 107, Training Loss: 0.9391540176138742\n",
      "Iteration 108, Training Loss: 0.9390293300645033\n",
      "Iteration 109, Training Loss: 0.9389062708498879\n",
      "Iteration 110, Training Loss: 0.9387874066533236\n",
      "Iteration 111, Training Loss: 0.9386701269827634\n",
      "Iteration 112, Training Loss: 0.9385545604256618\n",
      "Iteration 113, Training Loss: 0.9384413028968265\n",
      "Iteration 114, Training Loss: 0.9383286632433858\n",
      "Iteration 115, Training Loss: 0.9382172324821462\n",
      "Iteration 116, Training Loss: 0.9381090969573941\n",
      "Iteration 117, Training Loss: 0.9380037973597798\n",
      "Iteration 118, Training Loss: 0.9379012527794173\n",
      "Iteration 119, Training Loss: 0.9378010304723825\n",
      "Iteration 120, Training Loss: 0.9377011199902732\n",
      "Iteration 121, Training Loss: 0.9376027252976264\n",
      "Iteration 122, Training Loss: 0.9375057666632178\n",
      "Iteration 123, Training Loss: 0.9374123846573114\n",
      "Iteration 124, Training Loss: 0.9373225871009149\n",
      "Iteration 125, Training Loss: 0.9372349622538343\n",
      "Iteration 126, Training Loss: 0.9371490278826747\n",
      "Iteration 127, Training Loss: 0.9370645982485617\n",
      "Iteration 128, Training Loss: 0.9369824161838826\n",
      "Iteration 129, Training Loss: 0.9369008548684006\n",
      "Iteration 130, Training Loss: 0.9368204788444039\n",
      "Iteration 131, Training Loss: 0.9367409316516492\n",
      "Iteration 132, Training Loss: 0.9366626417051978\n",
      "Iteration 133, Training Loss: 0.9365858718006784\n",
      "Iteration 134, Training Loss: 0.9365105968600986\n",
      "Iteration 135, Training Loss: 0.9364385081082021\n",
      "Iteration 136, Training Loss: 0.9363665156606128\n",
      "Iteration 137, Training Loss: 0.93629566792889\n",
      "Iteration 138, Training Loss: 0.9362263778532116\n",
      "Iteration 139, Training Loss: 0.9361576046289577\n",
      "Iteration 140, Training Loss: 0.9360889196759581\n",
      "Iteration 141, Training Loss: 0.9360211632153377\n",
      "Iteration 142, Training Loss: 0.9359544907461463\n",
      "Iteration 143, Training Loss: 0.9358895991858946\n",
      "Iteration 144, Training Loss: 0.9358277256946683\n",
      "Iteration 145, Training Loss: 0.9357667206526283\n",
      "Iteration 146, Training Loss: 0.9357062123156199\n",
      "Iteration 147, Training Loss: 0.9356457122935076\n",
      "Iteration 148, Training Loss: 0.9355871440113444\n",
      "Iteration 149, Training Loss: 0.9355295969512945\n",
      "Iteration 150, Training Loss: 0.935472700669012\n",
      "Iteration 151, Training Loss: 0.9354182121840928\n",
      "Iteration 152, Training Loss: 0.9353641374025746\n",
      "Iteration 153, Training Loss: 0.9353102374650861\n",
      "Iteration 154, Training Loss: 0.9352574880475538\n",
      "Iteration 155, Training Loss: 0.9352065051278202\n",
      "Iteration 156, Training Loss: 0.9351563292927766\n",
      "Iteration 157, Training Loss: 0.9351076948649201\n",
      "Iteration 158, Training Loss: 0.935060309279854\n",
      "Iteration 159, Training Loss: 0.9350146842015574\n",
      "Iteration 160, Training Loss: 0.9349688397204575\n",
      "Iteration 161, Training Loss: 0.9349238017246951\n",
      "Iteration 162, Training Loss: 0.9348792272078535\n",
      "Iteration 163, Training Loss: 0.9348351188460817\n",
      "Iteration 164, Training Loss: 0.9347911539792038\n",
      "Iteration 165, Training Loss: 0.934747656241251\n",
      "Iteration 166, Training Loss: 0.9347045013217044\n",
      "Iteration 167, Training Loss: 0.9346615048577996\n",
      "Iteration 168, Training Loss: 0.9346188900997391\n",
      "Iteration 169, Training Loss: 0.9345760779719766\n",
      "Iteration 170, Training Loss: 0.9345337063988135\n",
      "Iteration 171, Training Loss: 0.9344912206194821\n",
      "Iteration 172, Training Loss: 0.9344497780696247\n",
      "Iteration 173, Training Loss: 0.9344085582428509\n",
      "Iteration 174, Training Loss: 0.9343680652969601\n",
      "Iteration 175, Training Loss: 0.9343277280119681\n",
      "Iteration 176, Training Loss: 0.9342877722944879\n",
      "Iteration 177, Training Loss: 0.9342474832221646\n",
      "Iteration 178, Training Loss: 0.9342085877463576\n",
      "Iteration 179, Training Loss: 0.9341704082821258\n",
      "Iteration 180, Training Loss: 0.9341321489575614\n",
      "Iteration 181, Training Loss: 0.9340942956450926\n",
      "Iteration 182, Training Loss: 0.9340561606077253\n",
      "Iteration 183, Training Loss: 0.9340181509005229\n",
      "Iteration 184, Training Loss: 0.9339805996286795\n",
      "Iteration 185, Training Loss: 0.9339424447188701\n",
      "Iteration 186, Training Loss: 0.9339055247036137\n",
      "Iteration 187, Training Loss: 0.9338695269818897\n",
      "Iteration 188, Training Loss: 0.9338334567543034\n",
      "Iteration 189, Training Loss: 0.9337981102499533\n",
      "Iteration 190, Training Loss: 0.9337626591918486\n",
      "Iteration 191, Training Loss: 0.9337284505708523\n",
      "Iteration 192, Training Loss: 0.9336952744850572\n",
      "Iteration 193, Training Loss: 0.9336621433121642\n",
      "Iteration 194, Training Loss: 0.9336292716596829\n",
      "Iteration 195, Training Loss: 0.9335975015570066\n",
      "Iteration 196, Training Loss: 0.9335669693560289\n",
      "Iteration 197, Training Loss: 0.9335360756186272\n",
      "Iteration 198, Training Loss: 0.9335061247173772\n",
      "Iteration 199, Training Loss: 0.933475822946236\n",
      "Iteration 200, Training Loss: 0.9334463371600307\n",
      "Iteration 201, Training Loss: 0.9334179449000625\n",
      "Iteration 202, Training Loss: 0.9333887248357466\n",
      "Iteration 203, Training Loss: 0.9333604189562743\n",
      "Iteration 204, Training Loss: 0.9333324377843769\n",
      "Iteration 205, Training Loss: 0.9333050382153892\n",
      "Iteration 206, Training Loss: 0.9332771229241198\n",
      "Iteration 207, Training Loss: 0.9332496270140759\n",
      "Iteration 208, Training Loss: 0.9332224575248885\n",
      "Iteration 209, Training Loss: 0.9331959209092605\n",
      "Iteration 210, Training Loss: 0.9331705340165968\n",
      "Iteration 211, Training Loss: 0.9331452786206608\n",
      "Iteration 212, Training Loss: 0.9331196904803717\n",
      "Iteration 213, Training Loss: 0.933094663254445\n",
      "Iteration 214, Training Loss: 0.933069768506977\n",
      "Iteration 215, Training Loss: 0.9330456240299679\n",
      "Iteration 216, Training Loss: 0.9330205598278494\n",
      "Iteration 217, Training Loss: 0.9329969472857088\n",
      "Iteration 218, Training Loss: 0.9329727276039028\n",
      "Iteration 219, Training Loss: 0.9329489514816358\n",
      "Iteration 220, Training Loss: 0.9329249519285986\n",
      "Iteration 221, Training Loss: 0.9329022345892167\n",
      "Iteration 222, Training Loss: 0.9328783166414418\n",
      "Iteration 223, Training Loss: 0.9328550552726605\n",
      "Iteration 224, Training Loss: 0.9328316710856687\n",
      "Iteration 225, Training Loss: 0.9328089446085946\n",
      "Iteration 226, Training Loss: 0.932785693921504\n",
      "Iteration 227, Training Loss: 0.9327630968003705\n",
      "Iteration 228, Training Loss: 0.9327403247052402\n",
      "Iteration 229, Training Loss: 0.9327185608272036\n",
      "Iteration 230, Training Loss: 0.9326960337391142\n",
      "Iteration 231, Training Loss: 0.932674793842578\n",
      "Iteration 232, Training Loss: 0.9326526636204991\n",
      "Iteration 233, Training Loss: 0.9326312611962067\n",
      "Iteration 234, Training Loss: 0.932610110188101\n",
      "Iteration 235, Training Loss: 0.9325895939883135\n",
      "Iteration 236, Training Loss: 0.9325678374829279\n",
      "Iteration 237, Training Loss: 0.9325468414295135\n",
      "Iteration 238, Training Loss: 0.9325262019261417\n",
      "Iteration 239, Training Loss: 0.9325057521522125\n",
      "Iteration 240, Training Loss: 0.9324853120340458\n",
      "Iteration 241, Training Loss: 0.932465323303788\n",
      "Iteration 242, Training Loss: 0.9324449498939851\n",
      "Iteration 243, Training Loss: 0.9324244883546381\n",
      "Iteration 244, Training Loss: 0.9324052727187894\n",
      "Iteration 245, Training Loss: 0.9323848503391154\n",
      "Iteration 246, Training Loss: 0.9323654204723708\n",
      "Iteration 247, Training Loss: 0.932345420471615\n",
      "Iteration 248, Training Loss: 0.9323264380871429\n",
      "Iteration 249, Training Loss: 0.9323062007957816\n",
      "Iteration 250, Training Loss: 0.9322869695388992\n",
      "Iteration 251, Training Loss: 0.9322678806081777\n",
      "Iteration 252, Training Loss: 0.9322475985167259\n",
      "Iteration 253, Training Loss: 0.932229285918031\n",
      "Iteration 254, Training Loss: 0.932209941099538\n",
      "Iteration 255, Training Loss: 0.9321919839925034\n",
      "Iteration 256, Training Loss: 0.9321732931150041\n",
      "Iteration 257, Training Loss: 0.9321542886501007\n",
      "Iteration 258, Training Loss: 0.9321362124421263\n",
      "Iteration 259, Training Loss: 0.9321173026383173\n",
      "Iteration 260, Training Loss: 0.9320991217943224\n",
      "Iteration 261, Training Loss: 0.932081076892757\n",
      "Iteration 262, Training Loss: 0.9320629294214163\n",
      "Iteration 263, Training Loss: 0.9320446000143708\n",
      "Iteration 264, Training Loss: 0.9320259534091254\n",
      "Iteration 265, Training Loss: 0.9320079508513748\n",
      "Iteration 266, Training Loss: 0.9319906470898724\n",
      "Iteration 267, Training Loss: 0.9319728558173049\n",
      "Iteration 268, Training Loss: 0.9319537462865098\n",
      "Iteration 269, Training Loss: 0.9319366313651378\n",
      "Iteration 270, Training Loss: 0.9319183430260249\n",
      "Iteration 271, Training Loss: 0.9319013781348475\n",
      "Iteration 272, Training Loss: 0.931884261690428\n",
      "Iteration 273, Training Loss: 0.931867398846738\n",
      "Iteration 274, Training Loss: 0.9318503967327831\n",
      "Iteration 275, Training Loss: 0.9318337082755356\n",
      "Iteration 276, Training Loss: 0.9318166602630618\n",
      "Iteration 277, Training Loss: 0.9317997387603769\n",
      "Iteration 278, Training Loss: 0.9317831064839986\n",
      "Iteration 279, Training Loss: 0.9317663340872141\n",
      "Iteration 280, Training Loss: 0.9317502938865025\n",
      "Iteration 281, Training Loss: 0.9317340981803168\n",
      "Iteration 282, Training Loss: 0.9317175174940393\n",
      "Iteration 283, Training Loss: 0.9317016283857134\n",
      "Iteration 284, Training Loss: 0.9316851608009348\n",
      "Iteration 285, Training Loss: 0.9316697261484905\n",
      "Iteration 286, Training Loss: 0.9316533595666701\n",
      "Iteration 287, Training Loss: 0.9316376525013997\n",
      "Iteration 288, Training Loss: 0.9316225985023027\n",
      "Iteration 289, Training Loss: 0.9316066629565728\n",
      "Iteration 290, Training Loss: 0.9315922611255276\n",
      "Iteration 291, Training Loss: 0.9315761919297088\n",
      "Iteration 292, Training Loss: 0.9315614045760118\n",
      "Iteration 293, Training Loss: 0.9315462101281456\n",
      "Iteration 294, Training Loss: 0.9315314602969843\n",
      "Iteration 295, Training Loss: 0.9315156240796876\n",
      "Iteration 296, Training Loss: 0.9315015615871416\n",
      "Iteration 297, Training Loss: 0.9314871228414745\n",
      "Iteration 298, Training Loss: 0.931472664886167\n",
      "Iteration 299, Training Loss: 0.9314589385170182\n",
      "Iteration 300, Training Loss: 0.9314441738215806\n",
      "Iteration 301, Training Loss: 0.9314307594432268\n",
      "Iteration 302, Training Loss: 0.9314158893282417\n",
      "Iteration 303, Training Loss: 0.9314036519884904\n",
      "Iteration 304, Training Loss: 0.931390120836902\n",
      "Iteration 305, Training Loss: 0.9313764028963164\n",
      "Iteration 306, Training Loss: 0.9313634052406519\n",
      "Iteration 307, Training Loss: 0.931351068508038\n",
      "Iteration 308, Training Loss: 0.9313384113343797\n",
      "Iteration 309, Training Loss: 0.9313251940838755\n",
      "Iteration 310, Training Loss: 0.9313126797390922\n",
      "Iteration 311, Training Loss: 0.9312998043103132\n",
      "Iteration 312, Training Loss: 0.9312875998989864\n",
      "Iteration 313, Training Loss: 0.9312745799799722\n",
      "Iteration 314, Training Loss: 0.9312617497504114\n",
      "Iteration 315, Training Loss: 0.9312497139625475\n",
      "Iteration 316, Training Loss: 0.9312369562278702\n",
      "Iteration 317, Training Loss: 0.9312241137569585\n",
      "Iteration 318, Training Loss: 0.9312120314305032\n",
      "Iteration 319, Training Loss: 0.9311991463366606\n",
      "Iteration 320, Training Loss: 0.9311880100440544\n",
      "Iteration 321, Training Loss: 0.9311750148027322\n",
      "Iteration 322, Training Loss: 0.9311635411034022\n",
      "Iteration 323, Training Loss: 0.9311517957162856\n",
      "Iteration 324, Training Loss: 0.9311391912348065\n",
      "Iteration 325, Training Loss: 0.9311272449990473\n",
      "Iteration 326, Training Loss: 0.9311156188269802\n",
      "Iteration 327, Training Loss: 0.9311036945144051\n",
      "Iteration 328, Training Loss: 0.9310912818623767\n",
      "Iteration 329, Training Loss: 0.931079661712113\n",
      "Iteration 330, Training Loss: 0.9310682348076181\n",
      "Iteration 331, Training Loss: 0.9310561360930628\n",
      "Iteration 332, Training Loss: 0.9310452812371598\n",
      "Iteration 333, Training Loss: 0.9310340327980255\n",
      "Iteration 334, Training Loss: 0.9310223451349809\n",
      "Iteration 335, Training Loss: 0.9310121481442166\n",
      "Iteration 336, Training Loss: 0.9310008444182543\n",
      "Iteration 337, Training Loss: 0.9309904454753478\n",
      "Iteration 338, Training Loss: 0.9309791528383111\n",
      "Iteration 339, Training Loss: 0.9309691463803996\n",
      "Iteration 340, Training Loss: 0.9309579354859788\n",
      "Iteration 341, Training Loss: 0.9309476272381656\n",
      "Iteration 342, Training Loss: 0.93093643034392\n",
      "Iteration 343, Training Loss: 0.9309269078977566\n",
      "Iteration 344, Training Loss: 0.930914666204643\n",
      "Iteration 345, Training Loss: 0.9309050179775895\n",
      "Iteration 346, Training Loss: 0.9308948449679799\n",
      "Iteration 347, Training Loss: 0.9308839056642991\n",
      "Iteration 348, Training Loss: 0.9308734227486466\n",
      "Iteration 349, Training Loss: 0.9308637019961973\n",
      "Iteration 350, Training Loss: 0.9308527762095539\n",
      "Iteration 351, Training Loss: 0.9308427309338724\n",
      "Iteration 352, Training Loss: 0.9308321123738799\n",
      "Iteration 353, Training Loss: 0.9308219623453465\n",
      "Iteration 354, Training Loss: 0.9308124662764328\n",
      "Iteration 355, Training Loss: 0.9308026735531664\n",
      "Iteration 356, Training Loss: 0.9307938691499205\n",
      "Iteration 357, Training Loss: 0.9307845533949918\n",
      "Iteration 358, Training Loss: 0.9307758678295598\n",
      "Iteration 359, Training Loss: 0.9307669034032081\n",
      "Iteration 360, Training Loss: 0.9307588749884906\n",
      "Iteration 361, Training Loss: 0.9307492704791215\n",
      "Iteration 362, Training Loss: 0.9307410854065222\n",
      "Iteration 363, Training Loss: 0.9307318818576003\n",
      "Iteration 364, Training Loss: 0.930723310522907\n",
      "Iteration 365, Training Loss: 0.930714794872622\n",
      "Iteration 366, Training Loss: 0.930706231794568\n",
      "Iteration 367, Training Loss: 0.9306971773597724\n",
      "Iteration 368, Training Loss: 0.9306897206875694\n",
      "Iteration 369, Training Loss: 0.9306802721403167\n",
      "Iteration 370, Training Loss: 0.9306731380397535\n",
      "Iteration 371, Training Loss: 0.930663902617637\n",
      "Iteration 372, Training Loss: 0.9306562346534495\n",
      "Iteration 373, Training Loss: 0.9306483399049991\n",
      "Iteration 374, Training Loss: 0.9306390925416768\n",
      "Iteration 375, Training Loss: 0.9306315400591443\n",
      "Iteration 376, Training Loss: 0.93062366988147\n",
      "Iteration 377, Training Loss: 0.9306143003779004\n",
      "Iteration 378, Training Loss: 0.9306070555009573\n",
      "Iteration 379, Training Loss: 0.9305977117722695\n",
      "Iteration 380, Training Loss: 0.930590848483621\n",
      "Iteration 381, Training Loss: 0.930582431091248\n",
      "Iteration 382, Training Loss: 0.9305743707093403\n",
      "Iteration 383, Training Loss: 0.9305658028571363\n",
      "Iteration 384, Training Loss: 0.9305592670150975\n",
      "Iteration 385, Training Loss: 0.9305505107911206\n",
      "Iteration 386, Training Loss: 0.9305435955988328\n",
      "Iteration 387, Training Loss: 0.93053616310858\n",
      "Iteration 388, Training Loss: 0.9305287194748456\n",
      "Iteration 389, Training Loss: 0.9305212866528004\n",
      "Iteration 390, Training Loss: 0.9305142330760784\n",
      "Iteration 391, Training Loss: 0.9305063220489024\n",
      "Iteration 392, Training Loss: 0.9304998446712912\n",
      "Iteration 393, Training Loss: 0.9304911968167219\n",
      "Iteration 394, Training Loss: 0.9304857855729751\n",
      "Iteration 395, Training Loss: 0.930477471920381\n",
      "Iteration 396, Training Loss: 0.9304701837021233\n",
      "Iteration 397, Training Loss: 0.9304630418544941\n",
      "Iteration 398, Training Loss: 0.9304550916139047\n",
      "Iteration 399, Training Loss: 0.9304488495817798\n",
      "Iteration 400, Training Loss: 0.9304412017014536\n",
      "Iteration 401, Training Loss: 0.930434033107381\n",
      "Iteration 402, Training Loss: 0.9304271845993309\n",
      "Iteration 403, Training Loss: 0.9304190549711304\n",
      "Iteration 404, Training Loss: 0.9304135758205369\n",
      "Iteration 405, Training Loss: 0.9304057232157961\n",
      "Iteration 406, Training Loss: 0.9303980703911067\n",
      "Iteration 407, Training Loss: 0.9303908912373008\n",
      "Iteration 408, Training Loss: 0.930384424744984\n",
      "Iteration 409, Training Loss: 0.9303773746726686\n",
      "Iteration 410, Training Loss: 0.9303697108855217\n",
      "Iteration 411, Training Loss: 0.9303630983176631\n",
      "Iteration 412, Training Loss: 0.9303563407464569\n",
      "Iteration 413, Training Loss: 0.9303478906210654\n",
      "Iteration 414, Training Loss: 0.9303427860361813\n",
      "Iteration 415, Training Loss: 0.9303347954412791\n",
      "Iteration 416, Training Loss: 0.9303282561914888\n",
      "Iteration 417, Training Loss: 0.9303207964745672\n",
      "Iteration 418, Training Loss: 0.9303135972229063\n",
      "Iteration 419, Training Loss: 0.930308052793683\n",
      "Iteration 420, Training Loss: 0.9302984536825115\n",
      "Iteration 421, Training Loss: 0.9302942731310881\n",
      "Iteration 422, Training Loss: 0.9302848523407559\n",
      "Iteration 423, Training Loss: 0.9302786407463021\n",
      "Iteration 424, Training Loss: 0.9302705822923552\n",
      "Iteration 425, Training Loss: 0.9302653723479998\n",
      "Iteration 426, Training Loss: 0.9302579067285344\n",
      "Iteration 427, Training Loss: 0.9302503605501272\n",
      "Iteration 428, Training Loss: 0.9302434987532833\n",
      "Iteration 429, Training Loss: 0.9302378003517812\n",
      "Iteration 430, Training Loss: 0.930229851137216\n",
      "Iteration 431, Training Loss: 0.9302240765443092\n",
      "Iteration 432, Training Loss: 0.9302161851262125\n",
      "Iteration 433, Training Loss: 0.9302106169837349\n",
      "Iteration 434, Training Loss: 0.9302037519789722\n",
      "Iteration 435, Training Loss: 0.9301958173058454\n",
      "Iteration 436, Training Loss: 0.9301918499919462\n",
      "Iteration 437, Training Loss: 0.9301830618157416\n",
      "Iteration 438, Training Loss: 0.9301776570689031\n",
      "Iteration 439, Training Loss: 0.9301699567051492\n",
      "Iteration 440, Training Loss: 0.9301645728687907\n",
      "Iteration 441, Training Loss: 0.9301568755907133\n",
      "Iteration 442, Training Loss: 0.9301518189607311\n",
      "Iteration 443, Training Loss: 0.9301439456479991\n",
      "Iteration 444, Training Loss: 0.9301389108848018\n",
      "Iteration 445, Training Loss: 0.9301333530363362\n",
      "Iteration 446, Training Loss: 0.9301260877669031\n",
      "Iteration 447, Training Loss: 0.9301194998095439\n",
      "Iteration 448, Training Loss: 0.9301132713540485\n",
      "Iteration 449, Training Loss: 0.9301080462052859\n",
      "Iteration 450, Training Loss: 0.9301012455924237\n",
      "Iteration 451, Training Loss: 0.9300941850580604\n",
      "Iteration 452, Training Loss: 0.9300899049546488\n",
      "Iteration 453, Training Loss: 0.9300818370435098\n",
      "Iteration 454, Training Loss: 0.9300770032118865\n",
      "Iteration 455, Training Loss: 0.9300704988167338\n",
      "Iteration 456, Training Loss: 0.930064476792288\n",
      "Iteration 457, Training Loss: 0.9300586465000492\n",
      "Iteration 458, Training Loss: 0.9300511015890015\n",
      "Iteration 459, Training Loss: 0.9300471788873336\n",
      "Iteration 460, Training Loss: 0.9300396619543219\n",
      "Iteration 461, Training Loss: 0.9300351848771777\n",
      "Iteration 462, Training Loss: 0.9300270469530771\n",
      "Iteration 463, Training Loss: 0.9300222934762685\n",
      "Iteration 464, Training Loss: 0.9300162734769649\n",
      "Iteration 465, Training Loss: 0.9300110006656244\n",
      "Iteration 466, Training Loss: 0.9300032592902685\n",
      "Iteration 467, Training Loss: 0.9299999128526076\n",
      "Iteration 468, Training Loss: 0.9299914604783582\n",
      "Iteration 469, Training Loss: 0.9299870980789958\n",
      "Iteration 470, Training Loss: 0.9299806397254439\n",
      "Iteration 471, Training Loss: 0.9299738983542785\n",
      "Iteration 472, Training Loss: 0.9299688904265195\n",
      "Iteration 473, Training Loss: 0.9299624236321672\n",
      "Iteration 474, Training Loss: 0.9299576718980366\n",
      "Iteration 475, Training Loss: 0.9299515750867052\n",
      "Iteration 476, Training Loss: 0.9299450853305966\n",
      "Iteration 477, Training Loss: 0.9299396932596908\n",
      "Iteration 478, Training Loss: 0.9299333444929002\n",
      "Iteration 479, Training Loss: 0.9299289701778964\n",
      "Iteration 480, Training Loss: 0.9299213128894526\n",
      "Iteration 481, Training Loss: 0.9299165158768412\n",
      "Iteration 482, Training Loss: 0.9299101916754134\n",
      "Iteration 483, Training Loss: 0.9299051938557107\n",
      "Iteration 484, Training Loss: 0.9298990172860813\n",
      "Iteration 485, Training Loss: 0.9298939514689286\n",
      "Iteration 486, Training Loss: 0.9298870161511716\n",
      "Iteration 487, Training Loss: 0.9298833719181162\n",
      "Iteration 488, Training Loss: 0.9298759882537472\n",
      "Iteration 489, Training Loss: 0.9298726596996832\n",
      "Iteration 490, Training Loss: 0.9298648169273218\n",
      "Iteration 491, Training Loss: 0.9298610516146917\n",
      "Iteration 492, Training Loss: 0.9298547520022653\n",
      "Iteration 493, Training Loss: 0.9298501509400869\n",
      "Iteration 494, Training Loss: 0.9298436502780087\n",
      "Iteration 495, Training Loss: 0.9298387087061311\n",
      "Iteration 496, Training Loss: 0.9298336544941034\n",
      "Iteration 497, Training Loss: 0.9298277954646544\n",
      "Iteration 498, Training Loss: 0.9298242393740935\n",
      "Iteration 499, Training Loss: 0.9298177227506146\n",
      "Iteration 500, Training Loss: 0.9298119547359267\n",
      "Iteration 501, Training Loss: 0.9298079070422857\n",
      "Iteration 502, Training Loss: 0.9298013206407354\n",
      "Iteration 503, Training Loss: 0.9297975647623927\n",
      "Iteration 504, Training Loss: 0.9297922565350024\n",
      "Iteration 505, Training Loss: 0.9297875275412357\n",
      "Iteration 506, Training Loss: 0.9297827607916777\n",
      "Iteration 507, Training Loss: 0.9297771945001809\n",
      "Iteration 508, Training Loss: 0.9297725915478352\n",
      "Iteration 509, Training Loss: 0.9297676978298788\n",
      "Iteration 510, Training Loss: 0.9297634045118047\n",
      "Iteration 511, Training Loss: 0.9297580467817251\n",
      "Iteration 512, Training Loss: 0.9297526119293971\n",
      "Iteration 513, Training Loss: 0.9297494421315503\n",
      "Iteration 514, Training Loss: 0.929742219379588\n",
      "Iteration 515, Training Loss: 0.9297394100705433\n",
      "Iteration 516, Training Loss: 0.9297327086101563\n",
      "Iteration 517, Training Loss: 0.9297285821185682\n",
      "Iteration 518, Training Loss: 0.9297247630101481\n",
      "Iteration 519, Training Loss: 0.9297170979189537\n",
      "Iteration 520, Training Loss: 0.9297150004265\n",
      "Iteration 521, Training Loss: 0.9297092399146039\n",
      "Iteration 522, Training Loss: 0.9297050688883698\n",
      "Iteration 523, Training Loss: 0.9296996565110361\n",
      "Iteration 524, Training Loss: 0.9296962270782504\n",
      "Iteration 525, Training Loss: 0.9296904385138248\n",
      "Iteration 526, Training Loss: 0.9296853850453458\n",
      "Iteration 527, Training Loss: 0.9296801624767791\n",
      "Iteration 528, Training Loss: 0.9296772780734706\n",
      "Iteration 529, Training Loss: 0.9296707828196763\n",
      "Iteration 530, Training Loss: 0.9296667451043836\n",
      "Iteration 531, Training Loss: 0.9296612944313412\n",
      "Iteration 532, Training Loss: 0.9296569311315795\n",
      "Iteration 533, Training Loss: 0.9296529471191531\n",
      "Iteration 534, Training Loss: 0.929648742666724\n",
      "Iteration 535, Training Loss: 0.9296433360523564\n",
      "Iteration 536, Training Loss: 0.9296390551352415\n",
      "Iteration 537, Training Loss: 0.9296345794692396\n",
      "Iteration 538, Training Loss: 0.9296303074122408\n",
      "Iteration 539, Training Loss: 0.9296248015683687\n",
      "Iteration 540, Training Loss: 0.9296219023915672\n",
      "Iteration 541, Training Loss: 0.9296167559519353\n",
      "Iteration 542, Training Loss: 0.9296124749127868\n",
      "Iteration 543, Training Loss: 0.9296067446415385\n",
      "Iteration 544, Training Loss: 0.9296051233072284\n",
      "Iteration 545, Training Loss: 0.9295993326976495\n",
      "Iteration 546, Training Loss: 0.9295951021248164\n",
      "Iteration 547, Training Loss: 0.9295904359106937\n",
      "Iteration 548, Training Loss: 0.9295868441275629\n",
      "Iteration 549, Training Loss: 0.9295825019349786\n",
      "Iteration 550, Training Loss: 0.9295773185038884\n",
      "Iteration 551, Training Loss: 0.9295732718284714\n",
      "Iteration 552, Training Loss: 0.9295693025838203\n",
      "Iteration 553, Training Loss: 0.9295650046387013\n",
      "Iteration 554, Training Loss: 0.929559472719267\n",
      "Iteration 555, Training Loss: 0.9295574082891492\n",
      "Iteration 556, Training Loss: 0.9295501762691835\n",
      "Iteration 557, Training Loss: 0.9295488901298838\n",
      "Iteration 558, Training Loss: 0.9295431925879909\n",
      "Iteration 559, Training Loss: 0.9295392200816598\n",
      "Iteration 560, Training Loss: 0.9295348752479183\n",
      "Iteration 561, Training Loss: 0.9295318696678139\n",
      "Iteration 562, Training Loss: 0.9295268553828607\n",
      "Iteration 563, Training Loss: 0.9295231144066834\n",
      "Iteration 564, Training Loss: 0.9295178558076511\n",
      "Iteration 565, Training Loss: 0.9295157163592153\n",
      "Iteration 566, Training Loss: 0.9295101014126176\n",
      "Iteration 567, Training Loss: 0.9295072279792834\n",
      "Iteration 568, Training Loss: 0.9295014885193364\n",
      "Iteration 569, Training Loss: 0.9294989106866977\n",
      "Iteration 570, Training Loss: 0.9294952358949705\n",
      "Iteration 571, Training Loss: 0.9294902409273841\n",
      "Iteration 572, Training Loss: 0.9294872306683774\n",
      "Iteration 573, Training Loss: 0.9294828390880746\n",
      "Iteration 574, Training Loss: 0.9294792428411865\n",
      "Iteration 575, Training Loss: 0.9294741185452212\n",
      "Iteration 576, Training Loss: 0.9294719111517218\n",
      "Iteration 577, Training Loss: 0.9294662700207129\n",
      "Iteration 578, Training Loss: 0.9294650774892682\n",
      "Iteration 579, Training Loss: 0.929458899952369\n",
      "Iteration 580, Training Loss: 0.9294558299688851\n",
      "Iteration 581, Training Loss: 0.9294520385684011\n",
      "Iteration 582, Training Loss: 0.929448282067489\n",
      "Iteration 583, Training Loss: 0.9294440839137662\n",
      "Iteration 584, Training Loss: 0.9294393131869532\n",
      "Iteration 585, Training Loss: 0.9294374199137203\n",
      "Iteration 586, Training Loss: 0.9294336954796895\n",
      "Iteration 587, Training Loss: 0.9294279234980989\n",
      "Iteration 588, Training Loss: 0.9294272608834226\n",
      "Iteration 589, Training Loss: 0.9294193366644117\n",
      "Iteration 590, Training Loss: 0.9294192054987473\n",
      "Iteration 591, Training Loss: 0.9294139968841555\n",
      "Iteration 592, Training Loss: 0.9294116256803454\n",
      "Iteration 593, Training Loss: 0.9294057910145663\n",
      "Iteration 594, Training Loss: 0.9294049255323198\n",
      "Iteration 595, Training Loss: 0.9293984641569341\n",
      "Iteration 596, Training Loss: 0.9293977369064558\n",
      "Iteration 597, Training Loss: 0.9293913059981745\n",
      "Iteration 598, Training Loss: 0.9293887653408541\n",
      "Iteration 599, Training Loss: 0.9293853265618383\n",
      "Iteration 600, Training Loss: 0.9293811388003028\n",
      "Iteration 601, Training Loss: 0.9293779564390163\n",
      "Iteration 602, Training Loss: 0.929374188198712\n",
      "Iteration 603, Training Loss: 0.9293691204627061\n",
      "Iteration 604, Training Loss: 0.9293673865817995\n",
      "Iteration 605, Training Loss: 0.9293633552532183\n",
      "Iteration 606, Training Loss: 0.9293596852257958\n",
      "Iteration 607, Training Loss: 0.9293578241010743\n",
      "Iteration 608, Training Loss: 0.9293518325050291\n",
      "Iteration 609, Training Loss: 0.9293506741657267\n",
      "Iteration 610, Training Loss: 0.9293453284584425\n",
      "Iteration 611, Training Loss: 0.9293428237110436\n",
      "Iteration 612, Training Loss: 0.9293374917154418\n",
      "Iteration 613, Training Loss: 0.9293348842972087\n",
      "Iteration 614, Training Loss: 0.9293303053302253\n",
      "Iteration 615, Training Loss: 0.9293303765774596\n",
      "Iteration 616, Training Loss: 0.9293230637441764\n",
      "Iteration 617, Training Loss: 0.9293217708613037\n",
      "Iteration 618, Training Loss: 0.9293177102005896\n",
      "Iteration 619, Training Loss: 0.9293138890484026\n",
      "Iteration 620, Training Loss: 0.929310646997148\n",
      "Iteration 621, Training Loss: 0.9293082296450857\n",
      "Iteration 622, Training Loss: 0.9293036126048301\n",
      "Iteration 623, Training Loss: 0.9293003969435087\n",
      "Iteration 624, Training Loss: 0.9292972274632494\n",
      "Iteration 625, Training Loss: 0.9292932846212971\n",
      "Iteration 626, Training Loss: 0.9292917097228242\n",
      "Iteration 627, Training Loss: 0.9292859824455881\n",
      "Iteration 628, Training Loss: 0.9292839230905949\n",
      "Iteration 629, Training Loss: 0.9292787420295799\n",
      "Iteration 630, Training Loss: 0.9292782913390545\n",
      "Iteration 631, Training Loss: 0.9292723776624375\n",
      "Iteration 632, Training Loss: 0.9292707664800941\n",
      "Iteration 633, Training Loss: 0.9292667553447518\n",
      "Iteration 634, Training Loss: 0.9292637151596442\n",
      "Iteration 635, Training Loss: 0.929260006630486\n",
      "Iteration 636, Training Loss: 0.9292574815603882\n",
      "Iteration 637, Training Loss: 0.9292530416533191\n",
      "Iteration 638, Training Loss: 0.92925162947381\n",
      "Iteration 639, Training Loss: 0.9292452825995065\n",
      "Iteration 640, Training Loss: 0.9292452701947393\n",
      "Iteration 641, Training Loss: 0.9292399782173293\n",
      "Iteration 642, Training Loss: 0.9292370262497546\n",
      "Iteration 643, Training Loss: 0.9292331815111918\n",
      "Iteration 644, Training Loss: 0.9292320040724956\n",
      "Iteration 645, Training Loss: 0.9292270278725269\n",
      "Iteration 646, Training Loss: 0.9292249713930519\n",
      "Iteration 647, Training Loss: 0.9292217020855478\n",
      "Iteration 648, Training Loss: 0.9292169298564348\n",
      "Iteration 649, Training Loss: 0.9292151184944049\n",
      "Iteration 650, Training Loss: 0.9292113606894022\n",
      "Iteration 651, Training Loss: 0.9292079289340793\n",
      "Iteration 652, Training Loss: 0.9292059679122665\n",
      "Iteration 653, Training Loss: 0.9292012567434597\n",
      "Iteration 654, Training Loss: 0.9291993283499687\n",
      "Iteration 655, Training Loss: 0.9291945019941067\n",
      "Iteration 656, Training Loss: 0.9291929086784756\n",
      "Iteration 657, Training Loss: 0.9291893450768831\n",
      "Iteration 658, Training Loss: 0.9291851819159415\n",
      "Iteration 659, Training Loss: 0.9291835979953919\n",
      "Iteration 660, Training Loss: 0.9291786218778008\n",
      "Iteration 661, Training Loss: 0.9291777270907811\n",
      "Iteration 662, Training Loss: 0.9291731841975259\n",
      "Iteration 663, Training Loss: 0.9291710492949912\n",
      "Iteration 664, Training Loss: 0.9291673032166673\n",
      "Iteration 665, Training Loss: 0.9291657228013612\n",
      "Iteration 666, Training Loss: 0.929161623291577\n",
      "Iteration 667, Training Loss: 0.9291580466432678\n",
      "Iteration 668, Training Loss: 0.9291571559592741\n",
      "Iteration 669, Training Loss: 0.9291514150178659\n",
      "Iteration 670, Training Loss: 0.9291515141757681\n",
      "Iteration 671, Training Loss: 0.9291474655475331\n",
      "Iteration 672, Training Loss: 0.9291459004343037\n",
      "Iteration 673, Training Loss: 0.9291412480566219\n",
      "Iteration 674, Training Loss: 0.9291395691005968\n",
      "Iteration 675, Training Loss: 0.9291340725095112\n",
      "Iteration 676, Training Loss: 0.9291350796873349\n",
      "Iteration 677, Training Loss: 0.9291295277102851\n",
      "Iteration 678, Training Loss: 0.9291273870246785\n",
      "Iteration 679, Training Loss: 0.9291236442901717\n",
      "Iteration 680, Training Loss: 0.9291227717770528\n",
      "Iteration 681, Training Loss: 0.9291180044031289\n",
      "Iteration 682, Training Loss: 0.9291168050159638\n",
      "Iteration 683, Training Loss: 0.9291133423095975\n",
      "Iteration 684, Training Loss: 0.9291113482443818\n",
      "Iteration 685, Training Loss: 0.9291070243673084\n",
      "Iteration 686, Training Loss: 0.929106163702174\n",
      "Iteration 687, Training Loss: 0.9291022014334215\n",
      "Iteration 688, Training Loss: 0.9290999215537177\n",
      "Iteration 689, Training Loss: 0.9290965075250694\n",
      "Iteration 690, Training Loss: 0.9290932735060224\n",
      "Iteration 691, Training Loss: 0.9290917064848443\n",
      "Iteration 692, Training Loss: 0.9290891302593375\n",
      "Iteration 693, Training Loss: 0.9290854692838078\n",
      "Iteration 694, Training Loss: 0.9290833156241417\n",
      "Iteration 695, Training Loss: 0.9290812210497984\n",
      "Iteration 696, Training Loss: 0.9290780755879836\n",
      "Iteration 697, Training Loss: 0.9290748472036775\n",
      "Iteration 698, Training Loss: 0.9290732596789005\n",
      "Iteration 699, Training Loss: 0.9290686581881139\n",
      "Iteration 700, Training Loss: 0.9290698461729232\n",
      "Iteration 701, Training Loss: 0.9290639573631608\n",
      "Iteration 702, Training Loss: 0.9290615124275082\n",
      "Iteration 703, Training Loss: 0.9290587327053403\n",
      "Iteration 704, Training Loss: 0.9290570000915848\n",
      "Iteration 705, Training Loss: 0.9290539610788435\n",
      "Iteration 706, Training Loss: 0.9290522364927605\n",
      "Iteration 707, Training Loss: 0.9290490842430087\n",
      "Iteration 708, Training Loss: 0.9290463019324204\n",
      "Iteration 709, Training Loss: 0.9290439645873715\n",
      "Iteration 710, Training Loss: 0.9290412041718257\n",
      "Iteration 711, Training Loss: 0.9290389006531491\n",
      "Iteration 712, Training Loss: 0.9290355514555187\n",
      "Iteration 713, Training Loss: 0.9290341657854356\n",
      "Iteration 714, Training Loss: 0.929029970432231\n",
      "Iteration 715, Training Loss: 0.9290298558098399\n",
      "Iteration 716, Training Loss: 0.9290254079289197\n",
      "Iteration 717, Training Loss: 0.9290237382460622\n",
      "Iteration 718, Training Loss: 0.9290203779534262\n",
      "Iteration 719, Training Loss: 0.9290180711456731\n",
      "Iteration 720, Training Loss: 0.9290167508773505\n",
      "Iteration 721, Training Loss: 0.9290121510049019\n",
      "Iteration 722, Training Loss: 0.9290114229790642\n",
      "Iteration 723, Training Loss: 0.9290072778424822\n",
      "Iteration 724, Training Loss: 0.9290064067890632\n",
      "Iteration 725, Training Loss: 0.9290022599471613\n",
      "Iteration 726, Training Loss: 0.9290023216260695\n",
      "Iteration 727, Training Loss: 0.9289976265639714\n",
      "Iteration 728, Training Loss: 0.9289966389045672\n",
      "Iteration 729, Training Loss: 0.9289916010662924\n",
      "Iteration 730, Training Loss: 0.9289926411171348\n",
      "Iteration 731, Training Loss: 0.9289861463743251\n",
      "Iteration 732, Training Loss: 0.9289860787733403\n",
      "Iteration 733, Training Loss: 0.9289827849182968\n",
      "Iteration 734, Training Loss: 0.9289800623300261\n",
      "Iteration 735, Training Loss: 0.9289788904604033\n",
      "Iteration 736, Training Loss: 0.928975936787763\n",
      "Iteration 737, Training Loss: 0.928972798012235\n",
      "Iteration 738, Training Loss: 0.928971419940922\n",
      "Iteration 739, Training Loss: 0.9289678521157632\n",
      "Iteration 740, Training Loss: 0.9289680917703761\n",
      "Iteration 741, Training Loss: 0.9289631815116738\n",
      "Iteration 742, Training Loss: 0.9289616171825641\n",
      "Iteration 743, Training Loss: 0.9289576948454158\n",
      "Iteration 744, Training Loss: 0.9289574040048558\n",
      "Iteration 745, Training Loss: 0.9289518211335358\n",
      "Iteration 746, Training Loss: 0.9289519780679579\n",
      "Iteration 747, Training Loss: 0.9289498407751994\n",
      "Iteration 748, Training Loss: 0.9289460591038475\n",
      "Iteration 749, Training Loss: 0.9289451488773731\n",
      "Iteration 750, Training Loss: 0.9289409020244509\n",
      "Iteration 751, Training Loss: 0.928941122029582\n",
      "Iteration 752, Training Loss: 0.928934887244143\n",
      "Iteration 753, Training Loss: 0.9289354942265257\n",
      "Iteration 754, Training Loss: 0.9289321055072756\n",
      "Iteration 755, Training Loss: 0.9289299624096852\n",
      "Iteration 756, Training Loss: 0.9289274485276492\n",
      "Iteration 757, Training Loss: 0.9289256145967677\n",
      "Iteration 758, Training Loss: 0.9289223757987674\n",
      "Iteration 759, Training Loss: 0.9289197422311861\n",
      "Iteration 760, Training Loss: 0.9289182459084834\n",
      "Iteration 761, Training Loss: 0.9289149397015306\n",
      "Iteration 762, Training Loss: 0.9289137209154564\n",
      "Iteration 763, Training Loss: 0.928910532941642\n",
      "Iteration 764, Training Loss: 0.9289100071967022\n",
      "Iteration 765, Training Loss: 0.9289059652168444\n",
      "Iteration 766, Training Loss: 0.928904569979353\n",
      "Iteration 767, Training Loss: 0.928901128333331\n",
      "Iteration 768, Training Loss: 0.9288996331125836\n",
      "Iteration 769, Training Loss: 0.9288982241571145\n",
      "Iteration 770, Training Loss: 0.9288934354671221\n",
      "Iteration 771, Training Loss: 0.928895326441638\n",
      "Iteration 772, Training Loss: 0.9288902247773804\n",
      "Iteration 773, Training Loss: 0.9288891958138965\n",
      "Iteration 774, Training Loss: 0.9288863747659506\n",
      "Iteration 775, Training Loss: 0.928884255295776\n",
      "Iteration 776, Training Loss: 0.9288820207064994\n",
      "Iteration 777, Training Loss: 0.9288802757133164\n",
      "Iteration 778, Training Loss: 0.9288774178804071\n",
      "Iteration 779, Training Loss: 0.9288758192316531\n",
      "Iteration 780, Training Loss: 0.9288734404576111\n",
      "Iteration 781, Training Loss: 0.9288738357575496\n",
      "Iteration 782, Training Loss: 0.928868044093424\n",
      "Iteration 783, Training Loss: 0.9288682854931968\n",
      "Iteration 784, Training Loss: 0.9288645794691639\n",
      "Iteration 785, Training Loss: 0.9288643704594992\n",
      "Iteration 786, Training Loss: 0.9288610107368356\n",
      "Iteration 787, Training Loss: 0.9288589261068971\n",
      "Iteration 788, Training Loss: 0.9288570957270388\n",
      "Iteration 789, Training Loss: 0.9288556314840338\n",
      "Iteration 790, Training Loss: 0.9288523341998307\n",
      "Iteration 791, Training Loss: 0.928850349559978\n",
      "Iteration 792, Training Loss: 0.928848408405011\n",
      "Iteration 793, Training Loss: 0.9288466836240457\n",
      "Iteration 794, Training Loss: 0.9288434343064215\n",
      "Iteration 795, Training Loss: 0.928843383937751\n",
      "Iteration 796, Training Loss: 0.9288403446317907\n",
      "Iteration 797, Training Loss: 0.9288394160819378\n",
      "Iteration 798, Training Loss: 0.9288350570924604\n",
      "Iteration 799, Training Loss: 0.928834229162128\n",
      "Iteration 800, Training Loss: 0.9288321576618435\n",
      "Iteration 801, Training Loss: 0.928831142663192\n",
      "Iteration 802, Training Loss: 0.9288276611502192\n",
      "Iteration 803, Training Loss: 0.9288278977411515\n",
      "Iteration 804, Training Loss: 0.9288232647002079\n",
      "Iteration 805, Training Loss: 0.9288226051390889\n",
      "Iteration 806, Training Loss: 0.9288203106906903\n",
      "Iteration 807, Training Loss: 0.9288201357159959\n",
      "Iteration 808, Training Loss: 0.9288164766627695\n",
      "Iteration 809, Training Loss: 0.9288156393282282\n",
      "Iteration 810, Training Loss: 0.9288120601853309\n",
      "Iteration 811, Training Loss: 0.9288102647332753\n",
      "Iteration 812, Training Loss: 0.9288088857496049\n",
      "Iteration 813, Training Loss: 0.928805913209979\n",
      "Iteration 814, Training Loss: 0.9288051589344782\n",
      "Iteration 815, Training Loss: 0.9288029587643631\n",
      "Iteration 816, Training Loss: 0.9288011766211897\n",
      "Iteration 817, Training Loss: 0.9287981946516486\n",
      "Iteration 818, Training Loss: 0.9287987713623211\n",
      "Iteration 819, Training Loss: 0.9287935006232728\n",
      "Iteration 820, Training Loss: 0.9287944252950149\n",
      "Iteration 821, Training Loss: 0.9287914092013418\n",
      "Iteration 822, Training Loss: 0.9287896269180284\n",
      "Iteration 823, Training Loss: 0.9287876767191483\n",
      "Iteration 824, Training Loss: 0.9287856060234303\n",
      "Iteration 825, Training Loss: 0.9287855455823522\n",
      "Iteration 826, Training Loss: 0.9287815024296042\n",
      "Iteration 827, Training Loss: 0.9287810932543354\n",
      "Iteration 828, Training Loss: 0.9287783691623729\n",
      "Iteration 829, Training Loss: 0.9287775785114727\n",
      "Iteration 830, Training Loss: 0.9287737932143924\n",
      "Iteration 831, Training Loss: 0.9287736086753464\n",
      "Iteration 832, Training Loss: 0.9287706121185636\n",
      "Iteration 833, Training Loss: 0.9287697362882462\n",
      "Iteration 834, Training Loss: 0.9287666776471675\n",
      "Iteration 835, Training Loss: 0.9287674871801361\n",
      "Iteration 836, Training Loss: 0.9287622716499133\n",
      "Iteration 837, Training Loss: 0.9287631895157507\n",
      "Iteration 838, Training Loss: 0.9287592266628515\n",
      "Iteration 839, Training Loss: 0.9287607741933765\n",
      "Iteration 840, Training Loss: 0.9287553700195956\n",
      "Iteration 841, Training Loss: 0.9287553825111597\n",
      "Iteration 842, Training Loss: 0.9287523832694317\n",
      "Iteration 843, Training Loss: 0.9287508847715935\n",
      "Iteration 844, Training Loss: 0.9287490050114077\n",
      "Iteration 845, Training Loss: 0.9287487893874952\n",
      "Iteration 846, Training Loss: 0.9287460439358497\n",
      "Iteration 847, Training Loss: 0.9287446736370844\n",
      "Iteration 848, Training Loss: 0.9287426170145137\n",
      "Iteration 849, Training Loss: 0.9287417489117076\n",
      "Iteration 850, Training Loss: 0.9287380430846818\n",
      "Iteration 851, Training Loss: 0.928737530384588\n",
      "Iteration 852, Training Loss: 0.9287358409527525\n",
      "Iteration 853, Training Loss: 0.92873526385116\n",
      "Iteration 854, Training Loss: 0.9287311039810816\n",
      "Iteration 855, Training Loss: 0.9287307507296876\n",
      "Iteration 856, Training Loss: 0.9287280011735489\n",
      "Iteration 857, Training Loss: 0.9287290296295826\n",
      "Iteration 858, Training Loss: 0.928724105189917\n",
      "Iteration 859, Training Loss: 0.9287241440418584\n",
      "Iteration 860, Training Loss: 0.9287211783821036\n",
      "Iteration 861, Training Loss: 0.9287186718279188\n",
      "Iteration 862, Training Loss: 0.9287198829465897\n",
      "Iteration 863, Training Loss: 0.9287153535121967\n",
      "Iteration 864, Training Loss: 0.9287148770175395\n",
      "Iteration 865, Training Loss: 0.9287130981446329\n",
      "Iteration 866, Training Loss: 0.9287107044503454\n",
      "Iteration 867, Training Loss: 0.9287118175567864\n",
      "Iteration 868, Training Loss: 0.9287077665672842\n",
      "Iteration 869, Training Loss: 0.9287080057353763\n",
      "Iteration 870, Training Loss: 0.9287029327547739\n",
      "Iteration 871, Training Loss: 0.9287050288885231\n",
      "Iteration 872, Training Loss: 0.9286997353667666\n",
      "Iteration 873, Training Loss: 0.9286996440869077\n",
      "Iteration 874, Training Loss: 0.9286981291239351\n",
      "Iteration 875, Training Loss: 0.9286962164304129\n",
      "Iteration 876, Training Loss: 0.9286947263451905\n",
      "Iteration 877, Training Loss: 0.9286936309073218\n",
      "Iteration 878, Training Loss: 0.9286906730682175\n",
      "Iteration 879, Training Loss: 0.9286902773775569\n",
      "Iteration 880, Training Loss: 0.9286882797371409\n",
      "Iteration 881, Training Loss: 0.9286868827289343\n",
      "Iteration 882, Training Loss: 0.928684566625002\n",
      "Iteration 883, Training Loss: 0.9286842322352017\n",
      "Iteration 884, Training Loss: 0.9286823303380756\n",
      "Iteration 885, Training Loss: 0.9286800394733559\n",
      "Iteration 886, Training Loss: 0.9286787696795175\n",
      "Iteration 887, Training Loss: 0.9286779466261185\n",
      "Iteration 888, Training Loss: 0.9286747812946534\n",
      "Iteration 889, Training Loss: 0.9286744545491888\n",
      "Iteration 890, Training Loss: 0.9286700465580232\n",
      "Iteration 891, Training Loss: 0.9286708182119492\n",
      "Iteration 892, Training Loss: 0.9286682691530652\n",
      "Iteration 893, Training Loss: 0.9286678941848016\n",
      "Iteration 894, Training Loss: 0.9286654402525235\n",
      "Iteration 895, Training Loss: 0.9286641862732502\n",
      "Iteration 896, Training Loss: 0.9286624902452731\n",
      "Iteration 897, Training Loss: 0.9286599263330332\n",
      "Iteration 898, Training Loss: 0.9286605394814799\n",
      "Iteration 899, Training Loss: 0.9286571276064243\n",
      "Iteration 900, Training Loss: 0.9286577888838692\n",
      "Iteration 901, Training Loss: 0.928653886172346\n",
      "Iteration 902, Training Loss: 0.928653455441256\n",
      "Iteration 903, Training Loss: 0.9286528997621708\n",
      "Iteration 904, Training Loss: 0.9286482722250259\n",
      "Iteration 905, Training Loss: 0.928649381209819\n",
      "Iteration 906, Training Loss: 0.9286470544929512\n",
      "Iteration 907, Training Loss: 0.9286448361298645\n",
      "Iteration 908, Training Loss: 0.9286455887435706\n",
      "Iteration 909, Training Loss: 0.9286404706541198\n",
      "Iteration 910, Training Loss: 0.9286423850084606\n",
      "Iteration 911, Training Loss: 0.9286391873835302\n",
      "Iteration 912, Training Loss: 0.9286394187026629\n",
      "Iteration 913, Training Loss: 0.92863674320044\n",
      "Iteration 914, Training Loss: 0.9286349341697762\n",
      "Iteration 915, Training Loss: 0.9286344043373079\n",
      "Iteration 916, Training Loss: 0.9286313742212067\n",
      "Iteration 917, Training Loss: 0.9286325957264603\n",
      "Iteration 918, Training Loss: 0.9286275770632828\n",
      "Iteration 919, Training Loss: 0.9286300637544552\n",
      "Iteration 920, Training Loss: 0.9286244476802407\n",
      "Iteration 921, Training Loss: 0.9286253741923435\n",
      "Iteration 922, Training Loss: 0.9286223431789038\n",
      "Iteration 923, Training Loss: 0.928623424584993\n",
      "Iteration 924, Training Loss: 0.9286197546091832\n",
      "Iteration 925, Training Loss: 0.9286190438603006\n",
      "Iteration 926, Training Loss: 0.9286178138737625\n",
      "Iteration 927, Training Loss: 0.9286163786016678\n",
      "Iteration 928, Training Loss: 0.9286131723959647\n",
      "Iteration 929, Training Loss: 0.9286137203333953\n",
      "Iteration 930, Training Loss: 0.9286122270116566\n",
      "Iteration 931, Training Loss: 0.9286100011737558\n",
      "Iteration 932, Training Loss: 0.9286088058388774\n",
      "Iteration 933, Training Loss: 0.9286078879423642\n",
      "Iteration 934, Training Loss: 0.9286058803459617\n",
      "Iteration 935, Training Loss: 0.9286052199194826\n",
      "Iteration 936, Training Loss: 0.9286023113278682\n",
      "Iteration 937, Training Loss: 0.9286037627394484\n",
      "Iteration 938, Training Loss: 0.928598943643874\n",
      "Iteration 939, Training Loss: 0.9286007300763549\n",
      "Iteration 940, Training Loss: 0.9285978838811606\n",
      "Iteration 941, Training Loss: 0.9285960444638035\n",
      "Iteration 942, Training Loss: 0.9285944845571462\n",
      "Iteration 943, Training Loss: 0.9285939171130967\n",
      "Iteration 944, Training Loss: 0.9285927063323375\n",
      "Iteration 945, Training Loss: 0.9285917470096693\n",
      "Iteration 946, Training Loss: 0.9285887081533282\n",
      "Iteration 947, Training Loss: 0.9285885206547932\n",
      "Iteration 948, Training Loss: 0.9285867054668925\n",
      "Iteration 949, Training Loss: 0.9285854129155425\n",
      "Iteration 950, Training Loss: 0.9285850017507169\n",
      "Iteration 951, Training Loss: 0.9285823829981432\n",
      "Iteration 952, Training Loss: 0.9285821466036684\n",
      "Iteration 953, Training Loss: 0.9285796279660872\n",
      "Iteration 954, Training Loss: 0.9285805994191104\n",
      "Iteration 955, Training Loss: 0.9285771416092986\n",
      "Iteration 956, Training Loss: 0.9285760636776268\n",
      "Iteration 957, Training Loss: 0.9285745394349594\n",
      "Iteration 958, Training Loss: 0.9285736294051997\n",
      "Iteration 959, Training Loss: 0.9285733977736497\n",
      "Iteration 960, Training Loss: 0.928570178184952\n",
      "Iteration 961, Training Loss: 0.9285702019828763\n",
      "Iteration 962, Training Loss: 0.9285689360568415\n",
      "Iteration 963, Training Loss: 0.9285663665463444\n",
      "Iteration 964, Training Loss: 0.9285671848722672\n",
      "Iteration 965, Training Loss: 0.9285626079821584\n",
      "Iteration 966, Training Loss: 0.9285638473691615\n",
      "Iteration 967, Training Loss: 0.9285616156706931\n",
      "Iteration 968, Training Loss: 0.9285607187206181\n",
      "Iteration 969, Training Loss: 0.9285596368321078\n",
      "Iteration 970, Training Loss: 0.9285579218033072\n",
      "Iteration 971, Training Loss: 0.9285584000606979\n",
      "Iteration 972, Training Loss: 0.9285530696676142\n",
      "Iteration 973, Training Loss: 0.9285567186575805\n",
      "Iteration 974, Training Loss: 0.9285506865440497\n",
      "Iteration 975, Training Loss: 0.9285521228528806\n",
      "Iteration 976, Training Loss: 0.9285509063501941\n",
      "Iteration 977, Training Loss: 0.9285492415780102\n",
      "Iteration 978, Training Loss: 0.9285484735877214\n",
      "Iteration 979, Training Loss: 0.9285467297543635\n",
      "Iteration 980, Training Loss: 0.9285454140575683\n",
      "Iteration 981, Training Loss: 0.9285437705341699\n",
      "Iteration 982, Training Loss: 0.9285435077288383\n",
      "Iteration 983, Training Loss: 0.9285412344215329\n",
      "Iteration 984, Training Loss: 0.9285420506650027\n",
      "Iteration 985, Training Loss: 0.9285372610177169\n",
      "Iteration 986, Training Loss: 0.9285398872245018\n",
      "Iteration 987, Training Loss: 0.9285343907785949\n",
      "Iteration 988, Training Loss: 0.928536702434055\n",
      "Iteration 989, Training Loss: 0.9285331173500142\n",
      "Iteration 990, Training Loss: 0.9285344489206323\n",
      "Iteration 991, Training Loss: 0.9285307348830623\n",
      "Iteration 992, Training Loss: 0.9285312260606274\n",
      "Iteration 993, Training Loss: 0.9285299534104507\n",
      "Iteration 994, Training Loss: 0.9285285172335864\n",
      "Iteration 995, Training Loss: 0.9285273255461823\n",
      "Iteration 996, Training Loss: 0.9285253198667497\n",
      "Iteration 997, Training Loss: 0.9285245578049927\n",
      "Iteration 998, Training Loss: 0.9285234491004942\n",
      "Iteration 999, Training Loss: 0.9285236211082083\n",
      "Iteration 0, Training Loss: 1.0\n",
      "Iteration 1, Training Loss: 0.9872706329380041\n",
      "Iteration 2, Training Loss: 0.9748595000525584\n",
      "Iteration 3, Training Loss: 0.9631765668716016\n",
      "Iteration 4, Training Loss: 0.9552208797290741\n",
      "Iteration 5, Training Loss: 0.9505976641614434\n",
      "Iteration 6, Training Loss: 0.9472578275159365\n",
      "Iteration 7, Training Loss: 0.9447835002162942\n",
      "Iteration 8, Training Loss: 0.9427294281271325\n",
      "Iteration 9, Training Loss: 0.9410577522196979\n",
      "Iteration 10, Training Loss: 0.9396006130345813\n",
      "Iteration 11, Training Loss: 0.9383638058892881\n",
      "Iteration 12, Training Loss: 0.9373045710159755\n",
      "Iteration 13, Training Loss: 0.9364883161499415\n",
      "Iteration 14, Training Loss: 0.9357819459692307\n",
      "Iteration 15, Training Loss: 0.9352014513680333\n",
      "Iteration 16, Training Loss: 0.9347182430105542\n",
      "Iteration 17, Training Loss: 0.9342998815224778\n",
      "Iteration 18, Training Loss: 0.9339148008299378\n",
      "Iteration 19, Training Loss: 0.9335722838060597\n",
      "Iteration 20, Training Loss: 0.9332678140439837\n",
      "Iteration 21, Training Loss: 0.9330018362091439\n",
      "Iteration 22, Training Loss: 0.9327772128956219\n",
      "Iteration 23, Training Loss: 0.9325511860208983\n",
      "Iteration 24, Training Loss: 0.93235608903921\n",
      "Iteration 25, Training Loss: 0.9321571826879345\n",
      "Iteration 26, Training Loss: 0.9319850142874416\n",
      "Iteration 27, Training Loss: 0.9318073215424858\n",
      "Iteration 28, Training Loss: 0.9316421006789253\n",
      "Iteration 29, Training Loss: 0.931485137674063\n",
      "Iteration 30, Training Loss: 0.9313491636464268\n",
      "Iteration 31, Training Loss: 0.9312196187939944\n",
      "Iteration 32, Training Loss: 0.9310928183846247\n",
      "Iteration 33, Training Loss: 0.9309808061672199\n",
      "Iteration 34, Training Loss: 0.930875065625035\n",
      "Iteration 35, Training Loss: 0.9307830449274036\n",
      "Iteration 36, Training Loss: 0.930702767026111\n",
      "Iteration 37, Training Loss: 0.9306032930727192\n",
      "Iteration 38, Training Loss: 0.9305444081821281\n",
      "Iteration 39, Training Loss: 0.9304590724942602\n",
      "Iteration 40, Training Loss: 0.9303871057245233\n",
      "Iteration 41, Training Loss: 0.9303197797137102\n",
      "Iteration 42, Training Loss: 0.9302485473530075\n",
      "Iteration 43, Training Loss: 0.9301855769201989\n",
      "Iteration 44, Training Loss: 0.9301098021789257\n",
      "Iteration 45, Training Loss: 0.9300638223782062\n",
      "Iteration 46, Training Loss: 0.9299879156167002\n",
      "Iteration 47, Training Loss: 0.9299414715783616\n",
      "Iteration 48, Training Loss: 0.9298622527632427\n",
      "Iteration 49, Training Loss: 0.9298311162652407\n",
      "Iteration 50, Training Loss: 0.9297686066454414\n",
      "Iteration 51, Training Loss: 0.9297259113059504\n",
      "Iteration 52, Training Loss: 0.9296854815713856\n",
      "Iteration 53, Training Loss: 0.9296172695666208\n",
      "Iteration 54, Training Loss: 0.9295828867187945\n",
      "Iteration 55, Training Loss: 0.9295475773170074\n",
      "Iteration 56, Training Loss: 0.9294972235256596\n",
      "Iteration 57, Training Loss: 0.9294589200273378\n",
      "Iteration 58, Training Loss: 0.9294245791203847\n",
      "Iteration 59, Training Loss: 0.9293875924728137\n",
      "Iteration 60, Training Loss: 0.929349070667879\n",
      "Iteration 61, Training Loss: 0.929310638319797\n",
      "Iteration 62, Training Loss: 0.9292806205097959\n",
      "Iteration 63, Training Loss: 0.9292399260411844\n",
      "Iteration 64, Training Loss: 0.9292260974184017\n",
      "Iteration 65, Training Loss: 0.9291869322288749\n",
      "Iteration 66, Training Loss: 0.9291375421071194\n",
      "Iteration 67, Training Loss: 0.9291393961650649\n",
      "Iteration 68, Training Loss: 0.9290843493286215\n",
      "Iteration 69, Training Loss: 0.9290802232929646\n",
      "Iteration 70, Training Loss: 0.9290437921485707\n",
      "Iteration 71, Training Loss: 0.9290081988609958\n",
      "Iteration 72, Training Loss: 0.9290004954649179\n",
      "Iteration 73, Training Loss: 0.9289634358853534\n",
      "Iteration 74, Training Loss: 0.92894028563046\n",
      "Iteration 75, Training Loss: 0.9289195314578447\n",
      "Iteration 76, Training Loss: 0.9289120526669443\n",
      "Iteration 77, Training Loss: 0.9288570784793069\n",
      "Iteration 78, Training Loss: 0.9288724226004247\n",
      "Iteration 79, Training Loss: 0.9288200025453626\n",
      "Iteration 80, Training Loss: 0.9288294679802092\n",
      "Iteration 81, Training Loss: 0.9287911655119917\n",
      "Iteration 82, Training Loss: 0.9287797227209655\n",
      "Iteration 83, Training Loss: 0.9287572738464974\n",
      "Iteration 84, Training Loss: 0.9287379281140141\n",
      "Iteration 85, Training Loss: 0.9287277259832601\n",
      "Iteration 86, Training Loss: 0.9287042581774415\n",
      "Iteration 87, Training Loss: 0.9286896557354055\n",
      "Iteration 88, Training Loss: 0.9286784684637853\n",
      "Iteration 89, Training Loss: 0.9286515893678605\n",
      "Iteration 90, Training Loss: 0.9286508175518389\n",
      "Iteration 91, Training Loss: 0.9286162701908457\n",
      "Iteration 92, Training Loss: 0.9286151960496613\n",
      "Iteration 93, Training Loss: 0.9285957756663228\n",
      "Iteration 94, Training Loss: 0.9285855533053845\n",
      "Iteration 95, Training Loss: 0.92856704833445\n",
      "Iteration 96, Training Loss: 0.9285722264578407\n",
      "Iteration 97, Training Loss: 0.9285426386153592\n",
      "Iteration 98, Training Loss: 0.928529743794811\n",
      "Iteration 99, Training Loss: 0.9285213950345329\n",
      "Iteration 100, Training Loss: 0.9285026341302669\n",
      "Iteration 101, Training Loss: 0.9284974060361832\n",
      "Iteration 102, Training Loss: 0.9284882411674636\n",
      "Iteration 103, Training Loss: 0.9284581079171529\n",
      "Iteration 104, Training Loss: 0.9284734408550329\n",
      "Iteration 105, Training Loss: 0.9284438962627173\n",
      "Iteration 106, Training Loss: 0.9284629248675844\n",
      "Iteration 107, Training Loss: 0.9284297820295472\n",
      "Iteration 108, Training Loss: 0.9284244078117908\n",
      "Iteration 109, Training Loss: 0.9284203572442321\n",
      "Iteration 110, Training Loss: 0.9284164161153551\n",
      "Iteration 111, Training Loss: 0.9283866599909802\n",
      "Iteration 112, Training Loss: 0.9284152039221303\n",
      "Iteration 113, Training Loss: 0.9283653942306493\n",
      "Iteration 114, Training Loss: 0.9283759799353495\n",
      "Iteration 115, Training Loss: 0.9283567252242915\n",
      "Iteration 116, Training Loss: 0.9283736499133831\n",
      "Iteration 117, Training Loss: 0.9283427424485405\n",
      "Iteration 118, Training Loss: 0.9283531187632555\n",
      "Iteration 119, Training Loss: 0.928321251728166\n",
      "Iteration 120, Training Loss: 0.9283487450323915\n",
      "Iteration 121, Training Loss: 0.9283094148254175\n",
      "Iteration 122, Training Loss: 0.9283349598137141\n",
      "Iteration 123, Training Loss: 0.9282943521483663\n",
      "Iteration 124, Training Loss: 0.9283210690752061\n",
      "Iteration 125, Training Loss: 0.9282745078665187\n",
      "Iteration 126, Training Loss: 0.9283139452526445\n",
      "Iteration 127, Training Loss: 0.9282704828893655\n",
      "Iteration 128, Training Loss: 0.9282954373769375\n",
      "Iteration 129, Training Loss: 0.9282619036566597\n",
      "Iteration 130, Training Loss: 0.9282814873664\n",
      "Iteration 131, Training Loss: 0.9282549291990548\n",
      "Iteration 132, Training Loss: 0.9282739563200078\n",
      "Iteration 133, Training Loss: 0.9282452834925364\n",
      "Iteration 134, Training Loss: 0.9282701722520069\n",
      "Iteration 135, Training Loss: 0.9282226823993103\n",
      "Iteration 136, Training Loss: 0.9282502192413886\n",
      "Iteration 137, Training Loss: 0.9282323570909473\n",
      "Iteration 138, Training Loss: 0.928239495401313\n",
      "Iteration 139, Training Loss: 0.9282219583668783\n",
      "Iteration 140, Training Loss: 0.9282502962414946\n",
      "Iteration 141, Training Loss: 0.9281899712204995\n",
      "Iteration 142, Training Loss: 0.9282300059568422\n",
      "Iteration 143, Training Loss: 0.9282004847854194\n",
      "Iteration 144, Training Loss: 0.9282143000548975\n",
      "Iteration 145, Training Loss: 0.9281951810783985\n",
      "Iteration 146, Training Loss: 0.9282125824092196\n",
      "Iteration 147, Training Loss: 0.9281892266124988\n",
      "Iteration 148, Training Loss: 0.9282011300897355\n",
      "Iteration 149, Training Loss: 0.9281874371830215\n",
      "Iteration 150, Training Loss: 0.9281889022135702\n",
      "Iteration 151, Training Loss: 0.9281912550083414\n",
      "Early stopping at iteration 151\n",
      "Iteration 0, Training Loss: 1.0\n",
      "Iteration 1, Training Loss: 0.9987270632938003\n",
      "Iteration 2, Training Loss: 0.9974604912711318\n",
      "Iteration 3, Training Loss: 0.9962002521085765\n",
      "Iteration 4, Training Loss: 0.9949463141418341\n",
      "Iteration 5, Training Loss: 0.9936986458649253\n",
      "Iteration 6, Training Loss: 0.9924572159294012\n",
      "Iteration 7, Training Loss: 0.9912219931435545\n",
      "Iteration 8, Training Loss: 0.9899929464716372\n",
      "Iteration 9, Training Loss: 0.9887700450330794\n",
      "Iteration 10, Training Loss: 0.9875532581017143\n",
      "Iteration 11, Training Loss: 0.9863425551050063\n",
      "Iteration 12, Training Loss: 0.9851379056232817\n",
      "Iteration 13, Training Loss: 0.9839392793889657\n",
      "Iteration 14, Training Loss: 0.9827466462858213\n",
      "Iteration 15, Training Loss: 0.9815599763481926\n",
      "Iteration 16, Training Loss: 0.9803792397602521\n",
      "Iteration 17, Training Loss: 0.9792044068552512\n",
      "Iteration 18, Training Loss: 0.9780354481147754\n",
      "Iteration 19, Training Loss: 0.9768723341680019\n",
      "Iteration 20, Training Loss: 0.9757150357909625\n",
      "Iteration 21, Training Loss: 0.974563523905808\n",
      "Iteration 22, Training Loss: 0.9734206227081225\n",
      "Iteration 23, Training Loss: 0.9722919900431322\n",
      "Iteration 24, Training Loss: 0.9711690005414668\n",
      "Iteration 25, Training Loss: 0.9700516259873099\n",
      "Iteration 26, Training Loss: 0.9689456942099157\n",
      "Iteration 27, Training Loss: 0.9678838439929712\n",
      "Iteration 28, Training Loss: 0.9668636160841279\n",
      "Iteration 29, Training Loss: 0.9658772314967775\n",
      "Iteration 30, Training Loss: 0.9649145951401199\n",
      "Iteration 31, Training Loss: 0.9639781781556597\n",
      "Iteration 32, Training Loss: 0.9630843982847593\n",
      "Iteration 33, Training Loss: 0.9622567365867526\n",
      "Iteration 34, Training Loss: 0.9614622448073191\n",
      "Iteration 35, Training Loss: 0.9607092062013389\n",
      "Iteration 36, Training Loss: 0.9600138254578293\n",
      "Iteration 37, Training Loss: 0.959362382720967\n",
      "Iteration 38, Training Loss: 0.9587447041817815\n",
      "Iteration 39, Training Loss: 0.9581658120409638\n",
      "Iteration 40, Training Loss: 0.9576155334758548\n",
      "Iteration 41, Training Loss: 0.9570799787340969\n",
      "Iteration 42, Training Loss: 0.956572651474123\n",
      "Iteration 43, Training Loss: 0.9560806082386213\n",
      "Iteration 44, Training Loss: 0.9556077581257145\n",
      "Iteration 45, Training Loss: 0.9551574086433219\n",
      "Iteration 46, Training Loss: 0.9547184715759015\n",
      "Iteration 47, Training Loss: 0.9542857259668818\n",
      "Iteration 48, Training Loss: 0.9538675233689939\n",
      "Iteration 49, Training Loss: 0.9534657016026745\n",
      "Iteration 50, Training Loss: 0.9530896427201957\n",
      "Iteration 51, Training Loss: 0.952727282146936\n",
      "Iteration 52, Training Loss: 0.9523747333376175\n",
      "Iteration 53, Training Loss: 0.9520339918360564\n",
      "Iteration 54, Training Loss: 0.9516987166288042\n",
      "Iteration 55, Training Loss: 0.9513731293830668\n",
      "Iteration 56, Training Loss: 0.9510564703868888\n",
      "Iteration 57, Training Loss: 0.9507477189434771\n",
      "Iteration 58, Training Loss: 0.9504508058796926\n",
      "Iteration 59, Training Loss: 0.9501602296041697\n",
      "Iteration 60, Training Loss: 0.9498715844731098\n",
      "Iteration 61, Training Loss: 0.9495905910116499\n",
      "Iteration 62, Training Loss: 0.9493182685431112\n",
      "Iteration 63, Training Loss: 0.9490513188136577\n",
      "Iteration 64, Training Loss: 0.9487959323430466\n",
      "Iteration 65, Training Loss: 0.9485484230275582\n",
      "Iteration 66, Training Loss: 0.9483118231688216\n",
      "Iteration 67, Training Loss: 0.9480782083114944\n",
      "Iteration 68, Training Loss: 0.9478565260227954\n",
      "Iteration 69, Training Loss: 0.9476377897146514\n",
      "Iteration 70, Training Loss: 0.9474238726634273\n",
      "Iteration 71, Training Loss: 0.9472130882403592\n",
      "Iteration 72, Training Loss: 0.9470072773801547\n",
      "Iteration 73, Training Loss: 0.9468056490810486\n",
      "Iteration 74, Training Loss: 0.9466071934304782\n",
      "Iteration 75, Training Loss: 0.9464112263911035\n",
      "Iteration 76, Training Loss: 0.946218743660527\n",
      "Iteration 77, Training Loss: 0.9460316143077269\n",
      "Iteration 78, Training Loss: 0.9458463894607344\n",
      "Iteration 79, Training Loss: 0.9456666052662535\n",
      "Iteration 80, Training Loss: 0.9454890748553342\n",
      "Iteration 81, Training Loss: 0.9453150851342428\n",
      "Iteration 82, Training Loss: 0.9451445921369966\n",
      "Iteration 83, Training Loss: 0.9449767815690686\n",
      "Iteration 84, Training Loss: 0.9448135724807027\n",
      "Iteration 85, Training Loss: 0.9446570443436884\n",
      "Iteration 86, Training Loss: 0.9445037249694547\n",
      "Iteration 87, Training Loss: 0.9443532614478017\n",
      "Iteration 88, Training Loss: 0.944205157364946\n",
      "Iteration 89, Training Loss: 0.9440624695037573\n",
      "Iteration 90, Training Loss: 0.9439235927318746\n",
      "Iteration 91, Training Loss: 0.9437865610630647\n",
      "Iteration 92, Training Loss: 0.9436506570483337\n",
      "Iteration 93, Training Loss: 0.9435175167746592\n",
      "Iteration 94, Training Loss: 0.9433863189910351\n",
      "Iteration 95, Training Loss: 0.9432564033075882\n",
      "Iteration 96, Training Loss: 0.9431300515780602\n",
      "Iteration 97, Training Loss: 0.9430045243826262\n",
      "Iteration 98, Training Loss: 0.9428818461712992\n",
      "Iteration 99, Training Loss: 0.9427619551483817\n",
      "Iteration 100, Training Loss: 0.9426430443648421\n",
      "Iteration 101, Training Loss: 0.9425258609440889\n",
      "Iteration 102, Training Loss: 0.9424093385355834\n",
      "Iteration 103, Training Loss: 0.9422946797396697\n",
      "Iteration 104, Training Loss: 0.9421826401310138\n",
      "Iteration 105, Training Loss: 0.9420722503954985\n",
      "Iteration 106, Training Loss: 0.9419648117336668\n",
      "Iteration 107, Training Loss: 0.9418593203996127\n",
      "Iteration 108, Training Loss: 0.9417557056441562\n",
      "Iteration 109, Training Loss: 0.9416540538370399\n",
      "Iteration 110, Training Loss: 0.9415552714132333\n",
      "Iteration 111, Training Loss: 0.9414569829015457\n",
      "Iteration 112, Training Loss: 0.9413596234611316\n",
      "Iteration 113, Training Loss: 0.9412644525928467\n",
      "Iteration 114, Training Loss: 0.9411697852808611\n",
      "Iteration 115, Training Loss: 0.9410752576256035\n",
      "Iteration 116, Training Loss: 0.9409822850082482\n",
      "Iteration 117, Training Loss: 0.9408906544074807\n",
      "Iteration 118, Training Loss: 0.9408041549566901\n",
      "Iteration 119, Training Loss: 0.9407180880031535\n",
      "Iteration 120, Training Loss: 0.9406324513843847\n",
      "Iteration 121, Training Loss: 0.9405472429487097\n",
      "Iteration 122, Training Loss: 0.9404629095376768\n",
      "Iteration 123, Training Loss: 0.9403808374022301\n",
      "Iteration 124, Training Loss: 0.9403014135642709\n",
      "Iteration 125, Training Loss: 0.9402235009628647\n",
      "Iteration 126, Training Loss: 0.9401469245526941\n",
      "Iteration 127, Training Loss: 0.9400707310245744\n",
      "Iteration 128, Training Loss: 0.9399949184640952\n",
      "Iteration 129, Training Loss: 0.9399194849664186\n",
      "Iteration 130, Training Loss: 0.9398445676904809\n",
      "Iteration 131, Training Loss: 0.9397720735664029\n",
      "Iteration 132, Training Loss: 0.9397010330863897\n",
      "Iteration 133, Training Loss: 0.9396303478087769\n",
      "Iteration 134, Training Loss: 0.9395600159575518\n",
      "Iteration 135, Training Loss: 0.9394900357655829\n",
      "Iteration 136, Training Loss: 0.9394206153351657\n",
      "Iteration 137, Training Loss: 0.9393539393501015\n",
      "Iteration 138, Training Loss: 0.939287657730871\n",
      "Iteration 139, Training Loss: 0.9392230114632444\n",
      "Iteration 140, Training Loss: 0.9391582291879316\n",
      "Iteration 141, Training Loss: 0.9390945326094408\n",
      "Iteration 142, Training Loss: 0.9390317954645716\n",
      "Iteration 143, Training Loss: 0.9389687219992426\n",
      "Iteration 144, Training Loss: 0.9389061235554045\n",
      "Iteration 145, Training Loss: 0.938845452137734\n",
      "Iteration 146, Training Loss: 0.9387854320767246\n",
      "Iteration 147, Training Loss: 0.9387257121160203\n",
      "Iteration 148, Training Loss: 0.9386663048813766\n",
      "Iteration 149, Training Loss: 0.9386081882513235\n",
      "Iteration 150, Training Loss: 0.9385516708523407\n",
      "Iteration 151, Training Loss: 0.9384957807967194\n",
      "Iteration 152, Training Loss: 0.9384425891712271\n",
      "Iteration 153, Training Loss: 0.9383892161625137\n",
      "Iteration 154, Training Loss: 0.9383389782376896\n",
      "Iteration 155, Training Loss: 0.9382902003853208\n",
      "Iteration 156, Training Loss: 0.9382422976715209\n",
      "Iteration 157, Training Loss: 0.9381940000491372\n",
      "Iteration 158, Training Loss: 0.9381461760925575\n",
      "Iteration 159, Training Loss: 0.9381002579685241\n",
      "Iteration 160, Training Loss: 0.9380545451812436\n",
      "Iteration 161, Training Loss: 0.9380084585313074\n",
      "Iteration 162, Training Loss: 0.9379638441270981\n",
      "Iteration 163, Training Loss: 0.9379193198716516\n",
      "Iteration 164, Training Loss: 0.9378781390525531\n",
      "Iteration 165, Training Loss: 0.9378365840997365\n",
      "Iteration 166, Training Loss: 0.9377958639452547\n",
      "Iteration 167, Training Loss: 0.9377547074619865\n",
      "Iteration 168, Training Loss: 0.9377153090303846\n",
      "Iteration 169, Training Loss: 0.9376763940405752\n",
      "Iteration 170, Training Loss: 0.9376383719799385\n",
      "Iteration 171, Training Loss: 0.937599929022746\n",
      "Iteration 172, Training Loss: 0.9375632195062447\n",
      "Iteration 173, Training Loss: 0.9375260406258068\n",
      "Iteration 174, Training Loss: 0.9374905881358662\n",
      "Iteration 175, Training Loss: 0.9374546176544438\n",
      "Iteration 176, Training Loss: 0.9374189582695095\n",
      "Iteration 177, Training Loss: 0.9373835169607498\n",
      "Iteration 178, Training Loss: 0.9373490181690511\n",
      "Iteration 179, Training Loss: 0.9373149397767679\n",
      "Iteration 180, Training Loss: 0.9372821441336323\n",
      "Iteration 181, Training Loss: 0.9372490448998158\n",
      "Iteration 182, Training Loss: 0.9372166141989682\n",
      "Iteration 183, Training Loss: 0.937183159530088\n",
      "Iteration 184, Training Loss: 0.9371505992291546\n",
      "Iteration 185, Training Loss: 0.9371204253945286\n",
      "Iteration 186, Training Loss: 0.9370909877092187\n",
      "Iteration 187, Training Loss: 0.9370610796096656\n",
      "Iteration 188, Training Loss: 0.9370300884362958\n",
      "Iteration 189, Training Loss: 0.9370018704944532\n",
      "Iteration 190, Training Loss: 0.9369734617412884\n",
      "Iteration 191, Training Loss: 0.9369452509318509\n",
      "Iteration 192, Training Loss: 0.9369173606493568\n",
      "Iteration 193, Training Loss: 0.9368921086227222\n",
      "Iteration 194, Training Loss: 0.9368644252125649\n",
      "Iteration 195, Training Loss: 0.9368382133532422\n",
      "Iteration 196, Training Loss: 0.9368114201041842\n",
      "Iteration 197, Training Loss: 0.9367842481611116\n",
      "Iteration 198, Training Loss: 0.9367583314277754\n",
      "Iteration 199, Training Loss: 0.9367314313413105\n",
      "Iteration 200, Training Loss: 0.9367079135516272\n",
      "Iteration 201, Training Loss: 0.936682713022996\n",
      "Iteration 202, Training Loss: 0.9366583239990227\n",
      "Iteration 203, Training Loss: 0.9366354604154848\n",
      "Iteration 204, Training Loss: 0.93661090064778\n",
      "Iteration 205, Training Loss: 0.9365876357666473\n",
      "Iteration 206, Training Loss: 0.9365645391614881\n",
      "Iteration 207, Training Loss: 0.936542216409402\n",
      "Iteration 208, Training Loss: 0.9365187976116529\n",
      "Iteration 209, Training Loss: 0.9364974213552518\n",
      "Iteration 210, Training Loss: 0.9364755177014028\n",
      "Iteration 211, Training Loss: 0.9364543336062581\n",
      "Iteration 212, Training Loss: 0.936431388961744\n",
      "Iteration 213, Training Loss: 0.936410432852025\n",
      "Iteration 214, Training Loss: 0.9363895773671284\n",
      "Iteration 215, Training Loss: 0.9363675970598369\n",
      "Iteration 216, Training Loss: 0.9363470626402516\n",
      "Iteration 217, Training Loss: 0.936326606572746\n",
      "Iteration 218, Training Loss: 0.9363062802201533\n",
      "Iteration 219, Training Loss: 0.9362867006863116\n",
      "Iteration 220, Training Loss: 0.93626527760678\n",
      "Iteration 221, Training Loss: 0.9362453175860549\n",
      "Iteration 222, Training Loss: 0.9362275411293202\n",
      "Iteration 223, Training Loss: 0.9362085640162248\n",
      "Iteration 224, Training Loss: 0.936189121662386\n",
      "Iteration 225, Training Loss: 0.9361722847447679\n",
      "Iteration 226, Training Loss: 0.9361543847271271\n",
      "Iteration 227, Training Loss: 0.9361365583647803\n",
      "Iteration 228, Training Loss: 0.9361201475930355\n",
      "Iteration 229, Training Loss: 0.9361018497965671\n",
      "Iteration 230, Training Loss: 0.9360849419571082\n",
      "Iteration 231, Training Loss: 0.9360689034603857\n",
      "Iteration 232, Training Loss: 0.9360534879594996\n",
      "Iteration 233, Training Loss: 0.9360363265200813\n",
      "Iteration 234, Training Loss: 0.9360206032843543\n",
      "Iteration 235, Training Loss: 0.936004988181353\n",
      "Iteration 236, Training Loss: 0.9359887417866525\n",
      "Iteration 237, Training Loss: 0.9359732865960619\n",
      "Iteration 238, Training Loss: 0.9359579522346002\n",
      "Iteration 239, Training Loss: 0.9359427696903915\n",
      "Iteration 240, Training Loss: 0.9359297157995586\n",
      "Iteration 241, Training Loss: 0.9359147773657981\n",
      "Iteration 242, Training Loss: 0.9359018427117567\n",
      "Iteration 243, Training Loss: 0.9358870649990515\n",
      "Iteration 244, Training Loss: 0.9358749452670936\n",
      "Iteration 245, Training Loss: 0.9358596849917555\n",
      "Iteration 246, Training Loss: 0.9358451398236528\n",
      "Iteration 247, Training Loss: 0.935833193306448\n",
      "Iteration 248, Training Loss: 0.9358187887137386\n",
      "Iteration 249, Training Loss: 0.9358050913300624\n",
      "Iteration 250, Training Loss: 0.9357915409062696\n",
      "Iteration 251, Training Loss: 0.9357811652250734\n",
      "Iteration 252, Training Loss: 0.9357663895844662\n",
      "Iteration 253, Training Loss: 0.935754312565604\n",
      "Iteration 254, Training Loss: 0.9357428202301548\n",
      "Iteration 255, Training Loss: 0.9357295527207771\n",
      "Iteration 256, Training Loss: 0.9357195683809011\n",
      "Iteration 257, Training Loss: 0.9357056962382477\n",
      "Iteration 258, Training Loss: 0.9356932879549851\n",
      "Iteration 259, Training Loss: 0.9356821698449694\n",
      "Iteration 260, Training Loss: 0.9356705336782083\n",
      "Iteration 261, Training Loss: 0.9356577159787015\n",
      "Iteration 262, Training Loss: 0.9356468486585555\n",
      "Iteration 263, Training Loss: 0.9356353559738659\n",
      "Iteration 264, Training Loss: 0.9356253129462009\n",
      "Iteration 265, Training Loss: 0.9356121354774594\n",
      "Iteration 266, Training Loss: 0.9356020541188741\n",
      "Iteration 267, Training Loss: 0.9355910647921428\n",
      "Iteration 268, Training Loss: 0.9355810612618789\n",
      "Iteration 269, Training Loss: 0.9355687776277162\n",
      "Iteration 270, Training Loss: 0.9355590081016755\n",
      "Iteration 271, Training Loss: 0.9355473858088076\n",
      "Iteration 272, Training Loss: 0.9355371562896921\n",
      "Iteration 273, Training Loss: 0.9355270253641629\n",
      "Iteration 274, Training Loss: 0.9355175062876745\n",
      "Iteration 275, Training Loss: 0.9355061498873706\n",
      "Iteration 276, Training Loss: 0.9354986556781187\n",
      "Iteration 277, Training Loss: 0.9354861907713914\n",
      "Iteration 278, Training Loss: 0.9354787958031104\n",
      "Iteration 279, Training Loss: 0.9354670288724536\n",
      "Iteration 280, Training Loss: 0.9354590690464988\n",
      "Iteration 281, Training Loss: 0.9354473591033011\n",
      "Iteration 282, Training Loss: 0.9354402271176536\n",
      "Iteration 283, Training Loss: 0.9354286430900232\n",
      "Iteration 284, Training Loss: 0.9354195688379491\n",
      "Iteration 285, Training Loss: 0.9354100833723829\n",
      "Iteration 286, Training Loss: 0.9353987282630347\n",
      "Iteration 287, Training Loss: 0.9353912240908983\n",
      "Iteration 288, Training Loss: 0.9353793965504937\n",
      "Iteration 289, Training Loss: 0.9353706592401566\n",
      "Iteration 290, Training Loss: 0.9353615332355788\n",
      "Iteration 291, Training Loss: 0.9353509217288916\n",
      "Iteration 292, Training Loss: 0.9353430835428289\n",
      "Iteration 293, Training Loss: 0.9353321536210083\n",
      "Iteration 294, Training Loss: 0.9353243171610919\n",
      "Iteration 295, Training Loss: 0.9353158783739526\n",
      "Iteration 296, Training Loss: 0.9353057528481862\n",
      "Iteration 297, Training Loss: 0.9352975438471882\n",
      "Iteration 298, Training Loss: 0.9352906500111637\n",
      "Iteration 299, Training Loss: 0.935279883401321\n",
      "Iteration 300, Training Loss: 0.9352731127569628\n",
      "Iteration 301, Training Loss: 0.9352645134894901\n",
      "Iteration 302, Training Loss: 0.9352582985275453\n",
      "Iteration 303, Training Loss: 0.9352492205503271\n",
      "Iteration 304, Training Loss: 0.9352424388322123\n",
      "Iteration 305, Training Loss: 0.935235827195005\n",
      "Iteration 306, Training Loss: 0.9352255262332541\n",
      "Iteration 307, Training Loss: 0.9352195689994981\n",
      "Iteration 308, Training Loss: 0.9352105427084916\n",
      "Iteration 309, Training Loss: 0.9352035637462136\n",
      "Iteration 310, Training Loss: 0.9351951771313411\n",
      "Iteration 311, Training Loss: 0.9351875521784573\n",
      "Iteration 312, Training Loss: 0.9351806568785933\n",
      "Iteration 313, Training Loss: 0.9351712265147865\n",
      "Iteration 314, Training Loss: 0.9351655598850234\n",
      "Iteration 315, Training Loss: 0.9351586722555815\n",
      "Iteration 316, Training Loss: 0.9351506265469663\n",
      "Iteration 317, Training Loss: 0.9351446185553624\n",
      "Iteration 318, Training Loss: 0.9351340291149399\n",
      "Iteration 319, Training Loss: 0.9351279299016841\n",
      "Iteration 320, Training Loss: 0.9351212615540412\n",
      "Iteration 321, Training Loss: 0.9351134527707138\n",
      "Iteration 322, Training Loss: 0.9351049905589802\n",
      "Iteration 323, Training Loss: 0.9351009903179662\n",
      "Iteration 324, Training Loss: 0.9350889373817443\n",
      "Iteration 325, Training Loss: 0.9350841987626581\n",
      "Iteration 326, Training Loss: 0.935077829165389\n",
      "Iteration 327, Training Loss: 0.9350688666942231\n",
      "Iteration 328, Training Loss: 0.9350664274020324\n",
      "Iteration 329, Training Loss: 0.9350538738230425\n",
      "Iteration 330, Training Loss: 0.9350499253963894\n",
      "Iteration 331, Training Loss: 0.9350431253749869\n",
      "Iteration 332, Training Loss: 0.9350363496848856\n",
      "Iteration 333, Training Loss: 0.9350300580347274\n",
      "Iteration 334, Training Loss: 0.9350214611812209\n",
      "Iteration 335, Training Loss: 0.9350158509678733\n",
      "Iteration 336, Training Loss: 0.9350091890618094\n",
      "Iteration 337, Training Loss: 0.9350037472881144\n",
      "Iteration 338, Training Loss: 0.9349947792609883\n",
      "Iteration 339, Training Loss: 0.9349911669620986\n",
      "Iteration 340, Training Loss: 0.9349839430076053\n",
      "Iteration 341, Training Loss: 0.9349755270018693\n",
      "Iteration 342, Training Loss: 0.9349709797879165\n",
      "Iteration 343, Training Loss: 0.9349632199976732\n",
      "Iteration 344, Training Loss: 0.934959256047453\n",
      "Iteration 345, Training Loss: 0.9349503124985156\n",
      "Iteration 346, Training Loss: 0.9349446923595968\n",
      "Iteration 347, Training Loss: 0.9349376427152516\n",
      "Iteration 348, Training Loss: 0.9349325752239673\n",
      "Iteration 349, Training Loss: 0.9349243143715074\n",
      "Iteration 350, Training Loss: 0.9349199617458512\n",
      "Iteration 351, Training Loss: 0.9349118955042297\n",
      "Iteration 352, Training Loss: 0.9349050430000805\n",
      "Iteration 353, Training Loss: 0.9349001199433729\n",
      "Iteration 354, Training Loss: 0.9348980098251268\n",
      "Iteration 355, Training Loss: 0.9348873075213087\n",
      "Iteration 356, Training Loss: 0.9348837988792786\n",
      "Iteration 357, Training Loss: 0.9348758788255488\n",
      "Iteration 358, Training Loss: 0.934872418774146\n",
      "Iteration 359, Training Loss: 0.9348659088809715\n",
      "Iteration 360, Training Loss: 0.9348592215601442\n",
      "Iteration 361, Training Loss: 0.934854578326891\n",
      "Iteration 362, Training Loss: 0.9348505445414719\n",
      "Iteration 363, Training Loss: 0.9348428723557499\n",
      "Iteration 364, Training Loss: 0.9348382303689645\n",
      "Iteration 365, Training Loss: 0.9348343566778061\n",
      "Iteration 366, Training Loss: 0.9348260798634941\n",
      "Iteration 367, Training Loss: 0.9348241652767502\n",
      "Iteration 368, Training Loss: 0.9348146946438257\n",
      "Iteration 369, Training Loss: 0.9348133914652396\n",
      "Iteration 370, Training Loss: 0.934805841469125\n",
      "Iteration 371, Training Loss: 0.9348007466101844\n",
      "Iteration 372, Training Loss: 0.9347941153330139\n",
      "Iteration 373, Training Loss: 0.9347902878570311\n",
      "Iteration 374, Training Loss: 0.9347847248616071\n",
      "Iteration 375, Training Loss: 0.9347791830062967\n",
      "Iteration 376, Training Loss: 0.9347737398873606\n",
      "Iteration 377, Training Loss: 0.9347689780089257\n",
      "Iteration 378, Training Loss: 0.9347667602029397\n",
      "Iteration 379, Training Loss: 0.9347589489508793\n",
      "Iteration 380, Training Loss: 0.934757208923861\n",
      "Iteration 381, Training Loss: 0.9347500011705211\n",
      "Iteration 382, Training Loss: 0.9347465834869548\n",
      "Iteration 383, Training Loss: 0.934741879303282\n",
      "Iteration 384, Training Loss: 0.9347404233931956\n",
      "Iteration 385, Training Loss: 0.9347303813008347\n",
      "Iteration 386, Training Loss: 0.9347338977963723\n",
      "Iteration 387, Training Loss: 0.9347248331894547\n",
      "Iteration 388, Training Loss: 0.9347214497061261\n",
      "Iteration 389, Training Loss: 0.9347195936850794\n",
      "Iteration 390, Training Loss: 0.9347137535017094\n",
      "Iteration 391, Training Loss: 0.9347104404828056\n",
      "Iteration 392, Training Loss: 0.9347054245303713\n",
      "Iteration 393, Training Loss: 0.9347023571461848\n",
      "Iteration 394, Training Loss: 0.9346979479397077\n",
      "Iteration 395, Training Loss: 0.9346953262079079\n",
      "Iteration 396, Training Loss: 0.9346891594888161\n",
      "Iteration 397, Training Loss: 0.9346865432223037\n",
      "Iteration 398, Training Loss: 0.9346823316379794\n",
      "Iteration 399, Training Loss: 0.9346791766199855\n",
      "Iteration 400, Training Loss: 0.934672517897669\n",
      "Iteration 401, Training Loss: 0.9346744984007\n",
      "Iteration 402, Training Loss: 0.9346662624281705\n",
      "Iteration 403, Training Loss: 0.9346621650197399\n",
      "Iteration 404, Training Loss: 0.9346590952728204\n",
      "Iteration 405, Training Loss: 0.9346562595200478\n",
      "Iteration 406, Training Loss: 0.934650785289305\n",
      "Iteration 407, Training Loss: 0.9346478356672905\n",
      "Iteration 408, Training Loss: 0.9346442894172948\n",
      "Iteration 409, Training Loss: 0.9346383915247359\n",
      "Iteration 410, Training Loss: 0.9346374363956185\n",
      "Iteration 411, Training Loss: 0.9346339130985832\n",
      "Iteration 412, Training Loss: 0.9346285218733617\n",
      "Iteration 413, Training Loss: 0.9346269622893264\n",
      "Iteration 414, Training Loss: 0.9346198656962511\n",
      "Iteration 415, Training Loss: 0.9346222225238104\n",
      "Iteration 416, Training Loss: 0.9346143623361008\n",
      "Iteration 417, Training Loss: 0.9346115538870852\n",
      "Iteration 418, Training Loss: 0.934608877892404\n",
      "Iteration 419, Training Loss: 0.9346043543925373\n",
      "Iteration 420, Training Loss: 0.9346022231886357\n",
      "Iteration 421, Training Loss: 0.9345965095786491\n",
      "Iteration 422, Training Loss: 0.9345970947585222\n",
      "Iteration 423, Training Loss: 0.9345882019726617\n",
      "Iteration 424, Training Loss: 0.9345911865930709\n",
      "Iteration 425, Training Loss: 0.9345823874860605\n",
      "Iteration 426, Training Loss: 0.9345828600457053\n",
      "Iteration 427, Training Loss: 0.9345766214445214\n",
      "Iteration 428, Training Loss: 0.9345765283573416\n",
      "Iteration 429, Training Loss: 0.9345709252190496\n",
      "Iteration 430, Training Loss: 0.9345665825693893\n",
      "Iteration 431, Training Loss: 0.9345678656537657\n",
      "Iteration 432, Training Loss: 0.9345591302877804\n",
      "Iteration 433, Training Loss: 0.9345616651419179\n",
      "Iteration 434, Training Loss: 0.934554818519472\n",
      "Iteration 435, Training Loss: 0.9345523095479508\n",
      "Iteration 436, Training Loss: 0.9345506925959813\n",
      "Iteration 437, Training Loss: 0.9345475487289773\n",
      "Iteration 438, Training Loss: 0.9345433058615217\n",
      "Iteration 439, Training Loss: 0.9345409834522221\n",
      "Iteration 440, Training Loss: 0.9345379015781596\n",
      "Iteration 441, Training Loss: 0.9345342851990053\n",
      "Iteration 442, Training Loss: 0.9345319846002222\n",
      "Iteration 443, Training Loss: 0.9345279348764816\n",
      "Iteration 444, Training Loss: 0.9345280894528224\n",
      "Iteration 445, Training Loss: 0.9345220288174746\n",
      "Iteration 446, Training Loss: 0.9345205344811663\n",
      "Iteration 447, Training Loss: 0.9345164935517187\n",
      "Iteration 448, Training Loss: 0.9345160619373233\n",
      "Iteration 449, Training Loss: 0.9345100128069143\n",
      "Iteration 450, Training Loss: 0.9345097036006964\n",
      "Iteration 451, Training Loss: 0.9345075124950873\n",
      "Iteration 452, Training Loss: 0.9345028116345367\n",
      "Iteration 453, Training Loss: 0.9345000333148807\n",
      "Iteration 454, Training Loss: 0.9345024568107919\n",
      "Iteration 455, Training Loss: 0.9344922506452048\n",
      "Iteration 456, Training Loss: 0.9344989560349629\n",
      "Iteration 457, Training Loss: 0.934487272918142\n",
      "Iteration 458, Training Loss: 0.9344902598367906\n",
      "Iteration 459, Training Loss: 0.9344842970249123\n",
      "Iteration 460, Training Loss: 0.9344840731753262\n",
      "Iteration 461, Training Loss: 0.9344782927876786\n",
      "Iteration 462, Training Loss: 0.9344801605063006\n",
      "Iteration 463, Training Loss: 0.9344724388536219\n",
      "Iteration 464, Training Loss: 0.9344728612831987\n",
      "Iteration 465, Training Loss: 0.9344695297225726\n",
      "Iteration 466, Training Loss: 0.9344665039558426\n",
      "Iteration 467, Training Loss: 0.9344650539547438\n",
      "Iteration 468, Training Loss: 0.9344624912120568\n",
      "Iteration 469, Training Loss: 0.9344585219653088\n",
      "Iteration 470, Training Loss: 0.9344567147954219\n",
      "Iteration 471, Training Loss: 0.9344574611285216\n",
      "Iteration 472, Training Loss: 0.9344487022817747\n",
      "Iteration 473, Training Loss: 0.9344523320674722\n",
      "Iteration 474, Training Loss: 0.9344453671988576\n",
      "Iteration 475, Training Loss: 0.9344460578348711\n",
      "Iteration 476, Training Loss: 0.9344404555831882\n",
      "Iteration 477, Training Loss: 0.934441707559386\n",
      "Iteration 478, Training Loss: 0.9344360980580748\n",
      "Iteration 479, Training Loss: 0.9344381690454898\n",
      "Iteration 480, Training Loss: 0.934428272267676\n",
      "Iteration 481, Training Loss: 0.9344339559346762\n",
      "Iteration 482, Training Loss: 0.9344252028027249\n",
      "Iteration 483, Training Loss: 0.9344278001993797\n",
      "Iteration 484, Training Loss: 0.9344235618211879\n",
      "Iteration 485, Training Loss: 0.9344204430742008\n",
      "Iteration 486, Training Loss: 0.9344174445693821\n",
      "Iteration 487, Training Loss: 0.9344163842332841\n",
      "Iteration 488, Training Loss: 0.9344134663189521\n",
      "Iteration 489, Training Loss: 0.9344123124974897\n",
      "Iteration 490, Training Loss: 0.9344105742512283\n",
      "Iteration 491, Training Loss: 0.9344065509181478\n",
      "Iteration 492, Training Loss: 0.9344055115690019\n",
      "Iteration 493, Training Loss: 0.9344038883463638\n",
      "Iteration 494, Training Loss: 0.9343983588907249\n",
      "Iteration 495, Training Loss: 0.9344024712830439\n",
      "Iteration 496, Training Loss: 0.9343933086521999\n",
      "Iteration 497, Training Loss: 0.9343966742310623\n",
      "Iteration 498, Training Loss: 0.9343925482158272\n",
      "Iteration 499, Training Loss: 0.9343903006169321\n",
      "Iteration 500, Training Loss: 0.9343905706259311\n",
      "Iteration 501, Training Loss: 0.9343834020137067\n",
      "Iteration 502, Training Loss: 0.9343862352013721\n",
      "Iteration 503, Training Loss: 0.9343791304160889\n",
      "Iteration 504, Training Loss: 0.9343858744102587\n",
      "Iteration 505, Training Loss: 0.9343753699008502\n",
      "Iteration 506, Training Loss: 0.9343795564598393\n",
      "Iteration 507, Training Loss: 0.934372243726845\n",
      "Iteration 508, Training Loss: 0.9343732252125642\n",
      "Iteration 509, Training Loss: 0.9343674616808344\n",
      "Iteration 510, Training Loss: 0.934371663113757\n",
      "Iteration 511, Training Loss: 0.9343626894608488\n",
      "Iteration 512, Training Loss: 0.934367397032081\n",
      "Iteration 513, Training Loss: 0.9343596497923847\n",
      "Iteration 514, Training Loss: 0.9343613081741847\n",
      "Iteration 515, Training Loss: 0.9343556781668031\n",
      "Iteration 516, Training Loss: 0.9343585107139046\n",
      "Iteration 517, Training Loss: 0.934352715661767\n",
      "Iteration 518, Training Loss: 0.934355174748649\n",
      "Iteration 519, Training Loss: 0.9343481271872651\n",
      "Iteration 520, Training Loss: 0.9343538114762012\n",
      "Iteration 521, Training Loss: 0.9343434444359353\n",
      "Iteration 522, Training Loss: 0.9343465570965229\n",
      "Iteration 523, Training Loss: 0.9343407662285553\n",
      "Iteration 524, Training Loss: 0.9343431224703034\n",
      "Iteration 525, Training Loss: 0.9343380744268691\n",
      "Iteration 526, Training Loss: 0.9343413092907769\n",
      "Iteration 527, Training Loss: 0.9343342584140538\n",
      "Iteration 528, Training Loss: 0.9343366441305421\n",
      "Iteration 529, Training Loss: 0.934330335952017\n",
      "Iteration 530, Training Loss: 0.9343328450099276\n",
      "Iteration 531, Training Loss: 0.9343283843354738\n",
      "Iteration 532, Training Loss: 0.9343289219228037\n",
      "Iteration 533, Training Loss: 0.9343229486851667\n",
      "Iteration 534, Training Loss: 0.9343298419974075\n",
      "Iteration 535, Training Loss: 0.9343215871379045\n",
      "Iteration 536, Training Loss: 0.934322785526081\n",
      "Iteration 537, Training Loss: 0.9343189667044035\n",
      "Iteration 538, Training Loss: 0.9343171345937406\n",
      "Iteration 539, Training Loss: 0.9343171813955989\n",
      "Iteration 540, Training Loss: 0.9343111174493606\n",
      "Iteration 541, Training Loss: 0.9343161210046904\n",
      "Iteration 542, Training Loss: 0.9343086447922527\n",
      "Iteration 543, Training Loss: 0.9343130606889554\n",
      "Iteration 544, Training Loss: 0.9343039902302007\n",
      "Iteration 545, Training Loss: 0.934310205263424\n",
      "Iteration 546, Training Loss: 0.9343020661948267\n",
      "Iteration 547, Training Loss: 0.9343047477040596\n",
      "Iteration 548, Training Loss: 0.9343009550662719\n",
      "Iteration 549, Training Loss: 0.9343004665802821\n",
      "Iteration 550, Training Loss: 0.934296915179102\n",
      "Iteration 551, Training Loss: 0.9342990249210132\n",
      "Iteration 552, Training Loss: 0.9342933210373731\n",
      "Iteration 553, Training Loss: 0.934297540037023\n",
      "Iteration 554, Training Loss: 0.9342887819037524\n",
      "Iteration 555, Training Loss: 0.9342952910181535\n",
      "Iteration 556, Training Loss: 0.9342854084075106\n",
      "Iteration 557, Training Loss: 0.9342918157498573\n",
      "Iteration 558, Training Loss: 0.9342870142474965\n",
      "Iteration 559, Training Loss: 0.9342863605466923\n",
      "Iteration 560, Training Loss: 0.9342810867896569\n",
      "Iteration 561, Training Loss: 0.9342850507800206\n",
      "Iteration 562, Training Loss: 0.9342754171989165\n",
      "Iteration 563, Training Loss: 0.9342844803377056\n",
      "Iteration 564, Training Loss: 0.9342757033075156\n",
      "Iteration 565, Training Loss: 0.9342770471000436\n",
      "Iteration 566, Training Loss: 0.9342760850489015\n",
      "Iteration 567, Training Loss: 0.9342707772183657\n",
      "Iteration 568, Training Loss: 0.9342734917009305\n",
      "Iteration 569, Training Loss: 0.9342694638567974\n",
      "Iteration 570, Training Loss: 0.9342686212231378\n",
      "Iteration 571, Training Loss: 0.9342694722389862\n",
      "Iteration 572, Training Loss: 0.9342636211561337\n",
      "Iteration 573, Training Loss: 0.9342687751588946\n",
      "Iteration 574, Training Loss: 0.9342598059420286\n",
      "Iteration 575, Training Loss: 0.934265679248814\n",
      "Iteration 576, Training Loss: 0.9342613627731091\n",
      "Iteration 577, Training Loss: 0.9342586746756922\n",
      "Iteration 578, Training Loss: 0.9342597466579725\n",
      "Iteration 579, Training Loss: 0.9342557601887065\n",
      "Iteration 580, Training Loss: 0.9342572889379812\n",
      "Iteration 581, Training Loss: 0.9342519121664045\n",
      "Iteration 582, Training Loss: 0.9342529335118095\n",
      "Iteration 583, Training Loss: 0.9342508857306137\n",
      "Iteration 584, Training Loss: 0.9342510417582125\n",
      "Iteration 585, Training Loss: 0.9342477692452279\n",
      "Iteration 586, Training Loss: 0.9342513490268735\n",
      "Iteration 587, Training Loss: 0.9342435166333135\n",
      "Iteration 588, Training Loss: 0.9342476554673095\n",
      "Iteration 589, Training Loss: 0.9342405507194742\n",
      "Iteration 590, Training Loss: 0.9342452982925809\n",
      "Iteration 591, Training Loss: 0.9342372664178731\n",
      "Iteration 592, Training Loss: 0.9342411019179415\n",
      "Iteration 593, Training Loss: 0.934237120333899\n",
      "Iteration 594, Training Loss: 0.9342415600373821\n",
      "Iteration 595, Training Loss: 0.9342319140578481\n",
      "Iteration 596, Training Loss: 0.9342387624705557\n",
      "Iteration 597, Training Loss: 0.9342321359561034\n",
      "Iteration 598, Training Loss: 0.9342325675451723\n",
      "Iteration 599, Training Loss: 0.9342311271731696\n",
      "Iteration 600, Training Loss: 0.9342315968632264\n",
      "Iteration 601, Training Loss: 0.9342277576067197\n",
      "Iteration 602, Training Loss: 0.9342295914031166\n",
      "Iteration 603, Training Loss: 0.9342249925328644\n",
      "Iteration 604, Training Loss: 0.9342247416535951\n",
      "Iteration 605, Training Loss: 0.934223984176823\n",
      "Iteration 606, Training Loss: 0.9342222793393141\n",
      "Iteration 607, Training Loss: 0.9342214032583405\n",
      "Iteration 608, Training Loss: 0.9342205077083977\n",
      "Iteration 609, Training Loss: 0.9342197977360411\n",
      "Iteration 610, Training Loss: 0.9342173761304966\n",
      "Iteration 611, Training Loss: 0.9342216881713572\n",
      "Iteration 612, Training Loss: 0.934212159947302\n",
      "Iteration 613, Training Loss: 0.9342183640522798\n",
      "Iteration 614, Training Loss: 0.9342125287858485\n",
      "Iteration 615, Training Loss: 0.9342118866876378\n",
      "Iteration 616, Training Loss: 0.9342124352685357\n",
      "Iteration 617, Training Loss: 0.9342063038346182\n",
      "Iteration 618, Training Loss: 0.9342158680150097\n",
      "Iteration 619, Training Loss: 0.9342037595522272\n",
      "Iteration 620, Training Loss: 0.934208675024199\n",
      "Iteration 621, Training Loss: 0.9342053914904974\n",
      "Iteration 622, Training Loss: 0.9342066205078793\n",
      "Iteration 623, Training Loss: 0.9342027393913036\n",
      "Iteration 624, Training Loss: 0.934205908076387\n",
      "Iteration 625, Training Loss: 0.9341989490629589\n",
      "Iteration 626, Training Loss: 0.93420472359469\n",
      "Iteration 627, Training Loss: 0.9341965935422781\n",
      "Iteration 628, Training Loss: 0.9342035561193807\n",
      "Iteration 629, Training Loss: 0.9341943318492109\n",
      "Iteration 630, Training Loss: 0.9341984189347333\n",
      "Iteration 631, Training Loss: 0.9341952011485634\n",
      "Iteration 632, Training Loss: 0.9341958139497691\n",
      "Iteration 633, Training Loss: 0.9341916059469797\n",
      "Iteration 634, Training Loss: 0.9341961417144707\n",
      "Iteration 635, Training Loss: 0.9341903800135686\n",
      "Iteration 636, Training Loss: 0.9341922067551028\n",
      "Iteration 637, Training Loss: 0.9341867123936902\n",
      "Iteration 638, Training Loss: 0.9341939943599092\n",
      "Iteration 639, Training Loss: 0.9341832984421057\n",
      "Iteration 640, Training Loss: 0.934190732540044\n",
      "Iteration 641, Training Loss: 0.934185677427079\n",
      "Iteration 642, Training Loss: 0.9341856913731695\n",
      "Iteration 643, Training Loss: 0.9341820929332875\n",
      "Iteration 644, Training Loss: 0.9341866239970469\n",
      "Iteration 645, Training Loss: 0.9341774463312486\n",
      "Iteration 646, Training Loss: 0.9341855375811418\n",
      "Iteration 647, Training Loss: 0.9341781338296913\n",
      "Iteration 648, Training Loss: 0.934183259811721\n",
      "Iteration 649, Training Loss: 0.9341757485512588\n",
      "Iteration 650, Training Loss: 0.9341802729666697\n",
      "Iteration 651, Training Loss: 0.9341735038179156\n",
      "Iteration 652, Training Loss: 0.9341779574076747\n",
      "Iteration 653, Training Loss: 0.9341725754800052\n",
      "Iteration 654, Training Loss: 0.9341797755548847\n",
      "Iteration 655, Training Loss: 0.9341691026285079\n",
      "Iteration 656, Training Loss: 0.9341754634998792\n",
      "Iteration 657, Training Loss: 0.9341698201804316\n",
      "Iteration 658, Training Loss: 0.9341705654532327\n",
      "Iteration 659, Training Loss: 0.9341695892740997\n",
      "Iteration 660, Training Loss: 0.9341678436594357\n",
      "Iteration 661, Training Loss: 0.9341681449194584\n",
      "Iteration 662, Training Loss: 0.9341675519485682\n",
      "Iteration 663, Training Loss: 0.9341638681031215\n",
      "Iteration 664, Training Loss: 0.9341680058564573\n",
      "Iteration 665, Training Loss: 0.9341607429279548\n",
      "Iteration 666, Training Loss: 0.9341657104131934\n",
      "Iteration 667, Training Loss: 0.9341603176061901\n",
      "Iteration 668, Training Loss: 0.934164318591593\n",
      "Iteration 669, Training Loss: 0.9341583776447945\n",
      "Iteration 670, Training Loss: 0.9341648306599115\n",
      "Iteration 671, Training Loss: 0.9341555316973859\n",
      "Iteration 672, Training Loss: 0.934161347320051\n",
      "Iteration 673, Training Loss: 0.9341570195263946\n",
      "Iteration 674, Training Loss: 0.934158542680736\n",
      "Iteration 675, Training Loss: 0.9341550639817015\n",
      "Iteration 676, Training Loss: 0.9341547021080062\n",
      "Iteration 677, Training Loss: 0.9341535188146822\n",
      "Iteration 678, Training Loss: 0.9341527261425029\n",
      "Iteration 679, Training Loss: 0.934153502995352\n",
      "Iteration 680, Training Loss: 0.9341500563091037\n",
      "Iteration 681, Training Loss: 0.9341526676526696\n",
      "Iteration 682, Training Loss: 0.9341492813236811\n",
      "Iteration 683, Training Loss: 0.9341520427933051\n",
      "Iteration 684, Training Loss: 0.9341454452965197\n",
      "Iteration 685, Training Loss: 0.9341492558732936\n",
      "Iteration 686, Training Loss: 0.9341482460994466\n",
      "Iteration 687, Training Loss: 0.9341447986864766\n",
      "Iteration 688, Training Loss: 0.9341482581920578\n",
      "Iteration 689, Training Loss: 0.9341435791992219\n",
      "Iteration 690, Training Loss: 0.9341456010433326\n",
      "Iteration 691, Training Loss: 0.9341415325397624\n",
      "Iteration 692, Training Loss: 0.9341462751558985\n",
      "Iteration 693, Training Loss: 0.9341384463770191\n",
      "Iteration 694, Training Loss: 0.9341450013260453\n",
      "Iteration 695, Training Loss: 0.9341395324411687\n",
      "Iteration 696, Training Loss: 0.9341410530009846\n",
      "Iteration 697, Training Loss: 0.9341388516595843\n",
      "Iteration 698, Training Loss: 0.9341390980835893\n",
      "Iteration 699, Training Loss: 0.934135706211951\n",
      "Iteration 700, Training Loss: 0.9341420135523827\n",
      "Iteration 701, Training Loss: 0.9341328659213894\n",
      "Iteration 702, Training Loss: 0.9341379798015225\n",
      "Iteration 703, Training Loss: 0.9341350759385135\n",
      "Iteration 704, Training Loss: 0.9341322671736532\n",
      "Iteration 705, Training Loss: 0.9341359001340905\n",
      "Iteration 706, Training Loss: 0.9341304517763264\n",
      "Iteration 707, Training Loss: 0.9341327628240375\n",
      "Iteration 708, Training Loss: 0.9341300178550636\n",
      "Iteration 709, Training Loss: 0.9341322252784149\n",
      "Iteration 710, Training Loss: 0.9341293847642066\n",
      "Iteration 711, Training Loss: 0.9341303299090193\n",
      "Early stopping at iteration 711\n",
      "Iteration 0, Training Loss: 1.0\n",
      "Iteration 1, Training Loss: 0.9872706329380041\n",
      "Iteration 2, Training Loss: 0.9751777342291081\n",
      "Iteration 3, Training Loss: 0.9640115624939459\n",
      "Iteration 4, Training Loss: 0.9562321684249183\n",
      "Iteration 5, Training Loss: 0.9519035229040276\n",
      "Iteration 6, Training Loss: 0.9487972453347929\n",
      "Iteration 7, Training Loss: 0.9465538827763748\n",
      "Iteration 8, Training Loss: 0.9447715252424518\n",
      "Iteration 9, Training Loss: 0.9433006963475202\n",
      "Iteration 10, Training Loss: 0.9420667129364936\n",
      "Iteration 11, Training Loss: 0.9410211972980537\n",
      "Iteration 12, Training Loss: 0.9401377444310496\n",
      "Iteration 13, Training Loss: 0.9393967763668704\n",
      "Iteration 14, Training Loss: 0.9387385792779109\n",
      "Iteration 15, Training Loss: 0.9381642198111954\n",
      "Iteration 16, Training Loss: 0.9377125118063999\n",
      "Iteration 17, Training Loss: 0.9373396975671584\n",
      "Iteration 18, Training Loss: 0.9370193728985869\n",
      "Iteration 19, Training Loss: 0.9367480452076445\n",
      "Iteration 20, Training Loss: 0.9365071879135737\n",
      "Iteration 21, Training Loss: 0.9362890537522099\n",
      "Iteration 22, Training Loss: 0.93610956173088\n",
      "Iteration 23, Training Loss: 0.9359420394231568\n",
      "Iteration 24, Training Loss: 0.9358174300371198\n",
      "Iteration 25, Training Loss: 0.9357006525936065\n",
      "Iteration 26, Training Loss: 0.935570887575504\n",
      "Iteration 27, Training Loss: 0.9354944779415729\n",
      "Iteration 28, Training Loss: 0.9353741898054143\n",
      "Iteration 29, Training Loss: 0.9352758138317153\n",
      "Iteration 30, Training Loss: 0.9352225078998755\n",
      "Iteration 31, Training Loss: 0.935107714384915\n",
      "Iteration 32, Training Loss: 0.9350601838290581\n",
      "Iteration 33, Training Loss: 0.9349841865280051\n",
      "Iteration 34, Training Loss: 0.9349259050006028\n",
      "Iteration 35, Training Loss: 0.9348453860442103\n",
      "Iteration 36, Training Loss: 0.9348175015377226\n",
      "Iteration 37, Training Loss: 0.9347299289416248\n",
      "Iteration 38, Training Loss: 0.9347490701840706\n",
      "Iteration 39, Training Loss: 0.9346267943722781\n",
      "Iteration 40, Training Loss: 0.9346678461133168\n",
      "Iteration 41, Training Loss: 0.9345691828162022\n",
      "Iteration 42, Training Loss: 0.934581160831382\n",
      "Iteration 43, Training Loss: 0.9345125527995153\n",
      "Iteration 44, Training Loss: 0.9345284349447965\n",
      "Iteration 45, Training Loss: 0.9344496118715537\n",
      "Iteration 46, Training Loss: 0.9344889279207198\n",
      "Iteration 47, Training Loss: 0.9343980858698234\n",
      "Iteration 48, Training Loss: 0.9344327798826623\n",
      "Iteration 49, Training Loss: 0.9343646534175998\n",
      "Iteration 50, Training Loss: 0.9343741795400905\n",
      "Iteration 51, Training Loss: 0.9343228272256549\n",
      "Iteration 52, Training Loss: 0.9343614012074313\n",
      "Iteration 53, Training Loss: 0.9342844778784404\n",
      "Iteration 54, Training Loss: 0.9343506366720863\n",
      "Iteration 55, Training Loss: 0.9342477437323041\n",
      "Iteration 56, Training Loss: 0.9342899823053198\n",
      "Iteration 57, Training Loss: 0.9342065211960279\n",
      "Iteration 58, Training Loss: 0.9342734453921204\n",
      "Iteration 59, Training Loss: 0.9342246071987729\n",
      "Iteration 60, Training Loss: 0.9342169246701801\n",
      "Iteration 61, Training Loss: 0.9341924523451612\n",
      "Iteration 62, Training Loss: 0.9342112742520141\n",
      "Iteration 63, Training Loss: 0.9341493252223938\n",
      "Iteration 64, Training Loss: 0.9342292188600523\n",
      "Iteration 65, Training Loss: 0.9341164910048934\n",
      "Iteration 66, Training Loss: 0.9342196430282301\n",
      "Iteration 67, Training Loss: 0.934135477402983\n",
      "Iteration 68, Training Loss: 0.9341637594963277\n",
      "Iteration 69, Training Loss: 0.9341225793011684\n",
      "Iteration 70, Training Loss: 0.9341506401251016\n",
      "Iteration 71, Training Loss: 0.9340979580790872\n",
      "Iteration 72, Training Loss: 0.9341828476655586\n",
      "Iteration 73, Training Loss: 0.9340628095059926\n",
      "Iteration 74, Training Loss: 0.9341441810174809\n",
      "Iteration 75, Training Loss: 0.9340834430754986\n",
      "Iteration 76, Training Loss: 0.9341077847140957\n",
      "Iteration 77, Training Loss: 0.9341134040548501\n",
      "Iteration 78, Training Loss: 0.9340572137996963\n",
      "Iteration 79, Training Loss: 0.9341269236936506\n",
      "Iteration 80, Training Loss: 0.9340497629475776\n",
      "Iteration 81, Training Loss: 0.9340619739135807\n",
      "Iteration 82, Training Loss: 0.9340813267712329\n",
      "Iteration 83, Training Loss: 0.9340554232889269\n",
      "Early stopping at iteration 83\n",
      "Iteration 0, Training Loss: 1.0\n",
      "Iteration 1, Training Loss: 0.9987270632938003\n",
      "Iteration 2, Training Loss: 0.9974668559546628\n",
      "Iteration 3, Training Loss: 0.9962192506889166\n",
      "Iteration 4, Training Loss: 0.9949841214758279\n",
      "Iteration 5, Training Loss: 0.9937613435548701\n",
      "Iteration 6, Training Loss: 0.9925507934131217\n",
      "Iteration 7, Training Loss: 0.991352348772791\n",
      "Iteration 8, Training Loss: 0.9901658885788635\n",
      "Iteration 9, Training Loss: 0.9889912929868753\n",
      "Iteration 10, Training Loss: 0.9878284433508069\n",
      "Iteration 11, Training Loss: 0.9866772222110992\n",
      "Iteration 12, Training Loss: 0.9855375132827887\n",
      "Iteration 13, Training Loss: 0.9844092014437612\n",
      "Iteration 14, Training Loss: 0.983292172723124\n",
      "Iteration 15, Training Loss: 0.9821863142896932\n",
      "Iteration 16, Training Loss: 0.9810915144405967\n",
      "Iteration 17, Training Loss: 0.9800076625899911\n",
      "Iteration 18, Training Loss: 0.9789346492578915\n",
      "Iteration 19, Training Loss: 0.9778723660591131\n",
      "Iteration 20, Training Loss: 0.9768207056923225\n",
      "Iteration 21, Training Loss: 0.9757795619291996\n",
      "Iteration 22, Training Loss: 0.974748829603708\n",
      "Iteration 23, Training Loss: 0.9737297636615398\n",
      "Iteration 24, Training Loss: 0.9727303114734748\n",
      "Iteration 25, Training Loss: 0.9717408538072905\n",
      "Iteration 26, Training Loss: 0.970761290717768\n",
      "Iteration 27, Training Loss: 0.9697917632520447\n",
      "Iteration 28, Training Loss: 0.9688434106588167\n",
      "Iteration 29, Training Loss: 0.9679383565867292\n",
      "Iteration 30, Training Loss: 0.9670646147391974\n",
      "Iteration 31, Training Loss: 0.9662274833501713\n",
      "Iteration 32, Training Loss: 0.9654126207451893\n",
      "Iteration 33, Training Loss: 0.9646295180371829\n",
      "Iteration 34, Training Loss: 0.9638645181766229\n",
      "Iteration 35, Training Loss: 0.963135098431703\n",
      "Iteration 36, Training Loss: 0.96246010798661\n",
      "Iteration 37, Training Loss: 0.9618168457943393\n",
      "Iteration 38, Training Loss: 0.9612036850288163\n",
      "Iteration 39, Training Loss: 0.9606256309502624\n",
      "Iteration 40, Training Loss: 0.9600909181606639\n",
      "Iteration 41, Training Loss: 0.9595840994239229\n",
      "Iteration 42, Training Loss: 0.9591103087707095\n",
      "Iteration 43, Training Loss: 0.958654645031439\n",
      "Iteration 44, Training Loss: 0.958230541999649\n",
      "Iteration 45, Training Loss: 0.957824774031782\n",
      "Iteration 46, Training Loss: 0.9574385132649554\n",
      "Iteration 47, Training Loss: 0.9570663346373789\n",
      "Iteration 48, Training Loss: 0.9567065926993978\n",
      "Iteration 49, Training Loss: 0.9563623657172572\n",
      "Iteration 50, Training Loss: 0.9560246048659817\n",
      "Iteration 51, Training Loss: 0.9557002859429963\n",
      "Iteration 52, Training Loss: 0.9553908997558489\n",
      "Iteration 53, Training Loss: 0.955094571572441\n",
      "Iteration 54, Training Loss: 0.954808728437011\n",
      "Iteration 55, Training Loss: 0.9545273357417128\n",
      "Iteration 56, Training Loss: 0.9542546246585326\n",
      "Iteration 57, Training Loss: 0.9539858193571863\n",
      "Iteration 58, Training Loss: 0.9537239889266477\n",
      "Iteration 59, Training Loss: 0.9534700850430515\n",
      "Iteration 60, Training Loss: 0.9532324963757466\n",
      "Iteration 61, Training Loss: 0.9530029735046458\n",
      "Iteration 62, Training Loss: 0.9527830280315531\n",
      "Iteration 63, Training Loss: 0.9525675562269244\n",
      "Iteration 64, Training Loss: 0.9523568305023222\n",
      "Iteration 65, Training Loss: 0.9521545826819\n",
      "Iteration 66, Training Loss: 0.9519553685462956\n",
      "Iteration 67, Training Loss: 0.9517664047962433\n",
      "Iteration 68, Training Loss: 0.9515828890148815\n",
      "Iteration 69, Training Loss: 0.9514029972587228\n",
      "Iteration 70, Training Loss: 0.9512270863621858\n",
      "Iteration 71, Training Loss: 0.9510555302558221\n",
      "Iteration 72, Training Loss: 0.9508894869226081\n",
      "Iteration 73, Training Loss: 0.9507251040227261\n",
      "Iteration 74, Training Loss: 0.9505635669639915\n",
      "Iteration 75, Training Loss: 0.9504109253614335\n",
      "Iteration 76, Training Loss: 0.9502635867526371\n",
      "Iteration 77, Training Loss: 0.9501213055715557\n",
      "Iteration 78, Training Loss: 0.949980517388448\n",
      "Iteration 79, Training Loss: 0.9498402188430661\n",
      "Iteration 80, Training Loss: 0.9497013232831382\n",
      "Iteration 81, Training Loss: 0.9495643388201217\n",
      "Iteration 82, Training Loss: 0.9494324759171389\n",
      "Iteration 83, Training Loss: 0.9493008680846057\n",
      "Iteration 84, Training Loss: 0.9491749459174378\n",
      "Iteration 85, Training Loss: 0.9490505673415107\n",
      "Iteration 86, Training Loss: 0.9489315837526976\n",
      "Iteration 87, Training Loss: 0.9488176008965008\n",
      "Iteration 88, Training Loss: 0.9487052132811441\n",
      "Iteration 89, Training Loss: 0.9485971516925364\n",
      "Iteration 90, Training Loss: 0.9484901707198151\n",
      "Iteration 91, Training Loss: 0.9483843316874652\n",
      "Iteration 92, Training Loss: 0.948281437479061\n",
      "Iteration 93, Training Loss: 0.9481802469060764\n",
      "Iteration 94, Training Loss: 0.9480815936300626\n",
      "Iteration 95, Training Loss: 0.9479840577800205\n",
      "Iteration 96, Training Loss: 0.9478919558076955\n",
      "Iteration 97, Training Loss: 0.9478001300747622\n",
      "Iteration 98, Training Loss: 0.9477110222808852\n",
      "Iteration 99, Training Loss: 0.9476300578071382\n",
      "Iteration 100, Training Loss: 0.9475474250961802\n",
      "Iteration 101, Training Loss: 0.9474656187123319\n",
      "Iteration 102, Training Loss: 0.9473846524618493\n",
      "Iteration 103, Training Loss: 0.9473059051034375\n",
      "Iteration 104, Training Loss: 0.9472255359227126\n",
      "Iteration 105, Training Loss: 0.9471486596903068\n",
      "Iteration 106, Training Loss: 0.9470740622433989\n",
      "Iteration 107, Training Loss: 0.9470002250183829\n",
      "Iteration 108, Training Loss: 0.9469284806791298\n",
      "Iteration 109, Training Loss: 0.946857658225185\n",
      "Iteration 110, Training Loss: 0.9467875439957795\n",
      "Iteration 111, Training Loss: 0.9467181469988676\n",
      "Iteration 112, Training Loss: 0.9466509972890886\n",
      "Iteration 113, Training Loss: 0.9465821055037572\n",
      "Iteration 114, Training Loss: 0.9465149780205027\n",
      "Iteration 115, Training Loss: 0.9464501505110119\n",
      "Iteration 116, Training Loss: 0.9463861354996846\n",
      "Iteration 117, Training Loss: 0.9463255817816397\n",
      "Iteration 118, Training Loss: 0.9462658732163345\n",
      "Iteration 119, Training Loss: 0.9462067617366824\n",
      "Iteration 120, Training Loss: 0.9461482704070036\n",
      "Iteration 121, Training Loss: 0.9460917378180371\n",
      "Iteration 122, Training Loss: 0.9460346524309415\n",
      "Iteration 123, Training Loss: 0.9459806179364382\n",
      "Iteration 124, Training Loss: 0.9459271237868798\n",
      "Iteration 125, Training Loss: 0.945874164578817\n",
      "Iteration 126, Training Loss: 0.945821734962835\n",
      "Iteration 127, Training Loss: 0.9457698296430126\n",
      "Iteration 128, Training Loss: 0.9457184433763887\n",
      "Iteration 129, Training Loss: 0.9456675728651531\n",
      "Iteration 130, Training Loss: 0.9456162134883118\n",
      "Iteration 131, Training Loss: 0.9455652539356252\n",
      "Iteration 132, Training Loss: 0.945515913426075\n",
      "Iteration 133, Training Loss: 0.9454671033481535\n",
      "Iteration 134, Training Loss: 0.9454176118862171\n",
      "Iteration 135, Training Loss: 0.9453686153388999\n",
      "Iteration 136, Training Loss: 0.9453201751171546\n",
      "Iteration 137, Training Loss: 0.9452724543380466\n",
      "Iteration 138, Training Loss: 0.9452277139070515\n",
      "Iteration 139, Training Loss: 0.9451837335300898\n",
      "Iteration 140, Training Loss: 0.9451403854328818\n",
      "Iteration 141, Training Loss: 0.9450986995200302\n",
      "Iteration 142, Training Loss: 0.9450576131331866\n",
      "Iteration 143, Training Loss: 0.9450169750314319\n",
      "Iteration 144, Training Loss: 0.9449768652611155\n",
      "Iteration 145, Training Loss: 0.9449397748915432\n",
      "Iteration 146, Training Loss: 0.9449030554256668\n",
      "Iteration 147, Training Loss: 0.9448667031544492\n",
      "Iteration 148, Training Loss: 0.9448309010700949\n",
      "Iteration 149, Training Loss: 0.9447941239719881\n",
      "Iteration 150, Training Loss: 0.9447588610153073\n",
      "Iteration 151, Training Loss: 0.9447239506881934\n",
      "Iteration 152, Training Loss: 0.9446894144463255\n",
      "Iteration 153, Training Loss: 0.9446553809217636\n",
      "Iteration 154, Training Loss: 0.9446229925110133\n",
      "Iteration 155, Training Loss: 0.9445922669756515\n",
      "Iteration 156, Training Loss: 0.944563160592144\n",
      "Iteration 157, Training Loss: 0.9445333247265626\n",
      "Iteration 158, Training Loss: 0.9445047705919981\n",
      "Iteration 159, Training Loss: 0.9444766126830646\n",
      "Iteration 160, Training Loss: 0.944448767336998\n",
      "Iteration 161, Training Loss: 0.944422478882965\n",
      "Iteration 162, Training Loss: 0.9443952059430576\n",
      "Iteration 163, Training Loss: 0.944369459937159\n",
      "Iteration 164, Training Loss: 0.9443454515117842\n",
      "Iteration 165, Training Loss: 0.9443194837040603\n",
      "Iteration 166, Training Loss: 0.9442961058865654\n",
      "Iteration 167, Training Loss: 0.9442716668951636\n",
      "Iteration 168, Training Loss: 0.9442489696669589\n",
      "Iteration 169, Training Loss: 0.9442264759227813\n",
      "Iteration 170, Training Loss: 0.9442042117048985\n",
      "Iteration 171, Training Loss: 0.944184727994253\n",
      "Iteration 172, Training Loss: 0.944160407563809\n",
      "Iteration 173, Training Loss: 0.9441428869064089\n",
      "Iteration 174, Training Loss: 0.9441203865298913\n",
      "Iteration 175, Training Loss: 0.9441030398443749\n",
      "Iteration 176, Training Loss: 0.944082098181456\n",
      "Iteration 177, Training Loss: 0.9440638918278518\n",
      "Iteration 178, Training Loss: 0.9440445880556665\n",
      "Iteration 179, Training Loss: 0.9440267636307442\n",
      "Iteration 180, Training Loss: 0.9440078323175417\n",
      "Iteration 181, Training Loss: 0.9439880840916969\n",
      "Iteration 182, Training Loss: 0.9439697335348962\n",
      "Iteration 183, Training Loss: 0.9439477389263223\n",
      "Iteration 184, Training Loss: 0.9439321232230538\n",
      "Iteration 185, Training Loss: 0.9439129642481714\n",
      "Iteration 186, Training Loss: 0.9438952663814325\n",
      "Iteration 187, Training Loss: 0.9438791639393521\n",
      "Iteration 188, Training Loss: 0.943863236236879\n",
      "Iteration 189, Training Loss: 0.9438476388898279\n",
      "Iteration 190, Training Loss: 0.9438309252181021\n",
      "Iteration 191, Training Loss: 0.9438180070003325\n",
      "Iteration 192, Training Loss: 0.9438002219372904\n",
      "Iteration 193, Training Loss: 0.9437851061400814\n",
      "Iteration 194, Training Loss: 0.9437713616893775\n",
      "Iteration 195, Training Loss: 0.9437552814748562\n",
      "Iteration 196, Training Loss: 0.9437418559629306\n",
      "Iteration 197, Training Loss: 0.9437262475032114\n",
      "Iteration 198, Training Loss: 0.9437119620967012\n",
      "Iteration 199, Training Loss: 0.9436990421104177\n",
      "Iteration 200, Training Loss: 0.9436811993660026\n",
      "Iteration 201, Training Loss: 0.9436697729337435\n",
      "Iteration 202, Training Loss: 0.9436574303801986\n",
      "Iteration 203, Training Loss: 0.9436426207654394\n",
      "Iteration 204, Training Loss: 0.9436316894988058\n",
      "Iteration 205, Training Loss: 0.9436186968931097\n",
      "Iteration 206, Training Loss: 0.9436056391583987\n",
      "Iteration 207, Training Loss: 0.9435963658505719\n",
      "Iteration 208, Training Loss: 0.9435823075530695\n",
      "Iteration 209, Training Loss: 0.9435733817572963\n",
      "Iteration 210, Training Loss: 0.9435594990803922\n",
      "Iteration 211, Training Loss: 0.9435507802291495\n",
      "Iteration 212, Training Loss: 0.9435371703633548\n",
      "Iteration 213, Training Loss: 0.943524062766297\n",
      "Iteration 214, Training Loss: 0.9435132108888656\n",
      "Iteration 215, Training Loss: 0.9435012332126387\n",
      "Iteration 216, Training Loss: 0.9434918483166704\n",
      "Iteration 217, Training Loss: 0.9434800649540535\n",
      "Iteration 218, Training Loss: 0.9434672233790772\n",
      "Iteration 219, Training Loss: 0.9434594158946075\n",
      "Iteration 220, Training Loss: 0.9434468297260312\n",
      "Iteration 221, Training Loss: 0.9434345010893371\n",
      "Iteration 222, Training Loss: 0.9434270188775333\n",
      "Iteration 223, Training Loss: 0.9434146315446855\n",
      "Iteration 224, Training Loss: 0.9434049007256001\n",
      "Iteration 225, Training Loss: 0.9433939659863076\n",
      "Iteration 226, Training Loss: 0.9433857051920769\n",
      "Iteration 227, Training Loss: 0.9433713123177245\n",
      "Iteration 228, Training Loss: 0.9433673627000865\n",
      "Iteration 229, Training Loss: 0.943351838774623\n",
      "Iteration 230, Training Loss: 0.9433439280286583\n",
      "Iteration 231, Training Loss: 0.9433348564668924\n",
      "Iteration 232, Training Loss: 0.9433259240966213\n",
      "Iteration 233, Training Loss: 0.9433196719043133\n",
      "Iteration 234, Training Loss: 0.9433071967179416\n",
      "Iteration 235, Training Loss: 0.9433035419901011\n",
      "Iteration 236, Training Loss: 0.9432927547384957\n",
      "Iteration 237, Training Loss: 0.9432842588333852\n",
      "Iteration 238, Training Loss: 0.9432783619885131\n",
      "Iteration 239, Training Loss: 0.9432687482321533\n",
      "Iteration 240, Training Loss: 0.9432604936814857\n",
      "Iteration 241, Training Loss: 0.9432536239741677\n",
      "Iteration 242, Training Loss: 0.9432443347435828\n",
      "Iteration 243, Training Loss: 0.9432402334499579\n",
      "Iteration 244, Training Loss: 0.9432311654460563\n",
      "Iteration 245, Training Loss: 0.9432258424229245\n",
      "Iteration 246, Training Loss: 0.9432144095727948\n",
      "Iteration 247, Training Loss: 0.9432116073169454\n",
      "Iteration 248, Training Loss: 0.9432051846564677\n",
      "Iteration 249, Training Loss: 0.9431963117224819\n",
      "Iteration 250, Training Loss: 0.9431925594010051\n",
      "Iteration 251, Training Loss: 0.9431853817216798\n",
      "Iteration 252, Training Loss: 0.9431755398954534\n",
      "Iteration 253, Training Loss: 0.9431707070772299\n",
      "Iteration 254, Training Loss: 0.9431646730648487\n",
      "Iteration 255, Training Loss: 0.9431562415298449\n",
      "Iteration 256, Training Loss: 0.9431516792848911\n",
      "Iteration 257, Training Loss: 0.9431421893694543\n",
      "Iteration 258, Training Loss: 0.9431402709819827\n",
      "Iteration 259, Training Loss: 0.9431310489691032\n",
      "Iteration 260, Training Loss: 0.9431279245502073\n",
      "Iteration 261, Training Loss: 0.9431185858349986\n",
      "Iteration 262, Training Loss: 0.9431155804260138\n",
      "Iteration 263, Training Loss: 0.9431089043500417\n",
      "Iteration 264, Training Loss: 0.943099862354187\n",
      "Iteration 265, Training Loss: 0.9430994973360316\n",
      "Iteration 266, Training Loss: 0.9430931530778022\n",
      "Iteration 267, Training Loss: 0.943084296381493\n",
      "Iteration 268, Training Loss: 0.9430803508615911\n",
      "Iteration 269, Training Loss: 0.9430739925045711\n",
      "Iteration 270, Training Loss: 0.9430702858939879\n",
      "Iteration 271, Training Loss: 0.9430627834811149\n",
      "Iteration 272, Training Loss: 0.9430578209516839\n",
      "Iteration 273, Training Loss: 0.943052976067667\n",
      "Iteration 274, Training Loss: 0.9430459515426806\n",
      "Iteration 275, Training Loss: 0.9430424343035809\n",
      "Iteration 276, Training Loss: 0.9430340075531924\n",
      "Iteration 277, Training Loss: 0.943033149858544\n",
      "Iteration 278, Training Loss: 0.9430259494720544\n",
      "Iteration 279, Training Loss: 0.9430239314236065\n",
      "Iteration 280, Training Loss: 0.9430155936314134\n",
      "Iteration 281, Training Loss: 0.9430125587696392\n",
      "Iteration 282, Training Loss: 0.9430057999255691\n",
      "Iteration 283, Training Loss: 0.943001436946063\n",
      "Iteration 284, Training Loss: 0.9429958899839349\n",
      "Iteration 285, Training Loss: 0.9429929767298649\n",
      "Iteration 286, Training Loss: 0.9429850346635965\n",
      "Iteration 287, Training Loss: 0.9429833219102371\n",
      "Iteration 288, Training Loss: 0.942976695749888\n",
      "Iteration 289, Training Loss: 0.9429741744329688\n",
      "Iteration 290, Training Loss: 0.9429676183553892\n",
      "Iteration 291, Training Loss: 0.9429649236897949\n",
      "Iteration 292, Training Loss: 0.9429572439323122\n",
      "Iteration 293, Training Loss: 0.9429571278462582\n",
      "Iteration 294, Training Loss: 0.9429495020539425\n",
      "Iteration 295, Training Loss: 0.9429482223778473\n",
      "Iteration 296, Training Loss: 0.9429420094892934\n",
      "Iteration 297, Training Loss: 0.9429384718053664\n",
      "Iteration 298, Training Loss: 0.9429335625887245\n",
      "Iteration 299, Training Loss: 0.9429299858898555\n",
      "Iteration 300, Training Loss: 0.9429227303270099\n",
      "Iteration 301, Training Loss: 0.9429241546285713\n",
      "Iteration 302, Training Loss: 0.9429143895014132\n",
      "Iteration 303, Training Loss: 0.9429133585338945\n",
      "Iteration 304, Training Loss: 0.9429089330812822\n",
      "Iteration 305, Training Loss: 0.9429056843863657\n",
      "Iteration 306, Training Loss: 0.9428997917896778\n",
      "Iteration 307, Training Loss: 0.9428990050905924\n",
      "Iteration 308, Training Loss: 0.9428931737529512\n",
      "Iteration 309, Training Loss: 0.942891177766411\n",
      "Iteration 310, Training Loss: 0.9428843292972495\n",
      "Iteration 311, Training Loss: 0.9428849430855437\n",
      "Iteration 312, Training Loss: 0.9428771146078975\n",
      "Iteration 313, Training Loss: 0.9428765039110951\n",
      "Iteration 314, Training Loss: 0.942869748900761\n",
      "Iteration 315, Training Loss: 0.9428691759213246\n",
      "Iteration 316, Training Loss: 0.9428649448778933\n",
      "Iteration 317, Training Loss: 0.9428607831309175\n",
      "Iteration 318, Training Loss: 0.9428553709022239\n",
      "Iteration 319, Training Loss: 0.9428565041802984\n",
      "Iteration 320, Training Loss: 0.9428475213454633\n",
      "Iteration 321, Training Loss: 0.9428471932918385\n",
      "Iteration 322, Training Loss: 0.9428444280755284\n",
      "Iteration 323, Training Loss: 0.9428392195300728\n",
      "Iteration 324, Training Loss: 0.942840279802551\n",
      "Iteration 325, Training Loss: 0.9428350109407916\n",
      "Iteration 326, Training Loss: 0.9428311327568571\n",
      "Iteration 327, Training Loss: 0.9428302050103486\n",
      "Iteration 328, Training Loss: 0.9428214626216975\n",
      "Iteration 329, Training Loss: 0.9428213198334506\n",
      "Iteration 330, Training Loss: 0.9428175489089451\n",
      "Iteration 331, Training Loss: 0.9428162926046405\n",
      "Iteration 332, Training Loss: 0.9428101186370424\n",
      "Iteration 333, Training Loss: 0.9428102042622952\n",
      "Iteration 334, Training Loss: 0.9428054091471172\n",
      "Iteration 335, Training Loss: 0.9428045234613254\n",
      "Iteration 336, Training Loss: 0.9427984418074279\n",
      "Iteration 337, Training Loss: 0.942799905624153\n",
      "Iteration 338, Training Loss: 0.9427951195785671\n",
      "Iteration 339, Training Loss: 0.942794100526641\n",
      "Iteration 340, Training Loss: 0.9427881729553339\n",
      "Iteration 341, Training Loss: 0.9427884143634758\n",
      "Iteration 342, Training Loss: 0.9427828189561628\n",
      "Iteration 343, Training Loss: 0.9427819701140652\n",
      "Iteration 344, Training Loss: 0.9427811065660108\n",
      "Iteration 345, Training Loss: 0.942775307483917\n",
      "Iteration 346, Training Loss: 0.9427771015066999\n",
      "Iteration 347, Training Loss: 0.9427713063300782\n",
      "Iteration 348, Training Loss: 0.9427716528060945\n",
      "Iteration 349, Training Loss: 0.9427672066230498\n",
      "Iteration 350, Training Loss: 0.942768022436258\n",
      "Iteration 351, Training Loss: 0.94276232509796\n",
      "Iteration 352, Training Loss: 0.9427628792193898\n",
      "Iteration 353, Training Loss: 0.942758465926702\n",
      "Iteration 354, Training Loss: 0.942759097205621\n",
      "Iteration 355, Training Loss: 0.9427534705865506\n",
      "Iteration 356, Training Loss: 0.9427566624055854\n",
      "Iteration 357, Training Loss: 0.9427499752532987\n",
      "Iteration 358, Training Loss: 0.9427495903784844\n",
      "Iteration 359, Training Loss: 0.9427452915794116\n",
      "Iteration 360, Training Loss: 0.942746050711989\n",
      "Iteration 361, Training Loss: 0.9427430027824155\n",
      "Iteration 362, Training Loss: 0.9427412787754503\n",
      "Iteration 363, Training Loss: 0.9427383955102204\n",
      "Iteration 364, Training Loss: 0.9427391756875345\n",
      "Iteration 365, Training Loss: 0.9427341198044898\n",
      "Iteration 366, Training Loss: 0.9427362207483596\n",
      "Iteration 367, Training Loss: 0.9427307933248259\n",
      "Iteration 368, Training Loss: 0.9427316777743883\n",
      "Iteration 369, Training Loss: 0.9427263469400562\n",
      "Iteration 370, Training Loss: 0.9427285021612191\n",
      "Iteration 371, Training Loss: 0.9427244379792072\n",
      "Iteration 372, Training Loss: 0.9427218031962226\n",
      "Iteration 373, Training Loss: 0.9427243287876163\n",
      "Iteration 374, Training Loss: 0.9427141187992449\n",
      "Iteration 375, Training Loss: 0.9427175227464379\n",
      "Iteration 376, Training Loss: 0.9427186378985855\n",
      "Iteration 377, Training Loss: 0.9427133406141909\n",
      "Iteration 378, Training Loss: 0.9427131505941759\n",
      "Iteration 379, Training Loss: 0.9427117147995298\n",
      "Iteration 380, Training Loss: 0.9427105038462582\n",
      "Iteration 381, Training Loss: 0.9427079804958142\n",
      "Iteration 382, Training Loss: 0.9427066590728341\n",
      "Iteration 383, Training Loss: 0.9427027803600555\n",
      "Iteration 384, Training Loss: 0.9427039666270749\n",
      "Iteration 385, Training Loss: 0.9427001318185535\n",
      "Iteration 386, Training Loss: 0.9427014429686824\n",
      "Iteration 387, Training Loss: 0.9426940099370272\n",
      "Iteration 388, Training Loss: 0.9426979118978102\n",
      "Iteration 389, Training Loss: 0.9426953840945582\n",
      "Iteration 390, Training Loss: 0.9426941272297975\n",
      "Iteration 391, Training Loss: 0.9426929188429185\n",
      "Iteration 392, Training Loss: 0.9426916684532144\n",
      "Iteration 393, Training Loss: 0.9426891881779413\n",
      "Iteration 394, Training Loss: 0.942691873819269\n",
      "Iteration 395, Training Loss: 0.9426857829705872\n",
      "Iteration 396, Training Loss: 0.942686017921599\n",
      "Iteration 397, Training Loss: 0.9426835767500195\n",
      "Iteration 398, Training Loss: 0.9426849464003946\n",
      "Iteration 399, Training Loss: 0.942680096886419\n",
      "Iteration 400, Training Loss: 0.9426828646986793\n",
      "Iteration 401, Training Loss: 0.9426768486031881\n",
      "Iteration 402, Training Loss: 0.9426793632993552\n",
      "Iteration 403, Training Loss: 0.94267731851958\n",
      "Iteration 404, Training Loss: 0.9426737603864828\n",
      "Iteration 405, Training Loss: 0.9426752440355435\n",
      "Iteration 406, Training Loss: 0.9426716940379454\n",
      "Iteration 407, Training Loss: 0.9426719274838856\n",
      "Iteration 408, Training Loss: 0.9426708927484966\n",
      "Iteration 409, Training Loss: 0.9426711685719646\n",
      "Iteration 410, Training Loss: 0.9426653633292675\n",
      "Iteration 411, Training Loss: 0.9426695618218286\n",
      "Iteration 412, Training Loss: 0.942663580437384\n",
      "Iteration 413, Training Loss: 0.9426651198440175\n",
      "Iteration 414, Training Loss: 0.9426629385930246\n",
      "Iteration 415, Training Loss: 0.9426633202323322\n",
      "Iteration 416, Training Loss: 0.942658639859403\n",
      "Iteration 417, Training Loss: 0.9426602348989254\n",
      "Iteration 418, Training Loss: 0.94265595089312\n",
      "Iteration 419, Training Loss: 0.9426588616046027\n",
      "Iteration 420, Training Loss: 0.9426554947887951\n",
      "Iteration 421, Training Loss: 0.942657114089993\n",
      "Iteration 422, Training Loss: 0.9426525062037908\n",
      "Iteration 423, Training Loss: 0.942656677962974\n",
      "Iteration 424, Training Loss: 0.9426532997367375\n",
      "Iteration 425, Training Loss: 0.9426512913581984\n",
      "Iteration 426, Training Loss: 0.9426507475084017\n",
      "Iteration 427, Training Loss: 0.9426474919781139\n",
      "Iteration 428, Training Loss: 0.9426479195989353\n",
      "Iteration 429, Training Loss: 0.9426484163039929\n",
      "Iteration 430, Training Loss: 0.9426451952858235\n",
      "Iteration 431, Training Loss: 0.9426444076404332\n",
      "Iteration 432, Training Loss: 0.9426437212730799\n",
      "Iteration 433, Training Loss: 0.9426456683516699\n",
      "Iteration 434, Training Loss: 0.9426377143316583\n",
      "Iteration 435, Training Loss: 0.9426431236212184\n",
      "Iteration 436, Training Loss: 0.9426399178132622\n",
      "Iteration 437, Training Loss: 0.9426417286727481\n",
      "Iteration 438, Training Loss: 0.9426360866132519\n",
      "Iteration 439, Training Loss: 0.9426390955992083\n",
      "Iteration 440, Training Loss: 0.9426358980917562\n",
      "Iteration 441, Training Loss: 0.9426380930392354\n",
      "Iteration 442, Training Loss: 0.9426336591581275\n",
      "Iteration 443, Training Loss: 0.9426342606879768\n",
      "Iteration 444, Training Loss: 0.942630051009836\n",
      "Iteration 445, Training Loss: 0.9426369760695608\n",
      "Iteration 446, Training Loss: 0.9426312598431912\n",
      "Iteration 447, Training Loss: 0.9426330770669858\n",
      "Iteration 448, Training Loss: 0.9426300565230974\n",
      "Iteration 449, Training Loss: 0.9426296632892485\n",
      "Iteration 450, Training Loss: 0.9426290779749381\n",
      "Iteration 451, Training Loss: 0.9426297912219521\n",
      "Iteration 452, Training Loss: 0.9426242354450033\n",
      "Iteration 453, Training Loss: 0.9426287277830694\n",
      "Iteration 454, Training Loss: 0.942623128421554\n",
      "Iteration 455, Training Loss: 0.9426288275597126\n",
      "Iteration 456, Training Loss: 0.9426247300529486\n",
      "Iteration 457, Training Loss: 0.9426255178195866\n",
      "Iteration 458, Training Loss: 0.9426237158064635\n",
      "Iteration 459, Training Loss: 0.9426207686166777\n",
      "Iteration 460, Training Loss: 0.9426226753733151\n",
      "Iteration 461, Training Loss: 0.9426209103631538\n",
      "Iteration 462, Training Loss: 0.942619275198957\n",
      "Iteration 463, Training Loss: 0.9426200154920568\n",
      "Iteration 464, Training Loss: 0.9426172814417455\n",
      "Iteration 465, Training Loss: 0.9426180534631416\n",
      "Iteration 466, Training Loss: 0.9426163584842127\n",
      "Iteration 467, Training Loss: 0.9426183796271774\n",
      "Iteration 468, Training Loss: 0.9426166555311359\n",
      "Iteration 469, Training Loss: 0.9426174617646089\n",
      "Iteration 470, Training Loss: 0.9426120420947609\n",
      "Iteration 471, Training Loss: 0.9426155109128347\n",
      "Iteration 472, Training Loss: 0.9426114891844486\n",
      "Iteration 473, Training Loss: 0.9426148526037663\n",
      "Iteration 474, Training Loss: 0.9426094420247396\n",
      "Iteration 475, Training Loss: 0.9426153713501624\n",
      "Iteration 476, Training Loss: 0.942609866795123\n",
      "Iteration 477, Training Loss: 0.9426094344995853\n",
      "Iteration 478, Training Loss: 0.942610266750085\n",
      "Iteration 479, Training Loss: 0.9426101647127767\n",
      "Iteration 480, Training Loss: 0.9426073117154086\n",
      "Iteration 481, Training Loss: 0.9426082079406354\n",
      "Iteration 482, Training Loss: 0.9426065855364948\n",
      "Iteration 483, Training Loss: 0.9426074728014375\n",
      "Iteration 484, Training Loss: 0.9426058703621427\n",
      "Iteration 485, Training Loss: 0.942605479499488\n",
      "Iteration 486, Training Loss: 0.9426064626656668\n",
      "Iteration 487, Training Loss: 0.9426052053080718\n",
      "Iteration 488, Training Loss: 0.9426012882475344\n",
      "Iteration 489, Training Loss: 0.9426045726197986\n",
      "Iteration 490, Training Loss: 0.9426017165628059\n",
      "Iteration 491, Training Loss: 0.9426038800635008\n",
      "Iteration 492, Training Loss: 0.9426010277203559\n",
      "Iteration 493, Training Loss: 0.9426033047914232\n",
      "Iteration 494, Training Loss: 0.9426006675880777\n",
      "Iteration 495, Training Loss: 0.9426004536269873\n",
      "Iteration 496, Training Loss: 0.9425988668174855\n",
      "Iteration 497, Training Loss: 0.9426010929962482\n",
      "Iteration 498, Training Loss: 0.9425983534257891\n",
      "Iteration 499, Training Loss: 0.9426005599542404\n",
      "Iteration 500, Training Loss: 0.9425953722752297\n",
      "Iteration 501, Training Loss: 0.94260134024265\n",
      "Iteration 502, Training Loss: 0.9425926946613022\n",
      "Iteration 503, Training Loss: 0.9425960633677105\n",
      "Iteration 504, Training Loss: 0.942597064752726\n",
      "Iteration 505, Training Loss: 0.9425955502749099\n",
      "Iteration 506, Training Loss: 0.9425953321923086\n",
      "Iteration 507, Training Loss: 0.9425938258977468\n",
      "Iteration 508, Training Loss: 0.9425948495958705\n",
      "Iteration 509, Training Loss: 0.9425960337860103\n",
      "Iteration 510, Training Loss: 0.9425909760136933\n",
      "Iteration 511, Training Loss: 0.9425970307870891\n",
      "Iteration 512, Training Loss: 0.9425906068864656\n",
      "Iteration 513, Training Loss: 0.9425927966466491\n",
      "Iteration 514, Training Loss: 0.9425900757601229\n",
      "Iteration 515, Training Loss: 0.9425936378622475\n",
      "Iteration 516, Training Loss: 0.9425896563208809\n",
      "Iteration 517, Training Loss: 0.9425923034881145\n",
      "Iteration 518, Training Loss: 0.9425883590303913\n",
      "Iteration 519, Training Loss: 0.942590680514571\n",
      "Iteration 520, Training Loss: 0.9425879550775326\n",
      "Iteration 521, Training Loss: 0.9425902571412013\n",
      "Iteration 522, Training Loss: 0.9425864012359557\n",
      "Iteration 523, Training Loss: 0.9425925209858138\n",
      "Iteration 524, Training Loss: 0.9425862329510281\n",
      "Iteration 525, Training Loss: 0.9425886686546546\n",
      "Iteration 526, Training Loss: 0.9425859826989567\n",
      "Iteration 527, Training Loss: 0.9425883255460521\n",
      "Iteration 528, Training Loss: 0.9425868565404179\n",
      "Iteration 529, Training Loss: 0.9425866929792561\n",
      "Iteration 530, Training Loss: 0.94258922026113\n",
      "Iteration 531, Training Loss: 0.9425816638439245\n",
      "Iteration 532, Training Loss: 0.9425878357585948\n",
      "Iteration 533, Training Loss: 0.9425840026450552\n",
      "Iteration 534, Training Loss: 0.9425850904535764\n",
      "Iteration 535, Training Loss: 0.9425849373760464\n",
      "Iteration 536, Training Loss: 0.9425860720293158\n",
      "Iteration 537, Training Loss: 0.9425834479268639\n",
      "Iteration 538, Training Loss: 0.9425845210021496\n",
      "Iteration 539, Training Loss: 0.9425807082466506\n",
      "Iteration 540, Training Loss: 0.94258464488554\n",
      "Iteration 541, Training Loss: 0.9425807363778629\n",
      "Early stopping at iteration 541\n",
      "Iteration 0, Training Loss: 1.0\n",
      "Iteration 1, Training Loss: 0.9872706329380041\n",
      "Iteration 2, Training Loss: 0.975814202582208\n",
      "Iteration 3, Training Loss: 0.9656881102957398\n",
      "Iteration 4, Training Loss: 0.9584424741018861\n",
      "Iteration 5, Training Loss: 0.9546510469496488\n",
      "Iteration 6, Training Loss: 0.952040700882167\n",
      "Iteration 7, Training Loss: 0.9502371037717408\n",
      "Iteration 8, Training Loss: 0.9488697701555439\n",
      "Iteration 9, Training Loss: 0.9478026817399715\n",
      "Iteration 10, Training Loss: 0.9469696714573369\n",
      "Iteration 11, Training Loss: 0.9462976375642266\n",
      "Iteration 12, Training Loss: 0.9457311691452442\n",
      "Iteration 13, Training Loss: 0.9452114626619833\n",
      "Iteration 14, Training Loss: 0.9448008927834435\n",
      "Iteration 15, Training Loss: 0.9444370418536704\n",
      "Iteration 16, Training Loss: 0.9441646890537476\n",
      "Iteration 17, Training Loss: 0.9439613218051692\n",
      "Iteration 18, Training Loss: 0.9438054598191216\n",
      "Iteration 19, Training Loss: 0.9436436805646956\n",
      "Iteration 20, Training Loss: 0.9435108635037704\n",
      "Iteration 21, Training Loss: 0.9433789267472996\n",
      "Iteration 22, Training Loss: 0.9432849687191195\n",
      "Iteration 23, Training Loss: 0.9432399915840562\n",
      "Iteration 24, Training Loss: 0.9431107750927707\n",
      "Iteration 25, Training Loss: 0.9430948328919746\n",
      "Iteration 26, Training Loss: 0.9430064940915968\n",
      "Iteration 27, Training Loss: 0.9429754571373475\n",
      "Iteration 28, Training Loss: 0.9429223662119834\n",
      "Iteration 29, Training Loss: 0.9428749051674012\n",
      "Iteration 30, Training Loss: 0.9428970034935851\n",
      "Iteration 31, Training Loss: 0.9427653892573898\n",
      "Iteration 32, Training Loss: 0.9428091573415988\n",
      "Iteration 33, Training Loss: 0.9427602668487567\n",
      "Iteration 34, Training Loss: 0.9427417397944106\n",
      "Iteration 35, Training Loss: 0.9427118430979111\n",
      "Iteration 36, Training Loss: 0.9427239291634522\n",
      "Iteration 37, Training Loss: 0.9426361280129223\n",
      "Iteration 38, Training Loss: 0.9426799526197139\n",
      "Iteration 39, Training Loss: 0.9426565608773075\n",
      "Iteration 40, Training Loss: 0.9426609489844495\n",
      "Iteration 41, Training Loss: 0.9426152684704104\n",
      "Iteration 42, Training Loss: 0.9426606827482703\n",
      "Iteration 43, Training Loss: 0.9425547935357353\n",
      "Iteration 44, Training Loss: 0.9426447979157965\n",
      "Iteration 45, Training Loss: 0.9426120141793329\n",
      "Iteration 46, Training Loss: 0.9426076490565228\n",
      "Iteration 47, Training Loss: 0.9425920461604282\n",
      "Iteration 48, Training Loss: 0.9425895961705328\n",
      "Iteration 49, Training Loss: 0.9426005171905762\n",
      "Iteration 50, Training Loss: 0.9425521697554143\n",
      "Iteration 51, Training Loss: 0.9426043377024766\n",
      "Iteration 52, Training Loss: 0.9425386487222327\n",
      "Iteration 53, Training Loss: 0.9425781736456924\n",
      "Early stopping at iteration 53\n",
      "Iteration 0, Training Loss: 1.0\n",
      "Iteration 1, Training Loss: 0.9987270632938003\n",
      "Iteration 2, Training Loss: 0.9974795853217249\n",
      "Iteration 3, Training Loss: 0.9962570569090908\n",
      "Iteration 4, Training Loss: 0.9950589790647093\n",
      "Iteration 5, Training Loss: 0.9938848627772157\n",
      "Iteration 6, Training Loss: 0.9927342288154717\n",
      "Iteration 7, Training Loss: 0.9916066075329627\n",
      "Iteration 8, Training Loss: 0.9905015386761038\n",
      "Iteration 9, Training Loss: 0.9894185711963821\n",
      "Iteration 10, Training Loss: 0.9883572630662549\n",
      "Iteration 11, Training Loss: 0.9873171810987303\n",
      "Iteration 12, Training Loss: 0.9862979007705561\n",
      "Iteration 13, Training Loss: 0.9852990060489454\n",
      "Iteration 14, Training Loss: 0.984320089221767\n",
      "Iteration 15, Training Loss: 0.9833607507311319\n",
      "Iteration 16, Training Loss: 0.9824205990103098\n",
      "Iteration 17, Training Loss: 0.981499250323904\n",
      "Iteration 18, Training Loss: 0.9805963286112264\n",
      "Iteration 19, Training Loss: 0.9797114653328023\n",
      "Iteration 20, Training Loss: 0.9788442993199465\n",
      "Iteration 21, Training Loss: 0.9779944766273482\n",
      "Iteration 22, Training Loss: 0.9771616503886015\n",
      "Iteration 23, Training Loss: 0.9763454806746299\n",
      "Iteration 24, Training Loss: 0.9755456343549378\n",
      "Iteration 25, Training Loss: 0.9747617849616393\n",
      "Iteration 26, Training Loss: 0.973993696039845\n",
      "Iteration 27, Training Loss: 0.9732504175675984\n",
      "Iteration 28, Training Loss: 0.9725220046647969\n",
      "Iteration 29, Training Loss: 0.9718081600200513\n",
      "Iteration 30, Training Loss: 0.9711085922682007\n",
      "Iteration 31, Training Loss: 0.9704230158713869\n",
      "Iteration 32, Training Loss: 0.9697515527851608\n",
      "Iteration 33, Training Loss: 0.9691003457270533\n",
      "Iteration 34, Training Loss: 0.9684805767567215\n",
      "Iteration 35, Training Loss: 0.9678918092406598\n",
      "Iteration 36, Training Loss: 0.9673251689981042\n",
      "Iteration 37, Training Loss: 0.96678836256679\n",
      "Iteration 38, Training Loss: 0.9662732609508204\n",
      "Iteration 39, Training Loss: 0.965774186101402\n",
      "Iteration 40, Training Loss: 0.9652915230772298\n",
      "Iteration 41, Training Loss: 0.9648339025788807\n",
      "Iteration 42, Training Loss: 0.9643860417952679\n",
      "Iteration 43, Training Loss: 0.9639574553184109\n",
      "Iteration 44, Training Loss: 0.9635495519961873\n",
      "Iteration 45, Training Loss: 0.9631596545454402\n",
      "Iteration 46, Training Loss: 0.9628026795840499\n",
      "Iteration 47, Training Loss: 0.9624662636534697\n",
      "Iteration 48, Training Loss: 0.9621475486819917\n",
      "Iteration 49, Training Loss: 0.9618421138347146\n",
      "Iteration 50, Training Loss: 0.9615515140358493\n",
      "Iteration 51, Training Loss: 0.9612727059944755\n",
      "Iteration 52, Training Loss: 0.9609987713803162\n",
      "Iteration 53, Training Loss: 0.9607478699635058\n",
      "Iteration 54, Training Loss: 0.9605118933762455\n",
      "Iteration 55, Training Loss: 0.9602918675893846\n",
      "Iteration 56, Training Loss: 0.9600851949068417\n",
      "Iteration 57, Training Loss: 0.9598871520882116\n",
      "Iteration 58, Training Loss: 0.9596939474684152\n",
      "Iteration 59, Training Loss: 0.9595173336401178\n",
      "Iteration 60, Training Loss: 0.9593484373009892\n",
      "Iteration 61, Training Loss: 0.9591977364707459\n",
      "Iteration 62, Training Loss: 0.9590516128420119\n",
      "Iteration 63, Training Loss: 0.9589161963393\n",
      "Iteration 64, Training Loss: 0.9587846721120387\n",
      "Iteration 65, Training Loss: 0.9586558042776019\n",
      "Iteration 66, Training Loss: 0.9585352203871251\n",
      "Iteration 67, Training Loss: 0.9584176380567596\n",
      "Iteration 68, Training Loss: 0.9583048080870878\n",
      "Iteration 69, Training Loss: 0.958198528996569\n",
      "Iteration 70, Training Loss: 0.9580958748385071\n",
      "Iteration 71, Training Loss: 0.9579961510331889\n",
      "Iteration 72, Training Loss: 0.9578955216766376\n",
      "Iteration 73, Training Loss: 0.9578023300050815\n",
      "Iteration 74, Training Loss: 0.957708582456395\n",
      "Iteration 75, Training Loss: 0.9576188014826277\n",
      "Iteration 76, Training Loss: 0.957529046029274\n",
      "Iteration 77, Training Loss: 0.9574427441478625\n",
      "Iteration 78, Training Loss: 0.9573618304333948\n",
      "Iteration 79, Training Loss: 0.957279168139327\n",
      "Iteration 80, Training Loss: 0.9572011915681101\n",
      "Iteration 81, Training Loss: 0.9571220758053767\n",
      "Iteration 82, Training Loss: 0.9570444708725425\n",
      "Iteration 83, Training Loss: 0.9569688119046718\n",
      "Iteration 84, Training Loss: 0.9568939563165193\n",
      "Iteration 85, Training Loss: 0.9568216172544346\n",
      "Iteration 86, Training Loss: 0.9567497502980933\n",
      "Iteration 87, Training Loss: 0.9566853724488601\n",
      "Iteration 88, Training Loss: 0.956616665313565\n",
      "Iteration 89, Training Loss: 0.9565549441222645\n",
      "Iteration 90, Training Loss: 0.9564944654480473\n",
      "Iteration 91, Training Loss: 0.9564349874675915\n",
      "Iteration 92, Training Loss: 0.9563749021743542\n",
      "Iteration 93, Training Loss: 0.9563252530487527\n",
      "Iteration 94, Training Loss: 0.9562727336920935\n",
      "Iteration 95, Training Loss: 0.956222763301012\n",
      "Iteration 96, Training Loss: 0.9561695168155272\n",
      "Iteration 97, Training Loss: 0.9561243851041956\n",
      "Iteration 98, Training Loss: 0.9560778607374982\n",
      "Iteration 99, Training Loss: 0.9560324705786345\n",
      "Iteration 100, Training Loss: 0.9559901426356512\n",
      "Iteration 101, Training Loss: 0.9559494485141615\n",
      "Iteration 102, Training Loss: 0.9559061234675926\n",
      "Iteration 103, Training Loss: 0.9558696824978347\n",
      "Iteration 104, Training Loss: 0.955833085989843\n",
      "Iteration 105, Training Loss: 0.9557927852220124\n",
      "Iteration 106, Training Loss: 0.9557580077138473\n",
      "Iteration 107, Training Loss: 0.9557239680092066\n",
      "Iteration 108, Training Loss: 0.9556833527473284\n",
      "Iteration 109, Training Loss: 0.9556504630391252\n",
      "Iteration 110, Training Loss: 0.9556192612676521\n",
      "Iteration 111, Training Loss: 0.9555852296937954\n",
      "Iteration 112, Training Loss: 0.9555603929178197\n",
      "Iteration 113, Training Loss: 0.9555277487419871\n",
      "Iteration 114, Training Loss: 0.9554987439278236\n",
      "Iteration 115, Training Loss: 0.9554700812159121\n",
      "Iteration 116, Training Loss: 0.9554420115402397\n",
      "Iteration 117, Training Loss: 0.9554147538934104\n",
      "Iteration 118, Training Loss: 0.9553874467999892\n",
      "Iteration 119, Training Loss: 0.9553616416629997\n",
      "Iteration 120, Training Loss: 0.9553352589976034\n",
      "Iteration 121, Training Loss: 0.9553154281031025\n",
      "Iteration 122, Training Loss: 0.9552827266535401\n",
      "Iteration 123, Training Loss: 0.9552635759085277\n",
      "Iteration 124, Training Loss: 0.9552371289348798\n",
      "Iteration 125, Training Loss: 0.9552162113075151\n",
      "Iteration 126, Training Loss: 0.9551909689988424\n",
      "Iteration 127, Training Loss: 0.9551681886995157\n",
      "Iteration 128, Training Loss: 0.9551443181129794\n",
      "Iteration 129, Training Loss: 0.9551273922126032\n",
      "Iteration 130, Training Loss: 0.9551041743740675\n",
      "Iteration 131, Training Loss: 0.9550857371370064\n",
      "Iteration 132, Training Loss: 0.9550605901898852\n",
      "Iteration 133, Training Loss: 0.9550432272676893\n",
      "Iteration 134, Training Loss: 0.9550236292256324\n",
      "Iteration 135, Training Loss: 0.9549998356506639\n",
      "Iteration 136, Training Loss: 0.9549808968454038\n",
      "Iteration 137, Training Loss: 0.9549683279528934\n",
      "Iteration 138, Training Loss: 0.9549423590654538\n",
      "Iteration 139, Training Loss: 0.9549329218979887\n",
      "Iteration 140, Training Loss: 0.9549079167721763\n",
      "Iteration 141, Training Loss: 0.9548963545070738\n",
      "Iteration 142, Training Loss: 0.9548798951895882\n",
      "Iteration 143, Training Loss: 0.954866195187224\n",
      "Iteration 144, Training Loss: 0.9548505350650435\n",
      "Iteration 145, Training Loss: 0.9548397463366691\n",
      "Iteration 146, Training Loss: 0.9548152078475303\n",
      "Iteration 147, Training Loss: 0.9548096826941971\n",
      "Iteration 148, Training Loss: 0.9547979355364107\n",
      "Iteration 149, Training Loss: 0.954780657286068\n",
      "Iteration 150, Training Loss: 0.9547767733533501\n",
      "Iteration 151, Training Loss: 0.9547575983412457\n",
      "Iteration 152, Training Loss: 0.9547463901550985\n",
      "Iteration 153, Training Loss: 0.9547331371868963\n",
      "Iteration 154, Training Loss: 0.9547247225691672\n",
      "Iteration 155, Training Loss: 0.9547147473720181\n",
      "Iteration 156, Training Loss: 0.9547014396175815\n",
      "Iteration 157, Training Loss: 0.9546944721905203\n",
      "Iteration 158, Training Loss: 0.9546817789905346\n",
      "Iteration 159, Training Loss: 0.9546774273905643\n",
      "Iteration 160, Training Loss: 0.9546603552415337\n",
      "Iteration 161, Training Loss: 0.9546560318547297\n",
      "Iteration 162, Training Loss: 0.9546445881943787\n",
      "Iteration 163, Training Loss: 0.9546379916113932\n",
      "Iteration 164, Training Loss: 0.9546246998733824\n",
      "Iteration 165, Training Loss: 0.9546232915228455\n",
      "Iteration 166, Training Loss: 0.9546032262758148\n",
      "Iteration 167, Training Loss: 0.9546044228490879\n",
      "Iteration 168, Training Loss: 0.9545966228770129\n",
      "Iteration 169, Training Loss: 0.9545811334114959\n",
      "Iteration 170, Training Loss: 0.9545835237916361\n",
      "Iteration 171, Training Loss: 0.9545660045625591\n",
      "Iteration 172, Training Loss: 0.9545710691361398\n",
      "Iteration 173, Training Loss: 0.9545566072552493\n",
      "Iteration 174, Training Loss: 0.9545514610696821\n",
      "Iteration 175, Training Loss: 0.9545424730934496\n",
      "Iteration 176, Training Loss: 0.9545352195701785\n",
      "Iteration 177, Training Loss: 0.9545289229118148\n",
      "Iteration 178, Training Loss: 0.9545271258960959\n",
      "Iteration 179, Training Loss: 0.9545132424784767\n",
      "Iteration 180, Training Loss: 0.9545120389594294\n",
      "Iteration 181, Training Loss: 0.954498346455569\n",
      "Iteration 182, Training Loss: 0.9545000212378429\n",
      "Iteration 183, Training Loss: 0.9544936567809599\n",
      "Iteration 184, Training Loss: 0.9544808533554854\n",
      "Iteration 185, Training Loss: 0.9544822142211092\n",
      "Iteration 186, Training Loss: 0.9544747457953681\n",
      "Iteration 187, Training Loss: 0.9544712874115882\n",
      "Iteration 188, Training Loss: 0.9544636160938653\n",
      "Iteration 189, Training Loss: 0.9544661271464904\n",
      "Iteration 190, Training Loss: 0.9544482910581382\n",
      "Iteration 191, Training Loss: 0.9544508191199043\n",
      "Iteration 192, Training Loss: 0.9544429810461712\n",
      "Iteration 193, Training Loss: 0.9544385735977248\n",
      "Iteration 194, Training Loss: 0.9544358109578074\n",
      "Iteration 195, Training Loss: 0.9544243221580937\n",
      "Iteration 196, Training Loss: 0.9544294725528475\n",
      "Iteration 197, Training Loss: 0.9544225981792691\n",
      "Iteration 198, Training Loss: 0.9544179445855016\n",
      "Iteration 199, Training Loss: 0.9544136893352243\n",
      "Iteration 200, Training Loss: 0.9544046067652701\n",
      "Iteration 201, Training Loss: 0.9544079298364921\n",
      "Iteration 202, Training Loss: 0.9543941559678558\n",
      "Iteration 203, Training Loss: 0.9543997081446653\n",
      "Iteration 204, Training Loss: 0.9543840401100527\n",
      "Iteration 205, Training Loss: 0.9543947434424563\n",
      "Iteration 206, Training Loss: 0.9543862892462599\n",
      "Iteration 207, Training Loss: 0.9543823484608921\n",
      "Iteration 208, Training Loss: 0.9543738549164738\n",
      "Iteration 209, Training Loss: 0.9543779407753219\n",
      "Iteration 210, Training Loss: 0.9543667625660857\n",
      "Iteration 211, Training Loss: 0.9543636732938798\n",
      "Iteration 212, Training Loss: 0.9543651466334202\n",
      "Iteration 213, Training Loss: 0.954359771651583\n",
      "Iteration 214, Training Loss: 0.9543586256940013\n",
      "Iteration 215, Training Loss: 0.9543510239745064\n",
      "Iteration 216, Training Loss: 0.9543552247984808\n",
      "Iteration 217, Training Loss: 0.9543425053442101\n",
      "Iteration 218, Training Loss: 0.9543494725931996\n",
      "Iteration 219, Training Loss: 0.954346480915204\n",
      "Iteration 220, Training Loss: 0.9543313563777065\n",
      "Iteration 221, Training Loss: 0.9543383055584639\n",
      "Iteration 222, Training Loss: 0.9543334228244258\n",
      "Iteration 223, Training Loss: 0.9543303051815527\n",
      "Iteration 224, Training Loss: 0.9543233812358318\n",
      "Iteration 225, Training Loss: 0.9543336084374789\n",
      "Iteration 226, Training Loss: 0.9543184876484901\n",
      "Iteration 227, Training Loss: 0.9543233034005371\n",
      "Iteration 228, Training Loss: 0.95432084183419\n",
      "Iteration 229, Training Loss: 0.9543185394513579\n",
      "Iteration 230, Training Loss: 0.9543109967445141\n",
      "Iteration 231, Training Loss: 0.9543141722909408\n",
      "Iteration 232, Training Loss: 0.9543088130767583\n",
      "Iteration 233, Training Loss: 0.9543073520469414\n",
      "Iteration 234, Training Loss: 0.9542950807521363\n",
      "Iteration 235, Training Loss: 0.9543059144398218\n",
      "Iteration 236, Training Loss: 0.9542984360561477\n",
      "Iteration 237, Training Loss: 0.954296589442884\n",
      "Iteration 238, Training Loss: 0.9542971843839473\n",
      "Iteration 239, Training Loss: 0.954292669817539\n",
      "Iteration 240, Training Loss: 0.9542888955173588\n",
      "Iteration 241, Training Loss: 0.9542965399290597\n",
      "Iteration 242, Training Loss: 0.9542900776056984\n",
      "Iteration 243, Training Loss: 0.9542903798013447\n",
      "Iteration 244, Training Loss: 0.9542818085447677\n",
      "Iteration 245, Training Loss: 0.9542871860359348\n",
      "Iteration 246, Training Loss: 0.9542856535850857\n",
      "Iteration 247, Training Loss: 0.9542740107354001\n",
      "Iteration 248, Training Loss: 0.954284897697265\n",
      "Iteration 249, Training Loss: 0.9542734609734842\n",
      "Iteration 250, Training Loss: 0.954276709610621\n",
      "Iteration 251, Training Loss: 0.9542707195234591\n",
      "Iteration 252, Training Loss: 0.9542762221973269\n",
      "Iteration 253, Training Loss: 0.9542704467741469\n",
      "Iteration 254, Training Loss: 0.9542735294539039\n",
      "Iteration 255, Training Loss: 0.9542673705566957\n",
      "Iteration 256, Training Loss: 0.9542684426376037\n",
      "Iteration 257, Training Loss: 0.9542621735848213\n",
      "Iteration 258, Training Loss: 0.9542737238391859\n",
      "Iteration 259, Training Loss: 0.9542594225515164\n",
      "Iteration 260, Training Loss: 0.9542661526334514\n",
      "Iteration 261, Training Loss: 0.9542593431583937\n",
      "Iteration 262, Training Loss: 0.954261226650659\n",
      "Iteration 263, Training Loss: 0.9542595172353363\n",
      "Iteration 264, Training Loss: 0.9542587509087769\n",
      "Iteration 265, Training Loss: 0.9542524678486809\n",
      "Iteration 266, Training Loss: 0.9542615519882595\n",
      "Iteration 267, Training Loss: 0.9542504447672664\n",
      "Iteration 268, Training Loss: 0.9542592413939901\n",
      "Iteration 269, Training Loss: 0.9542507805101275\n",
      "Iteration 270, Training Loss: 0.9542519009348746\n",
      "Iteration 271, Training Loss: 0.9542465072288032\n",
      "Iteration 272, Training Loss: 0.954252700680317\n",
      "Iteration 273, Training Loss: 0.9542422233080652\n",
      "Iteration 274, Training Loss: 0.9542534887451098\n",
      "Iteration 275, Training Loss: 0.9542475583525818\n",
      "Iteration 276, Training Loss: 0.9542464989864239\n",
      "Iteration 277, Training Loss: 0.954248130652027\n",
      "Iteration 278, Training Loss: 0.9542328085135052\n",
      "Iteration 279, Training Loss: 0.9542490609859985\n",
      "Iteration 280, Training Loss: 0.9542459497108657\n",
      "Iteration 281, Training Loss: 0.9542445969555435\n",
      "Iteration 282, Training Loss: 0.9542418091977629\n",
      "Iteration 283, Training Loss: 0.9542430856829933\n",
      "Iteration 284, Training Loss: 0.9542375551420036\n",
      "Iteration 285, Training Loss: 0.9542416120262791\n",
      "Iteration 286, Training Loss: 0.9542407526774411\n",
      "Iteration 287, Training Loss: 0.954230333510825\n",
      "Iteration 288, Training Loss: 0.9542394610903429\n",
      "Early stopping at iteration 288\n",
      "Iteration 0, Training Loss: 1.0\n",
      "Iteration 1, Training Loss: 0.9872706329380041\n",
      "Iteration 2, Training Loss: 0.9770871392884076\n",
      "Iteration 3, Training Loss: 0.9689706890622148\n",
      "Iteration 4, Training Loss: 0.963085110452132\n",
      "Iteration 5, Training Loss: 0.9598091824535107\n",
      "Iteration 6, Training Loss: 0.9581438092137857\n",
      "Iteration 7, Training Loss: 0.957188946805559\n",
      "Iteration 8, Training Loss: 0.9564319734579942\n",
      "Iteration 9, Training Loss: 0.9560033578047497\n",
      "Iteration 10, Training Loss: 0.9555191168684989\n",
      "Iteration 11, Training Loss: 0.9553257955636825\n",
      "Iteration 12, Training Loss: 0.9550045263475637\n",
      "Iteration 13, Training Loss: 0.9548647339176479\n",
      "Iteration 14, Training Loss: 0.9546632163680401\n",
      "Iteration 15, Training Loss: 0.9546423294199576\n",
      "Iteration 16, Training Loss: 0.9544626907544191\n",
      "Iteration 17, Training Loss: 0.9546085767302739\n",
      "Iteration 18, Training Loss: 0.9543104958848492\n",
      "Iteration 19, Training Loss: 0.9545337004573974\n",
      "Iteration 20, Training Loss: 0.9542263101014619\n",
      "Iteration 21, Training Loss: 0.9544129080475697\n",
      "Iteration 22, Training Loss: 0.9543512318396697\n",
      "Iteration 23, Training Loss: 0.9542425363431636\n",
      "Iteration 24, Training Loss: 0.9543447987479174\n",
      "Iteration 25, Training Loss: 0.9542131874617468\n",
      "Iteration 26, Training Loss: 0.9543915253268194\n",
      "Iteration 27, Training Loss: 0.954174494775717\n",
      "Iteration 28, Training Loss: 0.9543599500199261\n",
      "Iteration 29, Training Loss: 0.9542226344325884\n",
      "Iteration 30, Training Loss: 0.9542758606170455\n",
      "Iteration 31, Training Loss: 0.9541144347592405\n",
      "Iteration 32, Training Loss: 0.9544113748651554\n",
      "Iteration 33, Training Loss: 0.9541898435306332\n",
      "Iteration 34, Training Loss: 0.9543204384588\n",
      "Iteration 35, Training Loss: 0.9542670526586939\n",
      "Iteration 36, Training Loss: 0.9542104460296359\n",
      "Iteration 37, Training Loss: 0.9542549637769555\n",
      "Iteration 38, Training Loss: 0.954153656422177\n",
      "Iteration 39, Training Loss: 0.9543884500873874\n",
      "Iteration 40, Training Loss: 0.9541551110986918\n",
      "Iteration 41, Training Loss: 0.954358622404792\n",
      "Early stopping at iteration 41\n",
      "Iteration 0, Training Loss: 1.0\n",
      "Iteration 1, Training Loss: 0.9987270632938003\n",
      "Iteration 2, Training Loss: 0.9975050440558488\n",
      "Iteration 3, Training Loss: 0.9963319055874152\n",
      "Iteration 4, Training Loss: 0.9952056926577191\n",
      "Iteration 5, Training Loss: 0.9941245282452107\n",
      "Iteration 6, Training Loss: 0.9930866104092027\n",
      "Iteration 7, Training Loss: 0.992090209286635\n",
      "Iteration 8, Training Loss: 0.99113366420897\n",
      "Iteration 9, Training Loss: 0.9902153809344117\n",
      "Iteration 10, Training Loss: 0.9893338289908357\n",
      "Iteration 11, Training Loss: 0.9884875391250026\n",
      "Iteration 12, Training Loss: 0.9876751008538028\n",
      "Iteration 13, Training Loss: 0.9868951601134514\n",
      "Iteration 14, Training Loss: 0.9861464170027137\n",
      "Iteration 15, Training Loss: 0.9854276236164056\n",
      "Iteration 16, Training Loss: 0.9847375819655497\n",
      "Iteration 17, Training Loss: 0.9840751419807282\n",
      "Iteration 18, Training Loss: 0.9834391995952995\n",
      "Iteration 19, Training Loss: 0.9828286949052879\n",
      "Iteration 20, Training Loss: 0.9822426104028769\n",
      "Iteration 21, Training Loss: 0.9816799692805621\n",
      "Iteration 22, Training Loss: 0.98113983380314\n",
      "Iteration 23, Training Loss: 0.9806213037448149\n",
      "Iteration 24, Training Loss: 0.9801235148888227\n",
      "Iteration 25, Training Loss: 0.9796456375870702\n",
      "Iteration 26, Training Loss: 0.9791868753773878\n",
      "Iteration 27, Training Loss: 0.9787464636560929\n",
      "Iteration 28, Training Loss: 0.9783236684036495\n",
      "Iteration 29, Training Loss: 0.9779177849613038\n",
      "Iteration 30, Training Loss: 0.9775281368566521\n",
      "Iteration 31, Training Loss: 0.9771540746761864\n",
      "Iteration 32, Training Loss: 0.9767949749829395\n",
      "Iteration 33, Training Loss: 0.9764502392774222\n",
      "Iteration 34, Training Loss: 0.9761192930001258\n",
      "Iteration 35, Training Loss: 0.9758015845739212\n",
      "Iteration 36, Training Loss: 0.9754965844847647\n",
      "Iteration 37, Training Loss: 0.9752037843991745\n",
      "Iteration 38, Training Loss: 0.974922696317008\n",
      "Iteration 39, Training Loss: 0.9746528517581281\n",
      "Iteration 40, Training Loss: 0.9743938009816033\n",
      "Iteration 41, Training Loss: 0.9741451122361398\n",
      "Iteration 42, Training Loss: 0.9739068741315356\n",
      "Iteration 43, Training Loss: 0.9736846946148245\n",
      "Iteration 44, Training Loss: 0.9734714022787819\n",
      "Iteration 45, Training Loss: 0.9732666416361812\n",
      "Iteration 46, Training Loss: 0.9730700714192843\n",
      "Iteration 47, Training Loss: 0.9728813640110632\n",
      "Iteration 48, Training Loss: 0.972700204899171\n",
      "Iteration 49, Training Loss: 0.9725262921517547\n",
      "Iteration 50, Training Loss: 0.9723593359142348\n",
      "Iteration 51, Training Loss: 0.9721990579262157\n",
      "Iteration 52, Training Loss: 0.9720451910577176\n",
      "Iteration 53, Training Loss: 0.9718974788639592\n",
      "Iteration 54, Training Loss: 0.9717556751579511\n",
      "Iteration 55, Training Loss: 0.9716195436001835\n",
      "Iteration 56, Training Loss: 0.9714888573047266\n",
      "Iteration 57, Training Loss: 0.9713633984610879\n",
      "Iteration 58, Training Loss: 0.9712429579711948\n",
      "Iteration 59, Training Loss: 0.9711273351008972\n",
      "Iteration 60, Training Loss: 0.9710163371454117\n",
      "Iteration 61, Training Loss: 0.9709097791081456\n",
      "Iteration 62, Training Loss: 0.9708074833923702\n",
      "Iteration 63, Training Loss: 0.9707092795052258\n",
      "Iteration 64, Training Loss: 0.9706150037735671\n",
      "Iteration 65, Training Loss: 0.9705244990711748\n",
      "Iteration 66, Training Loss: 0.970437614556878\n",
      "Iteration 67, Training Loss: 0.9703542054231534\n",
      "Iteration 68, Training Loss: 0.9702741326547778\n",
      "Iteration 69, Training Loss: 0.970197262797137\n",
      "Iteration 70, Training Loss: 0.9701234677338019\n",
      "Iteration 71, Training Loss: 0.9700526244730001\n",
      "Iteration 72, Training Loss: 0.9699846149426306\n",
      "Iteration 73, Training Loss: 0.9699193257934756\n",
      "Iteration 74, Training Loss: 0.9698566482102869\n",
      "Iteration 75, Training Loss: 0.9697966451801727\n",
      "Iteration 76, Training Loss: 0.9697439603425978\n",
      "Iteration 77, Training Loss: 0.9696882973774443\n",
      "Iteration 78, Training Loss: 0.9696348609308968\n",
      "Iteration 79, Training Loss: 0.9695836833353699\n",
      "Iteration 80, Training Loss: 0.9695395611859117\n",
      "Iteration 81, Training Loss: 0.9694920741870257\n",
      "Iteration 82, Training Loss: 0.9694464866680951\n",
      "Iteration 83, Training Loss: 0.9694028049250755\n",
      "Iteration 84, Training Loss: 0.9693659554653136\n",
      "Iteration 85, Training Loss: 0.9693254126952513\n",
      "Iteration 86, Training Loss: 0.9692864916359918\n",
      "Iteration 87, Training Loss: 0.9692491764695135\n",
      "Iteration 88, Training Loss: 0.9692185040437273\n",
      "Iteration 89, Training Loss: 0.9691838593305284\n",
      "Iteration 90, Training Loss: 0.9691506004058578\n",
      "Iteration 91, Training Loss: 0.9691186926692638\n",
      "Iteration 92, Training Loss: 0.9690932666860358\n",
      "Iteration 93, Training Loss: 0.9690636314671447\n",
      "Iteration 94, Training Loss: 0.9690352609990703\n",
      "Iteration 95, Training Loss: 0.9690129914249382\n",
      "Iteration 96, Training Loss: 0.9689918668303942\n",
      "Iteration 97, Training Loss: 0.9689713781532355\n",
      "Iteration 98, Training Loss: 0.9689516993631939\n",
      "Iteration 99, Training Loss: 0.9689278050626324\n",
      "Iteration 100, Training Loss: 0.9689099335331034\n",
      "Iteration 101, Training Loss: 0.9688978864678268\n",
      "Iteration 102, Training Loss: 0.9688760991258792\n",
      "Iteration 103, Training Loss: 0.9688602527013789\n",
      "Iteration 104, Training Loss: 0.9688450397084022\n",
      "Iteration 105, Training Loss: 0.968830666034956\n",
      "Iteration 106, Training Loss: 0.9688216598093929\n",
      "Iteration 107, Training Loss: 0.9688130367476333\n",
      "Iteration 108, Training Loss: 0.9687947356581388\n",
      "Iteration 109, Training Loss: 0.9687871947412806\n",
      "Iteration 110, Training Loss: 0.9687800626362838\n",
      "Iteration 111, Training Loss: 0.9687631560016821\n",
      "Iteration 112, Training Loss: 0.9687618025793788\n",
      "Iteration 113, Training Loss: 0.9687455280029671\n",
      "Iteration 114, Training Loss: 0.9687400788821887\n",
      "Iteration 115, Training Loss: 0.9687297525354718\n",
      "Iteration 116, Training Loss: 0.9687298273797312\n",
      "Iteration 117, Training Loss: 0.9687147840514619\n",
      "Iteration 118, Training Loss: 0.9687054396186016\n",
      "Iteration 119, Training Loss: 0.9687015968118823\n",
      "Iteration 120, Training Loss: 0.9686977788068445\n",
      "Iteration 121, Training Loss: 0.9686841025190842\n",
      "Iteration 122, Training Loss: 0.9686808981478584\n",
      "Iteration 123, Training Loss: 0.9686681340502389\n",
      "Iteration 124, Training Loss: 0.9686807257006796\n",
      "Iteration 125, Training Loss: 0.968667611880996\n",
      "Iteration 126, Training Loss: 0.9686601081570501\n",
      "Iteration 127, Training Loss: 0.9686529294022542\n",
      "Iteration 128, Training Loss: 0.9686512127793775\n",
      "Iteration 129, Training Loss: 0.9686494191328927\n",
      "Iteration 130, Training Loss: 0.9686376635259301\n",
      "Iteration 131, Training Loss: 0.9686363298723443\n",
      "Iteration 132, Training Loss: 0.968625290328968\n",
      "Iteration 133, Training Loss: 0.9686345847369707\n",
      "Iteration 134, Training Loss: 0.9686233584929541\n",
      "Iteration 135, Training Loss: 0.9686226584930627\n",
      "Iteration 136, Training Loss: 0.9686119931460625\n",
      "Iteration 137, Training Loss: 0.9686219803805108\n",
      "Iteration 138, Training Loss: 0.9686112897036471\n",
      "Iteration 139, Training Loss: 0.9686110047644807\n",
      "Iteration 140, Training Loss: 0.9686057717551901\n",
      "Iteration 141, Training Loss: 0.9686008411580846\n",
      "Iteration 142, Training Loss: 0.9686011794199455\n",
      "Iteration 143, Training Loss: 0.9685963311866156\n",
      "Iteration 144, Training Loss: 0.9685967092882355\n",
      "Iteration 145, Training Loss: 0.9685870331615813\n",
      "Iteration 146, Training Loss: 0.9685980655933422\n",
      "Iteration 147, Training Loss: 0.968588326522922\n",
      "Iteration 148, Training Loss: 0.9685889625484428\n",
      "Iteration 149, Training Loss: 0.9685846181748748\n",
      "Iteration 150, Training Loss: 0.9685805123112268\n",
      "Iteration 151, Training Loss: 0.9685816713964823\n",
      "Iteration 152, Training Loss: 0.9685776138581603\n",
      "Iteration 153, Training Loss: 0.9685787385817282\n",
      "Iteration 154, Training Loss: 0.9685697704577441\n",
      "Iteration 155, Training Loss: 0.9685713848009607\n",
      "Iteration 156, Training Loss: 0.9685727713954027\n",
      "Iteration 157, Training Loss: 0.9685691234862227\n",
      "Iteration 158, Training Loss: 0.9685705495874061\n",
      "Iteration 159, Training Loss: 0.9685669471452398\n",
      "Iteration 160, Training Loss: 0.9685637339793716\n",
      "Iteration 161, Training Loss: 0.9685703888529206\n",
      "Iteration 162, Training Loss: 0.9685617637326915\n",
      "Iteration 163, Training Loss: 0.9685634955663223\n",
      "Iteration 164, Training Loss: 0.9685553542680493\n",
      "Iteration 165, Training Loss: 0.9685674606405408\n",
      "Iteration 166, Training Loss: 0.9685588995423122\n",
      "Iteration 167, Training Loss: 0.9685557488531812\n",
      "Iteration 168, Training Loss: 0.9685527918061393\n",
      "Iteration 169, Training Loss: 0.9685651640565034\n",
      "Iteration 170, Training Loss: 0.968556714915407\n",
      "Iteration 171, Training Loss: 0.9685536450568043\n",
      "Iteration 172, Training Loss: 0.9685557053396859\n",
      "Iteration 173, Training Loss: 0.9685478371074859\n",
      "Iteration 174, Training Loss: 0.9685602811676728\n",
      "Iteration 175, Training Loss: 0.9685519976952257\n",
      "Iteration 176, Training Loss: 0.9685541803235977\n",
      "Iteration 177, Training Loss: 0.9685461822054267\n",
      "Iteration 178, Training Loss: 0.968558870899532\n",
      "Iteration 179, Training Loss: 0.9685506705272553\n",
      "Iteration 180, Training Loss: 0.9685478482059445\n",
      "Iteration 181, Training Loss: 0.9685501375571596\n",
      "Iteration 182, Training Loss: 0.9685424724009352\n",
      "Iteration 183, Training Loss: 0.9685551498992966\n",
      "Iteration 184, Training Loss: 0.9685470741598893\n",
      "Iteration 185, Training Loss: 0.9685494513466621\n",
      "Iteration 186, Training Loss: 0.9685416250718175\n",
      "Iteration 187, Training Loss: 0.9685444048441213\n",
      "Iteration 188, Training Loss: 0.9685468449538303\n",
      "Iteration 189, Training Loss: 0.9685392962546744\n",
      "Iteration 190, Training Loss: 0.9685468643499158\n",
      "Iteration 191, Training Loss: 0.9685392719377962\n",
      "Iteration 192, Training Loss: 0.9685521333115005\n",
      "Iteration 193, Training Loss: 0.9685441709570706\n",
      "Iteration 194, Training Loss: 0.9685466712593493\n",
      "Iteration 195, Training Loss: 0.9685389494800998\n",
      "Iteration 196, Training Loss: 0.9685418017134396\n",
      "Iteration 197, Training Loss: 0.9685443853105016\n",
      "Iteration 198, Training Loss: 0.9685368966259482\n",
      "Iteration 199, Training Loss: 0.9685445975426239\n",
      "Iteration 200, Training Loss: 0.96853705550827\n",
      "Iteration 201, Training Loss: 0.9685500442219004\n",
      "Iteration 202, Training Loss: 0.9685421603904218\n",
      "Iteration 203, Training Loss: 0.9685447459543739\n",
      "Iteration 204, Training Loss: 0.9685370965418763\n",
      "Iteration 205, Training Loss: 0.9685399989569471\n",
      "Iteration 206, Training Loss: 0.9685426819238787\n",
      "Iteration 207, Training Loss: 0.9685352396142409\n",
      "Iteration 208, Training Loss: 0.9685379637807516\n",
      "Iteration 209, Training Loss: 0.9685357070704158\n",
      "Iteration 210, Training Loss: 0.9685437523131757\n",
      "Iteration 211, Training Loss: 0.9685411639620338\n",
      "Iteration 212, Training Loss: 0.9685387345591842\n",
      "Iteration 213, Training Loss: 0.9685363733152671\n",
      "Iteration 214, Training Loss: 0.9685342745863288\n",
      "Iteration 215, Training Loss: 0.968547311392525\n",
      "Iteration 216, Training Loss: 0.9685395436869846\n",
      "Iteration 217, Training Loss: 0.9685371537457778\n",
      "Early stopping at iteration 217\n",
      "Iteration 0, Training Loss: 1.0\n",
      "Iteration 1, Training Loss: 0.9872706329380041\n",
      "Iteration 2, Training Loss: 0.9796330127008067\n",
      "Iteration 3, Training Loss: 0.9750504405584881\n",
      "Iteration 4, Training Loss: 0.9723091222460086\n",
      "Iteration 5, Training Loss: 0.970726427833109\n",
      "Iteration 6, Training Loss: 0.969777069432966\n",
      "Iteration 7, Training Loss: 0.9692595039251104\n",
      "Iteration 8, Training Loss: 0.968897303527214\n",
      "Iteration 9, Training Loss: 0.9687815747806517\n",
      "Iteration 10, Training Loss: 0.9687130201611884\n",
      "Iteration 11, Training Loss: 0.9686200365147185\n",
      "Iteration 12, Training Loss: 0.9685653534000341\n",
      "Iteration 13, Training Loss: 0.9686841018917195\n",
      "Iteration 14, Training Loss: 0.9685522603408451\n",
      "Iteration 15, Training Loss: 0.9685742619122614\n",
      "Iteration 16, Training Loss: 0.9685886484592972\n",
      "Iteration 17, Training Loss: 0.9685959785647493\n",
      "Iteration 18, Training Loss: 0.9685010490216313\n",
      "Iteration 19, Training Loss: 0.9686959083446629\n",
      "Iteration 20, Training Loss: 0.9685586327384018\n",
      "Iteration 21, Training Loss: 0.968578428714417\n",
      "Iteration 22, Training Loss: 0.9685413373292995\n",
      "Iteration 23, Training Loss: 0.9686177952982875\n",
      "Iteration 24, Training Loss: 0.9685135492100801\n",
      "Iteration 25, Training Loss: 0.9686530458256448\n",
      "Iteration 26, Training Loss: 0.9685337606951918\n",
      "Iteration 27, Training Loss: 0.9686136740106556\n",
      "Iteration 28, Training Loss: 0.9685616779257525\n",
      "Early stopping at iteration 28\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "svm = LinearSVM()\n",
    "num_samples = 10000\n",
    "train_losses, val_losses = svm.hyperparameter_tuning(x_train[:num_samples], y_train[:num_samples], verbose=True)\n",
    "plot_losses(train_losses[(svm._lambda, svm.lr)], val_losses[(svm._lambda, svm.lr)])\n",
    "svm.train(x_train[:num_samples], y_train[:num_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6fcab370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.0, 0.1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm._lambda, svm.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a3f88f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.8268476582978502\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "y_pred = svm.predict(x_train[num_samples:2*num_samples], scores=True)\n",
    "print(\"AUC-ROC:\", auc_roc(y_pred, y_train[num_samples:2*num_samples]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b1673b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 653., 2373., 1973., 1698., 1357.,  904.,  614.,  310.,  109.,\n",
       "           9.]),\n",
       " array([0.24377926, 0.30581386, 0.36784845, 0.42988305, 0.49191765,\n",
       "        0.55395225, 0.61598685, 0.67802144, 0.74005604, 0.80209064,\n",
       "        0.86412524]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a7db9a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.00, F1 Score: 0.1565\n",
      "Threshold: 0.01, F1 Score: 0.1565\n",
      "Threshold: 0.02, F1 Score: 0.1565\n",
      "Threshold: 0.03, F1 Score: 0.1565\n",
      "Threshold: 0.04, F1 Score: 0.1565\n",
      "Threshold: 0.05, F1 Score: 0.1565\n",
      "Threshold: 0.06, F1 Score: 0.1565\n",
      "Threshold: 0.07, F1 Score: 0.1565\n",
      "Threshold: 0.08, F1 Score: 0.1565\n",
      "Threshold: 0.09, F1 Score: 0.1565\n",
      "Threshold: 0.10, F1 Score: 0.1565\n",
      "Threshold: 0.11, F1 Score: 0.1565\n",
      "Threshold: 0.12, F1 Score: 0.1565\n",
      "Threshold: 0.13, F1 Score: 0.1565\n",
      "Threshold: 0.14, F1 Score: 0.1565\n",
      "Threshold: 0.15, F1 Score: 0.1565\n",
      "Threshold: 0.16, F1 Score: 0.1565\n",
      "Threshold: 0.17, F1 Score: 0.1565\n",
      "Threshold: 0.18, F1 Score: 0.1565\n",
      "Threshold: 0.19, F1 Score: 0.1565\n",
      "Threshold: 0.20, F1 Score: 0.1565\n",
      "Threshold: 0.21, F1 Score: 0.1565\n",
      "Threshold: 0.22, F1 Score: 0.1565\n",
      "Threshold: 0.23, F1 Score: 0.1565\n",
      "Threshold: 0.24, F1 Score: 0.1565\n",
      "Threshold: 0.25, F1 Score: 0.1565\n",
      "Threshold: 0.26, F1 Score: 0.1567\n",
      "Threshold: 0.27, F1 Score: 0.1571\n",
      "Threshold: 0.28, F1 Score: 0.1585\n",
      "Threshold: 0.29, F1 Score: 0.1607\n",
      "Threshold: 0.30, F1 Score: 0.1639\n",
      "Threshold: 0.31, F1 Score: 0.1691\n",
      "Threshold: 0.32, F1 Score: 0.1752\n",
      "Threshold: 0.33, F1 Score: 0.1827\n",
      "Threshold: 0.34, F1 Score: 0.1903\n",
      "Threshold: 0.35, F1 Score: 0.1983\n",
      "Threshold: 0.36, F1 Score: 0.2073\n",
      "Threshold: 0.37, F1 Score: 0.2160\n",
      "Threshold: 0.38, F1 Score: 0.2235\n",
      "Threshold: 0.39, F1 Score: 0.2313\n",
      "Threshold: 0.40, F1 Score: 0.2413\n",
      "Threshold: 0.41, F1 Score: 0.2501\n",
      "Threshold: 0.42, F1 Score: 0.2596\n",
      "Threshold: 0.43, F1 Score: 0.2680\n",
      "Threshold: 0.44, F1 Score: 0.2796\n",
      "Threshold: 0.45, F1 Score: 0.2876\n",
      "Threshold: 0.46, F1 Score: 0.2984\n",
      "Threshold: 0.47, F1 Score: 0.3073\n",
      "Threshold: 0.48, F1 Score: 0.3154\n",
      "Threshold: 0.49, F1 Score: 0.3243\n",
      "Threshold: 0.50, F1 Score: 0.3304\n",
      "Threshold: 0.51, F1 Score: 0.3401\n",
      "Threshold: 0.52, F1 Score: 0.3453\n",
      "Threshold: 0.53, F1 Score: 0.3505\n",
      "Threshold: 0.54, F1 Score: 0.3583\n",
      "Threshold: 0.55, F1 Score: 0.3613\n",
      "Threshold: 0.56, F1 Score: 0.3563\n",
      "Threshold: 0.57, F1 Score: 0.3517\n",
      "Threshold: 0.58, F1 Score: 0.3515\n",
      "Threshold: 0.59, F1 Score: 0.3440\n",
      "Threshold: 0.60, F1 Score: 0.3373\n",
      "Threshold: 0.61, F1 Score: 0.3327\n",
      "Threshold: 0.62, F1 Score: 0.3312\n",
      "Threshold: 0.63, F1 Score: 0.3233\n",
      "Threshold: 0.64, F1 Score: 0.3039\n",
      "Threshold: 0.65, F1 Score: 0.2887\n",
      "Threshold: 0.66, F1 Score: 0.2679\n",
      "Threshold: 0.67, F1 Score: 0.2530\n",
      "Threshold: 0.68, F1 Score: 0.2466\n",
      "Threshold: 0.69, F1 Score: 0.2313\n",
      "Threshold: 0.70, F1 Score: 0.2143\n",
      "Threshold: 0.71, F1 Score: 0.2002\n",
      "Threshold: 0.72, F1 Score: 0.1801\n",
      "Threshold: 0.73, F1 Score: 0.1531\n",
      "Threshold: 0.74, F1 Score: 0.1282\n",
      "Threshold: 0.75, F1 Score: 0.0922\n",
      "Threshold: 0.76, F1 Score: 0.0761\n",
      "Threshold: 0.77, F1 Score: 0.0578\n",
      "Threshold: 0.78, F1 Score: 0.0387\n",
      "Threshold: 0.79, F1 Score: 0.0207\n",
      "Threshold: 0.80, F1 Score: 0.0117\n",
      "Threshold: 0.81, F1 Score: 0.0094\n",
      "Threshold: 0.82, F1 Score: 0.0070\n",
      "Threshold: 0.83, F1 Score: 0.0047\n",
      "Threshold: 0.84, F1 Score: 0.0047\n",
      "Threshold: 0.85, F1 Score: 0.0024\n",
      "Threshold: 0.86, F1 Score: 0.0024\n",
      "Threshold: 0.87, F1 Score: 0.0000\n",
      "Threshold: 0.88, F1 Score: 0.0000\n",
      "Threshold: 0.89, F1 Score: 0.0000\n",
      "Threshold: 0.90, F1 Score: 0.0000\n",
      "Threshold: 0.91, F1 Score: 0.0000\n",
      "Threshold: 0.92, F1 Score: 0.0000\n",
      "Threshold: 0.93, F1 Score: 0.0000\n",
      "Threshold: 0.94, F1 Score: 0.0000\n",
      "Threshold: 0.95, F1 Score: 0.0000\n",
      "Threshold: 0.96, F1 Score: 0.0000\n",
      "Threshold: 0.97, F1 Score: 0.0000\n",
      "Threshold: 0.98, F1 Score: 0.0000\n",
      "Threshold: 0.99, F1 Score: 0.0000\n",
      "Threshold: 1.00, F1 Score: 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc(y_pred, y_train[num_samples:2*num_samples], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fffdf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.3661413856853596\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm.predict(x_train[num_samples:2*num_samples])\n",
    "print(\"F1 Score:\", f_score(y_pred, y_train[num_samples:2*num_samples]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
